{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a3ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use: cuda\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Omniglot\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Will use:\", device)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8780cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29600a",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fae8e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading MNIST dataset ...\n",
      "Elapsed time to read dataset: 0.223741 sec\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from hypnettorch.data import FashionMNISTData, MNISTData\n",
    "from hypnettorch.data.dataset import Dataset\n",
    "from hypnettorch.mnets import LeNet\n",
    "from hypnettorch.mnets.resnet import ResNet\n",
    "from hypnettorch.mnets.mlp import MLP\n",
    "from hypnettorch.hnets import HMLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import learn2learn as l2l\n",
    "import copy\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mnist = MNISTData(data_dir, use_one_hot=True, validation_size=0)\n",
    "fmnist = FashionMNISTData(data_dir, use_one_hot=True, validation_size=0)\n",
    "\n",
    "omniglot = l2l.vision.datasets.FullOmniglot(root=data_dir,\n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.Resize(28, interpolation=Image.LANCZOS),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                lambda x: 1.0 - x,\n",
    "                                            ]),\n",
    "                                            download=True)\n",
    "omniglot = l2l.data.MetaDataset(omniglot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e95df",
   "metadata": {},
   "source": [
    "## Convert the dataset to numpy for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf2a597",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension: (32460, 1, 28, 28)\n",
      "Labels dimension: (32460,)\n",
      "0\n",
      "1622\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for batching and shuffling the data\n",
    "batch_size = len(omniglot)  # Set batch size to the total number of examples to load all data at once\n",
    "data_loader = DataLoader(omniglot, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in data_loader:\n",
    "    images, labels = batch\n",
    "    # Convert PyTorch tensors to NumPy arrays\n",
    "    dataset = images.numpy()\n",
    "    dataset_lbl = labels.numpy()    \n",
    "    sizes = dataset.shape\n",
    "    \n",
    "print(\"Dataset dimension:\", dataset.shape)\n",
    "print(\"Labels dimension:\", dataset_lbl.shape)\n",
    "print(np.min(dataset_lbl))\n",
    "print(np.max(dataset_lbl))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48adcc",
   "metadata": {},
   "source": [
    "## Create 2 different datasets for two disjoint set of labels (deterministic for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe5509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0 ... 1622 1622 1622]\n",
      "(32460, 784)\n",
      "(32460,)\n",
      "Shape of the dataset_0: (22000, 784)\n",
      "Shape of the dataset_1: (10460, 784)\n",
      "Some labels in set 1: [0 0 0 0 0 0 0 0 0 0]\n",
      "Some labels in set 2: [1100 1100 1100 1100 1100 1100 1100 1100 1100 1100]\n",
      "Minimum and maximum amount of sample per classes in the dataset\n",
      "Each classes contains at least 20 samples\n",
      "Each classes contains at most 20 samples\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of training samples from each data handler.\n",
    "# mnist_inps, mnist_trgts = mnist.next_train_batch(4)\n",
    "# dataset_inps, dataset_trgts = dataset.next_train_batch(4)\n",
    "# dataset_full, dataset_full_lbl = dataset.next_train_batch(60000)\n",
    "print(dataset_lbl)\n",
    "\n",
    "n_classes = len(np.unique(dataset_lbl))\n",
    "dataset_full = dataset.reshape((dataset.shape[0], dataset.shape[2]*dataset.shape[3]))\n",
    "dataset_full_lbl = dataset_lbl\n",
    "\n",
    "print(dataset_full.shape)\n",
    "print(dataset_full_lbl.shape)\n",
    "\n",
    "# TODO-yz: you will need to use the same split as is used in evaluation (pull and merge with main to get access to datasets.get_benchmark_tasksets which gives you train/val/test split over classes)\n",
    "sep = 1100\n",
    "lbls_0 = [i for i in range(sep)]\n",
    "lbls_1 = [i for i in range(sep, n_classes)]\n",
    "\n",
    "mask_0 = np.isin(dataset_full_lbl, np.array(lbls_0))\n",
    "mask_1 = np.isin(dataset_full_lbl, np.array(lbls_1))\n",
    "dataset_0, dataset_0_lbl = dataset_full[mask_0], dataset_full_lbl[mask_0]\n",
    "\n",
    "print(\"Shape of the dataset_0:\",dataset_0.shape)\n",
    "\n",
    "dataset_1, dataset_1_lbl = dataset_full[mask_1], dataset_full_lbl[mask_1]\n",
    "\n",
    "print(\"Shape of the dataset_1:\",dataset_1.shape)\n",
    "\n",
    "print(\"Some labels in set 1:\", dataset_0_lbl[0:10])\n",
    "print(\"Some labels in set 2:\", dataset_1_lbl[0:10])\n",
    "assert(np.all(np.isin(dataset_0_lbl, lbls_0)))\n",
    "assert(np.all(np.isin(dataset_1_lbl, lbls_1)))\n",
    "\n",
    "# mnist.plot_samples('MNIST Examples', mnist_inps, outputs=mnist_trgts)\n",
    "# dataset.plot_samples('FashionMNIST Examples with lbl < sep', dataset_0[0:4], outputs=dataset_0_lbl[0:4])\n",
    "# dataset.plot_samples('FashionMNIST Examples with lbl >= sep', dataset_1[0:4], outputs=dataset_1_lbl[0:4])\n",
    "\n",
    "torch_dataset = torch.tensor(dataset_full_lbl)\n",
    "unique_values, counts = torch.unique(torch_dataset, return_counts=True)\n",
    "\n",
    "print(\"Minimum and maximum amount of sample per classes in the dataset\")\n",
    "print(\"Each classes contains at least\", torch.min(counts).item(), \"samples\")\n",
    "print(\"Each classes contains at most\", torch.max(counts).item(), \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7356d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom, rotate\n",
    "from scipy.interpolate import interp2d\n",
    "\n",
    "def rotate_dataset(dataset, angle):\n",
    "    dataset_unflatten = dataset.reshape(-1, 1, 28, 28)\n",
    "    rotated_data = rotate(dataset_unflatten, angle, axes=(2, 3), reshape=False)\n",
    "    return rotated_data.reshape(-1, 784)\n",
    "\n",
    "def zoom_dataset(dataset, zoom_factor):\n",
    "    dataset_unflatten = dataset.reshape(-1, 1, 28, 28)\n",
    "    zoomed_dataset = zoom(dataset_unflatten, (1, 1, zoom_factor, zoom_factor), order=1)\n",
    "    \n",
    "    original_size = dataset_unflatten.shape\n",
    "    zoomed_size = zoomed_dataset.shape\n",
    "    diff = int((zoomed_size[2] - original_size[2])/2)\n",
    "    interpolated_data = zoomed_dataset[:,:,diff:diff+original_size[2], diff:diff+original_size[2]]\n",
    "    return interpolated_data.reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4e57d",
   "metadata": {},
   "source": [
    "### Compute a pgd attack on test set to assert robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc33a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, z_length):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.z_length = z_length\n",
    "        resnet18 = models.resnet18(pretrained=False)\n",
    "        resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), bias=False)\n",
    "        resnet18.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, self.z_length)\n",
    "        self.resnet = resnet18\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return self.resnet(x)\n",
    "    \n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, z_length, p):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.z_length = z_length\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 16, 120)\n",
    "        self.dropout1 = nn.Dropout(p=p)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.dropout2 = nn.Dropout(p=p)\n",
    "        self.fc3 = nn.Linear(84, z_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "class DropResNet(nn.Module):\n",
    "    def __init__(self, z_length, dropout_prob=0.5):\n",
    "        super(DropResNet, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained ResNet-18 model\n",
    "        self.z_length = z_length\n",
    "        resnet18 = models.resnet18(pretrained=True)\n",
    "        resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), bias=False)\n",
    "        resnet18.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Remove the last fully connected layer\n",
    "        self.features = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "        \n",
    "        # Add custom fully connected layers with dropout\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(512, self.z_length)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0badd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_kernel(X, y, cnn, K):\n",
    "    \"\"\"\n",
    "    Compute Hypershot kernel for a support set X and label y\n",
    "    It takes the average of the z's for each label as suggested in the Hypershot paper\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): Support set used to compute the kernel\n",
    "        y (tensor): corresponding labels\n",
    "        cnn : CNN used to compute the embeddings\n",
    "        K: the K of K-shot K-way learning\n",
    "\n",
    "    Returns:\n",
    "        type: embeddings, kernel\n",
    "    \"\"\"\n",
    "    # Obtain the indices that would sort y_test\n",
    "    indices = torch.argsort(y)\n",
    "\n",
    "    # Use the indices to sort the rows of X_test\n",
    "    sorted_X = X[indices].to(device)\n",
    "    sorted_y = y[indices].to(device)\n",
    "    \n",
    "    reshaped_X = sorted_X.view(sorted_X.shape[0], 1, 28, 28).to(device)\n",
    "    nn_X = cnn(reshaped_X)\n",
    "    \n",
    "    # TODO-yz: think this can be turned into a one-liner to make things faster. Sorting looks correct to me btw\n",
    "    mean_X = torch.zeros((int(nn_X.shape[0] / K), nn_X.shape[1])).to(device)\n",
    "    for i in range(K):\n",
    "        mean_X[i] = torch.mean(nn_X[i*K:(i+1)*K], dim = 0)\n",
    "    norm_mean_X = F.normalize(mean_X, p=2, dim=1)\n",
    "    norm_X = F.normalize(nn_X, p=2, dim=1)\n",
    "    \n",
    "    assert(nn_X.shape==(sorted_X.shape[0], cnn.z_length))\n",
    "    \n",
    "    # TODO-yz: in the paper they used normalized dot product which is not the same as making the features 0-1-gaussian. See formula (6) in paper. Think it is okay to have f as the identity (its essentially the same as extending the feature extractor)\n",
    "    return mean_X, torch.matmul(norm_mean_X, torch.t(norm_mean_X))\n",
    "\n",
    "def get_s_and_q_sets(X, y, trgt_lbls, K, q_size):\n",
    "    # TODO-yz: this should become much easier if you use the datasets.py\n",
    "    \"\"\"\n",
    "    Computes a support set for data X for classes in y with K sample per classes\n",
    "    and corresponding query sets of size q_size.\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): Data used to compute the sets (can contain label you do not want for your sets)\n",
    "        y (tensor): corresponding labels\n",
    "        trgt_lbls : the labels that end up in the sets\n",
    "        K: the K of K-shot K-way learning\n",
    "        q_size: amount of sample per classes in query set\n",
    "\n",
    "    Returns:\n",
    "        type: support set, support set labels, query set, query set labels\n",
    "    \"\"\"\n",
    "    s_set = np.zeros((len(trgt_lbls) * K, X.shape[1]))\n",
    "    s_set_lbl = np.zeros((len(trgt_lbls) * K))\n",
    "    \n",
    "    q_set = np.zeros((len(trgt_lbls) * q_size, X.shape[1]))\n",
    "    q_set_lbl = np.zeros((len(trgt_lbls) * q_size))\n",
    "    \n",
    "    for j, l in enumerate(trgt_lbls):\n",
    "        mask = (y == l)\n",
    "        masked_data = X[mask]\n",
    "        masked_lbls = y[mask]\n",
    "        s_set[j*K:(j+1)*K] = masked_data[0:K]\n",
    "        s_set_lbl[j*K:(j+1)*K] = masked_lbls[0:K]\n",
    "        q_set[j*q_size:(j+1)*q_size] = masked_data[K:K+q_size]\n",
    "        q_set_lbl[j*q_size:(j+1)*q_size] = masked_lbls[K:K+q_size]\n",
    "    \n",
    "    s_set = torch.tensor(s_set, requires_grad=True).to(device).float()\n",
    "    s_set_lbl = torch.tensor(s_set_lbl, requires_grad=True).to(device).float()\n",
    "    q_set = torch.tensor(q_set, requires_grad=True).to(device).float()\n",
    "    q_set_lbl = torch.tensor(q_set_lbl, requires_grad=True).to(device).float()\n",
    "    \n",
    "    return s_set, s_set_lbl, q_set, q_set_lbl\n",
    "\n",
    "def get_q_sample_features(X, cnn, kernel, zs):\n",
    "    \"\"\"\n",
    "    Computes the final features used for classification, given a query sample mx\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): query sample \n",
    "        cnn: the cnn trained to compute the desired features\n",
    "        kernel: the kernel corresponding to the corresponding X's support set\n",
    "        zs: z space of the support set corresponding to the query sample\n",
    "\n",
    "    Returns:\n",
    "        type: final flattened features use by the main network\n",
    "    \"\"\"\n",
    "    # TODO-yz: you could pass the entire query set at once instead of iterating over every i in range(q_set.shape[0]). But you'd need to make sure that the first dimension corresponds to the images of the query set and the second dimension to all the ways. Also flattening would have to be done starting at dim 1\n",
    "    X = X.view(-1, 1, 28, 28)\n",
    "    # TODO-yz: not sure why you don't need to unsqueeze here (1,28,28) has no batch_dim\n",
    "    zs_q = cnn(X.view(X.shape[0], 1, 28, -1))\n",
    "    zs_q = F.normalize(zs_q, p=2, dim=1)\n",
    "    zs_q_m = torch.matmul(zs, torch.t(zs_q))\n",
    "    return torch.t(zs_q_m)\n",
    "\n",
    "def compute_sets_and_features(X, y, trgt_lbls, cnn, K, q_size):\n",
    "    s_set, s_set_lbl, q_set, q_set_lbl = get_s_and_q_sets(X, y, trgt_lbls, K, q_size)\n",
    "\n",
    "    # Kernel computation\n",
    "    z_space, kernel = compute_kernel(s_set, s_set_lbl, cnn, K)\n",
    "    all_q_features = get_q_sample_features(q_set, cnn, kernel, z_space).to(device)\n",
    "    all_q_features_lbls = torch.tensor(q_set_lbl).to(device)\n",
    "        \n",
    "    return s_set, s_set_lbl, q_set, q_set_lbl, z_space, kernel, all_q_features, all_q_features_lbls\n",
    "\n",
    "def extend_pred_to_nclasses(pred, n_c, lbls):\n",
    "    out = torch.zeros((pred.shape[0], n_classes)).to(device)\n",
    "    for i in range(out.shape[0]):\n",
    "        # TODO-yz: not sure how this runs, probably have some global c_lbls somewhere bc it's not defined here\n",
    "        out[i][lbls] = pred[i]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafc56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO-yz: could wrap the entire forward pass of your model into a custom nn.Module and then you can use torchattacks.PGD (you will have to do this anyway for evaluation to work + it's cleaner ;)\n",
    "def project(x_adv, x_orig):\n",
    "    epsilon = 8/255.0\n",
    "    x_adv_eps = torch.minimum(torch.maximum(x_adv, x_orig-epsilon), x_orig+epsilon)\n",
    "    return torch.clamp(x_adv_eps, 0, 1)\n",
    "\n",
    "def pgd_attack_data(X, y, t_mnet, t_hnet, K, cnn, kernel, zs):\n",
    "    criterion = nn.CrossEntropyLoss()    \n",
    "    x_adv = torch.clone(X).detach()\n",
    "   \n",
    "    for i in range(20):\n",
    "        x_adv = x_adv.requires_grad_(True)\n",
    "        x_features = torch.zeros((x_adv.shape[0], K)).to(device)\n",
    "        x_features_lbls = torch.zeros((x_adv.shape[0])).to(device)\n",
    "        for j in range(x_adv.shape[0]):\n",
    "            mx = x_adv[j].view(-1, X.shape[1])\n",
    "            x_sample_features = get_q_sample_features(mx, cnn, kernel, zs)\n",
    "            x_features[j] = x_sample_features\n",
    "            x_features_lbls[j] = y[j]\n",
    "            \n",
    "        # Apply to test set\n",
    "        W_mnet = t_hnet(cond_id=0)\n",
    "        logits = t_mnet.forward(x_features, weights=W_mnet)\n",
    "        loss_adv = criterion(logits, x_features_lbls.long())\n",
    "        loss_adv.backward(retain_graph=True)\n",
    "        \n",
    "        grad = x_adv.grad.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_adv = x_adv + 0.1 * torch.sign(grad)  # take a gradient update step to minimize the objective\n",
    "            x_adv = project(x_adv, X)               # ensure we stay in the allowed range\n",
    "            \n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbdcd4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_lbls(X_test, y_test, test_classes, hnet, mnet, Ks, cnn, n_c, q_size):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy for the sample with label test_classes in X_test.\n",
    "    Mainly used as utility for the calc_accuracy function below.\n",
    "    \n",
    "    Args:\n",
    "        X_test (tensor): entire test set\n",
    "        y_test (tensor): corresponding labels\n",
    "        test_classes: the classes we want to consider for testing accuracies (should contain Ks classes)\n",
    "        mnet : main net trained by the hypernetwork\n",
    "        Ks: the K of K-shot K-way\n",
    "        s_cnn: the cnn trained to compute the desired features\n",
    "\n",
    "    Returns:\n",
    "        type: accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test = get_s_and_q_sets(X_test, y_test, \\\n",
    "                                                                                 test_classes, Ks, q_size)\n",
    "        z_space, K = compute_kernel(s_set_test, s_set_lbl_test, cnn, Ks)\n",
    "        \n",
    "        all_q_features = get_q_sample_features(q_set_test, cnn, K, z_space).to(device)\n",
    "        all_q_features_lbls = torch.tensor(q_set_lbl_test).to(device)\n",
    "\n",
    "        # TODO-yz: forward pass looks pretty correct to me now :)\n",
    "        W_dataset_l_acc =  hnet(uncond_input=K.view(1, -1))\n",
    "        dataset_l_P_acc = mnet.forward(all_q_features, weights=W_dataset_l_acc)\n",
    "        # TODO-yz: not sure but seems like you could probably use torch.nn.functional.one_hot for this\n",
    "        prediction_extended_acc = extend_pred_to_nclasses(dataset_l_P_acc, n_c, test_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(prediction_extended_acc, all_q_features_lbls.long())\n",
    "        accuracy = (torch.argmax(prediction_extended_acc,dim=1) == all_q_features_lbls.long()).float().mean().item()\n",
    "        # print(\"Correctly predicted samples had labels:\", all_q_features_lbls[torch.argmax(prediction_extended_acc,dim=1) == all_q_features_lbls.long()])\n",
    "    return accuracy, loss.item()\n",
    "\n",
    "\n",
    "def calc_accuracy(X_test, y_test, hnet, mnet, Ks, cnn, n_c, q_size):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy for the entire X_test test set.\n",
    "    \n",
    "    Args:\n",
    "        X_test (tensor): entire test set\n",
    "        y_test (tensor): corresponding labels\n",
    "        mnet : main net trained by the hypernetwork\n",
    "        Ks: the K of K-shot K-way\n",
    "        s_cnn: the cnn trained to compute the desired features\n",
    "\n",
    "    Returns:\n",
    "        type: average accuracy over all the label batch (of Ks different labels each time)\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(X_test):\n",
    "        X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    else:  \n",
    "        X_test_t = torch.clone(X_test)\n",
    "        \n",
    "    if not torch.is_tensor(y_test):\n",
    "        y_test_t = torch.FloatTensor(y_test).to(device)\n",
    "    else:\n",
    "        y_test_t = torch.clone(y_test)\n",
    "        \n",
    "    diff_classes = torch.unique(y_test_t)\n",
    "    n_diff_classes = diff_classes.shape[0]\n",
    "    n_sets = int(n_diff_classes / Ks)\n",
    "    acc, loss = 0.0, 0.0\n",
    "    for i in range(n_sets):\n",
    "        lbls = diff_classes[i*Ks:(i+1)*Ks].tolist()\n",
    "        d_acc, d_loss = calc_accuracy_lbls(X_test, y_test, lbls, hnet, mnet, Ks, cnn, n_c, q_size)\n",
    "        acc += d_acc\n",
    "        loss += d_loss\n",
    "    acc = acc / n_sets\n",
    "    loss = loss / n_sets\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "437afb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_lbls_adv(X_test, y_test, test_classes, mnet, Ks, s_cnn, q_set_test_adv):\n",
    "    \"\"\"\n",
    "    Same as the calc_accuracy_lbls function but replace the query set with an attacked version of itself.\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test = get_s_and_q_sets(X_test, y_test, \\\n",
    "                                                                                 test_classes, Ks, 5) \n",
    "        q_set_test = q_set_test_adv\n",
    "        z_space, K = compute_kernel(s_set_test, s_set_lbl_test, s_cnn, Ks)\n",
    "        \n",
    "        # Accuracy\n",
    "        all_q_features = torch.zeros((q_set_test.shape[0], Ks)).to(device)\n",
    "        all_q_features_lbls = torch.zeros((q_set_test.shape[0])).to(device)\n",
    "        for i in range(q_set_test.shape[0]):\n",
    "            mx = q_set_test[i].view(-1, q_set_test.shape[1])\n",
    "            my = torch.argmax(q_set_lbl_test[i])\n",
    "            q_sample_features = get_q_sample_features(mx, s_cnn, K, z_space)\n",
    "            all_q_features[i] = q_sample_features\n",
    "            all_q_features_lbls[i] = my\n",
    "\n",
    "        # TODO-yz: here the forward pass doesn't look right, really recommend having one model where you call forward\n",
    "        W_dataset_l = hnet(cond_id=0)\n",
    "        dataset_l_P = mnet.forward(all_q_features, weights=W_dataset_l)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(dataset_l_P, all_q_features_lbls.long())\n",
    "        accuracy = (torch.argmax(dataset_l_P,dim=1) == all_q_features_lbls.long()).float().mean().item()\n",
    "        # print(\"Correctly predicted labels:\", all_q_features_lbls.long()[torch.argmax(dataset_l_P,dim=1) == all_q_features_lbls.long()])\n",
    "    return accuracy, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd4014d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quent\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\quent\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating an MLP with 277 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 11029 weights and 277 outputs (compression ratio: 39.82).\n",
      "The network consists of 11029 unconditional weights (11029 internally maintained) and 0 conditional weights (0 internally maintained).\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "----------------------- Epoch 0  -----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quent\\AppData\\Local\\Temp\\ipykernel_26068\\976550664.py:102: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_q_features_lbls = torch.tensor(q_set_lbl).to(device)\n",
      "C:\\Users\\quent\\AppData\\Local\\Temp\\ipykernel_26068\\3860525144.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  all_q_features_lbls = torch.tensor(q_set_lbl_test).to(device)\n",
      "C:\\Users\\quent\\AppData\\Local\\Temp\\ipykernel_26068\\976550664.py:110: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  out[i][lbls] = pred[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 7.091139316558838)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 7.15891170501709)\n",
      "Local train acc and loss at the end of set: 100 --> (0.19999998807907104, 3.2907145023345947)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 3.4086031913757324)\n",
      "Local train acc and loss at the end of set: 200 --> (0.19999998807907104, 2.1442248821258545)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 2.176351547241211)\n",
      "Global loss at the end of epoch: 0 : 797.3013560771942\n",
      "--> Global valid accuracy after epoch: 0 --> (0.20145453492348844, 1.8870286638086493)\n",
      "--> Global test accuracy after epoch: 0 --> (0.20423075843315858, 1.8861972506229694)\n",
      "\n",
      "----------------------- Epoch 1  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.8302284479141235)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 1.8791453838348389)\n",
      "Local train acc and loss at the end of set: 100 --> (0.19999998807907104, 1.6199374198913574)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 1.6152377128601074)\n",
      "Local train acc and loss at the end of set: 200 --> (0.2800000011920929, 1.621063232421875)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.1599999964237213, 1.6255079507827759)\n",
      "Global loss at the end of epoch: 1 : 364.5467303991318\n",
      "--> Global valid accuracy after epoch: 1 --> (0.2027272630821575, 1.624702534350482)\n",
      "--> Global test accuracy after epoch: 1 --> (0.20538460613729861, 1.6265378869496858)\n",
      "\n",
      "----------------------- Epoch 2  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.11999999731779099, 1.6206955909729004)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 1.633901834487915)\n",
      "Local train acc and loss at the end of set: 100 --> (0.19999998807907104, 1.6178048849105835)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 1.6209179162979126)\n",
      "Local train acc and loss at the end of set: 200 --> (0.2800000011920929, 1.6084799766540527)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.11999999731779099, 1.6160478591918945)\n",
      "Global loss at the end of epoch: 2 : 358.7486035823822\n",
      "--> Global valid accuracy after epoch: 2 --> (0.20145453688773243, 1.6193255717104131)\n",
      "--> Global test accuracy after epoch: 2 --> (0.20307691474086964, 1.6184493899345398)\n",
      "\n",
      "----------------------- Epoch 3  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6166306734085083)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.1599999964237213, 1.6138103008270264)\n",
      "Local train acc and loss at the end of set: 100 --> (0.19999998807907104, 1.6370124816894531)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 1.631781816482544)\n",
      "Local train acc and loss at the end of set: 200 --> (0.2800000011920929, 1.615140676498413)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.2800000011920929, 1.6226897239685059)\n",
      "Global loss at the end of epoch: 3 : 358.11211919784546\n",
      "--> Global valid accuracy after epoch: 3 --> (0.199272717196833, 1.6226476750590584)\n",
      "--> Global test accuracy after epoch: 3 --> (0.19499999105643767, 1.6213112943447554)\n",
      "\n",
      "----------------------- Epoch 4  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.2800000011920929, 1.6040518283843994)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.2800000011920929, 1.6064594984054565)\n",
      "Local train acc and loss at the end of set: 100 --> (0.19999998807907104, 1.6227483749389648)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 1.630617380142212)\n",
      "Local train acc and loss at the end of set: 200 --> (0.19999998807907104, 1.6237804889678955)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 1.6108779907226562)\n",
      "Global loss at the end of epoch: 4 : 357.1134715080261\n",
      "--> Global valid accuracy after epoch: 4 --> (0.19763635552742265, 1.623048985004425)\n",
      "--> Global test accuracy after epoch: 4 --> (0.19115383700969127, 1.6225662437769084)\n",
      "\n",
      "----------------------- Epoch 5  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.5978753566741943)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6088500022888184)\n",
      "Local train acc and loss at the end of set: 100 --> (0.3999999761581421, 1.5984866619110107)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 1.6282457113265991)\n",
      "Local train acc and loss at the end of set: 200 --> (0.3199999928474426, 1.5880062580108643)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 1.6248023509979248)\n",
      "Global loss at the end of epoch: 5 : 353.49058425426483\n",
      "--> Global valid accuracy after epoch: 5 --> (0.20327271866527472, 1.6251752880486574)\n",
      "--> Global test accuracy after epoch: 5 --> (0.1919230682632098, 1.6280948863579676)\n",
      "\n",
      "----------------------- Epoch 6  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.5979663133621216)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 1.5985058546066284)\n",
      "Local train acc and loss at the end of set: 100 --> (0.35999998450279236, 1.4557300806045532)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 1.845776081085205)\n",
      "Local train acc and loss at the end of set: 200 --> (0.23999999463558197, 1.6068612337112427)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 1.8298155069351196)\n",
      "Global loss at the end of epoch: 6 : 321.25636541843414\n",
      "--> Global valid accuracy after epoch: 6 --> (0.20381817336786878, 1.8959313121708956)\n",
      "--> Global test accuracy after epoch: 6 --> (0.2061538379639387, 1.8447869431514006)\n",
      "\n",
      "----------------------- Epoch 7  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.3199999928474426, 1.2914444208145142)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 1.8496825695037842)\n",
      "Local train acc and loss at the end of set: 100 --> (0.4399999976158142, 1.0441186428070068)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.23999999463558197, 1.7705035209655762)\n",
      "Local train acc and loss at the end of set: 200 --> (0.3999999761581421, 1.0867747068405151)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.1599999964237213, 1.8265430927276611)\n",
      "Global loss at the end of epoch: 7 : 254.39009010791779\n",
      "--> Global valid accuracy after epoch: 7 --> (0.20799999165941369, 2.020567785609852)\n",
      "--> Global test accuracy after epoch: 7 --> (0.21076922254780164, 1.9702006658682456)\n",
      "\n",
      "----------------------- Epoch 8  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.3999999761581421, 1.033132553100586)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 2.1457419395446777)\n",
      "Local train acc and loss at the end of set: 100 --> (0.5199999809265137, 0.9384811520576477)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.3199999928474426, 2.0867321491241455)\n",
      "Local train acc and loss at the end of set: 200 --> (0.47999998927116394, 0.9803630113601685)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.1599999964237213, 2.0141592025756836)\n",
      "Global loss at the end of epoch: 8 : 225.41055864095688\n",
      "--> Global valid accuracy after epoch: 8 --> (0.20799999230287292, 2.2896977776830845)\n",
      "--> Global test accuracy after epoch: 8 --> (0.19884614603450665, 2.2020616359435596)\n",
      "\n",
      "----------------------- Epoch 9  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.3999999761581421, 0.9279674291610718)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 2.2397289276123047)\n",
      "Local train acc and loss at the end of set: 100 --> (0.47999998927116394, 0.9036781787872314)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.4399999976158142, 2.0294013023376465)\n",
      "Local train acc and loss at the end of set: 200 --> (0.47999998927116394, 0.9147448539733887)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 1.8218071460723877)\n",
      "Global loss at the end of epoch: 9 : 203.6680070757866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Global valid accuracy after epoch: 9 --> (0.20981817411428147, 2.603711659799923)\n",
      "--> Global test accuracy after epoch: 9 --> (0.2099999922256057, 2.485013173176692)\n",
      "\n",
      "----------------------- Epoch 10  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7999999523162842, 0.6919561624526978)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 3.280329704284668)\n",
      "Local train acc and loss at the end of set: 100 --> (0.47999998927116394, 0.918348491191864)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.2800000011920929, 2.254089117050171)\n",
      "Local train acc and loss at the end of set: 200 --> (0.7199999690055847, 0.6807985901832581)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 2.2492916584014893)\n",
      "Global loss at the end of epoch: 10 : 179.7565712928772\n",
      "--> Global valid accuracy after epoch: 10 --> (0.20745453824373808, 2.9557469237934457)\n",
      "--> Global test accuracy after epoch: 10 --> (0.21153845394460055, 2.8324784957445583)\n",
      "\n",
      "----------------------- Epoch 11  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.5999999642372131, 0.8446753025054932)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.2800000011920929, 2.6805498600006104)\n",
      "Local train acc and loss at the end of set: 100 --> (0.6800000071525574, 0.6613779664039612)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.3199999928474426, 2.9032437801361084)\n",
      "Local train acc and loss at the end of set: 200 --> (0.5199999809265137, 0.7333714365959167)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 2.72434663772583)\n",
      "Global loss at the end of epoch: 11 : 163.39682763814926\n",
      "--> Global valid accuracy after epoch: 11 --> (0.2099999930032275, 3.2702253049070187)\n",
      "--> Global test accuracy after epoch: 11 --> (0.20346153169297254, 3.2375696851656985)\n",
      "\n",
      "----------------------- Epoch 12  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7599999904632568, 0.5788772702217102)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 3.025820255279541)\n",
      "Local train acc and loss at the end of set: 100 --> (0.6399999856948853, 0.6989946961402893)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 3.729304790496826)\n",
      "Local train acc and loss at the end of set: 200 --> (0.8799999952316284, 0.5930360555648804)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 2.610259771347046)\n",
      "Global loss at the end of epoch: 12 : 149.8949327468872\n",
      "--> Global valid accuracy after epoch: 12 --> (0.20727271935479208, 3.602784283052791)\n",
      "--> Global test accuracy after epoch: 12 --> (0.20461537617330366, 3.505051068388499)\n",
      "\n",
      "----------------------- Epoch 13  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7199999690055847, 0.530073881149292)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 3.845026969909668)\n",
      "Local train acc and loss at the end of set: 100 --> (0.6399999856948853, 0.7053139209747314)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.3999999761581421, 3.5280230045318604)\n",
      "Local train acc and loss at the end of set: 200 --> (0.8399999737739563, 0.3671320378780365)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 3.4429750442504883)\n",
      "Global loss at the end of epoch: 13 : 123.19988119602203\n",
      "--> Global valid accuracy after epoch: 13 --> (0.20745453814213927, 4.287919600985267)\n",
      "--> Global test accuracy after epoch: 13 --> (0.20961537727942833, 4.225085496902466)\n",
      "\n",
      "----------------------- Epoch 14  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8399999737739563, 0.48504704236984253)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 3.5441298484802246)\n",
      "Local train acc and loss at the end of set: 100 --> (0.6800000071525574, 0.6646856069564819)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 3.9139819145202637)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.24998363852500916)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.2800000011920929, 3.361879825592041)\n",
      "Global loss at the end of epoch: 14 : 98.68676084280014\n",
      "--> Global valid accuracy after epoch: 14 --> (0.21090908389199864, 4.623980580676686)\n",
      "--> Global test accuracy after epoch: 14 --> (0.2107692247686478, 4.508694336964534)\n",
      "\n",
      "----------------------- Epoch 15  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8399999737739563, 0.41960617899894714)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.2800000011920929, 2.747738838195801)\n",
      "Local train acc and loss at the end of set: 100 --> (0.7599999904632568, 0.4299330413341522)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 4.4292893409729)\n",
      "Local train acc and loss at the end of set: 200 --> (0.8799999952316284, 0.20136362314224243)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 6.881840705871582)\n",
      "Global loss at the end of epoch: 15 : 81.88421150296926\n",
      "--> Global valid accuracy after epoch: 15 --> (0.20872726521708748, 4.7562625402754)\n",
      "--> Global test accuracy after epoch: 15 --> (0.2146153778124314, 4.704823526052328)\n",
      "\n",
      "----------------------- Epoch 16  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8799999952316284, 0.41371920704841614)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.1599999964237213, 6.90200138092041)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9199999570846558, 0.2475556880235672)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.23999999463558197, 4.364598751068115)\n",
      "Local train acc and loss at the end of set: 200 --> (0.8799999952316284, 0.19451181590557098)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 5.343317985534668)\n",
      "Global loss at the end of epoch: 16 : 62.32997138053179\n",
      "--> Global valid accuracy after epoch: 16 --> (0.21363635638898068, 5.190184299512343)\n",
      "--> Global test accuracy after epoch: 16 --> (0.2088461471005128, 5.061241555672425)\n",
      "\n",
      "----------------------- Epoch 17  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9199999570846558, 0.13901031017303467)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 4.183352947235107)\n",
      "Local train acc and loss at the end of set: 100 --> (0.8399999737739563, 0.3564588129520416)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 6.470582962036133)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.08094033598899841)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 5.320070743560791)\n",
      "Global loss at the end of epoch: 17 : 48.0944615714252\n",
      "--> Global valid accuracy after epoch: 17 --> (0.20454544787379828, 5.592101493206891)\n",
      "--> Global test accuracy after epoch: 17 --> (0.20615383860870048, 5.523788449855951)\n",
      "\n",
      "----------------------- Epoch 18  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9199999570846558, 0.23273970186710358)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 5.300844669342041)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.06862777471542358)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.2800000011920929, 6.148271560668945)\n",
      "Local train acc and loss at the end of set: 200 --> (0.8799999952316284, 0.28786414861679077)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 5.829614639282227)\n",
      "Global loss at the end of epoch: 18 : 37.74075098708272\n",
      "--> Global valid accuracy after epoch: 18 --> (0.21363635672764344, 5.944705189358104)\n",
      "--> Global test accuracy after epoch: 18 --> (0.21499999311680978, 5.745815909825838)\n",
      "\n",
      "----------------------- Epoch 19  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9199999570846558, 0.1742534190416336)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 7.4177937507629395)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9599999785423279, 0.06886937469244003)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.23999999463558197, 5.248874664306641)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.32953929901123047)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 7.3580121994018555)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global loss at the end of epoch: 19 : 33.01240942813456\n",
      "--> Global valid accuracy after epoch: 19 --> (0.20836362947117198, 5.884430684826591)\n",
      "--> Global test accuracy after epoch: 19 --> (0.2157692237972067, 5.8602374746249275)\n",
      "\n",
      "----------------------- Epoch 20  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.24018825590610504)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 6.402101993560791)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9199999570846558, 0.20209187269210815)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 5.139620304107666)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.05970229580998421)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 8.552838325500488)\n",
      "Global loss at the end of epoch: 20 : 25.58707440085709\n",
      "--> Global valid accuracy after epoch: 20 --> (0.21290908418595791, 6.28347000208768)\n",
      "--> Global test accuracy after epoch: 20 --> (0.21615383874338406, 6.303657655532543)\n",
      "\n",
      "----------------------- Epoch 21  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.0221349336206913)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.11999999731779099, 5.757009983062744)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9599999785423279, 0.11725598573684692)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 5.426189422607422)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.017872141674160957)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 6.118228912353516)\n",
      "Global loss at the end of epoch: 21 : 20.385857765562832\n",
      "--> Global valid accuracy after epoch: 21 --> (0.20945453870702874, 6.613925114544955)\n",
      "--> Global test accuracy after epoch: 21 --> (0.2080769159902747, 6.275826289103581)\n",
      "\n",
      "----------------------- Epoch 22  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9199999570846558, 0.1283906102180481)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 6.498892784118652)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9599999785423279, 0.3851640224456787)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.3999999761581421, 4.2829742431640625)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.02635430544614792)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 3.396249294281006)\n",
      "Global loss at the end of epoch: 22 : 21.636992387939245\n",
      "--> Global valid accuracy after epoch: 22 --> (0.2121818111701445, 6.403164803981781)\n",
      "--> Global test accuracy after epoch: 22 --> (0.20461537789266843, 6.414678821196923)\n",
      "\n",
      "----------------------- Epoch 23  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.1698693335056305)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 3.7610747814178467)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9599999785423279, 0.23797635734081268)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.3999999761581421, 7.584203720092773)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9199999570846558, 0.09851132333278656)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 9.003582954406738)\n",
      "Global loss at the end of epoch: 23 : 17.881952037569135\n",
      "--> Global valid accuracy after epoch: 23 --> (0.21072726581584322, 6.65938827232881)\n",
      "--> Global test accuracy after epoch: 23 --> (0.21499999290188918, 6.779423759533809)\n",
      "\n",
      "----------------------- Epoch 24  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.09008029848337173)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 5.534992218017578)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.026407847180962563)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.23999999463558197, 5.397773265838623)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.07725511491298676)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 3.4343559741973877)\n",
      "Global loss at the end of epoch: 24 : 16.659807870630175\n",
      "--> Global valid accuracy after epoch: 24 --> (0.2094545389102264, 6.8556761676614935)\n",
      "--> Global test accuracy after epoch: 24 --> (0.2176923003907387, 6.831351853333986)\n",
      "\n",
      "----------------------- Epoch 25  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.24375981092453003)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 5.041734218597412)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.01903028041124344)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 6.691628456115723)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.14535439014434814)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 4.201931953430176)\n",
      "Global loss at the end of epoch: 25 : 20.672485671471804\n",
      "--> Global valid accuracy after epoch: 25 --> (0.2139999933540821, 6.2143782366405835)\n",
      "--> Global test accuracy after epoch: 25 --> (0.21307691623671696, 6.407888832000586)\n",
      "\n",
      "----------------------- Epoch 26  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.17282404005527496)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.35999998450279236, 4.309257507324219)\n",
      "Local train acc and loss at the end of set: 100 --> (0.8799999952316284, 0.19982261955738068)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 7.222662925720215)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.03810303285717964)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 7.957645893096924)\n",
      "Global loss at the end of epoch: 26 : 16.7099955487065\n",
      "--> Global valid accuracy after epoch: 26 --> (0.20981817543506623, 6.56996304880489)\n",
      "--> Global test accuracy after epoch: 26 --> (0.21653845368956143, 6.438640124522722)\n",
      "\n",
      "----------------------- Epoch 27  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.18665382266044617)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 5.047185897827148)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9599999785423279, 0.0880061686038971)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 5.431910991668701)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9199999570846558, 0.3015965521335602)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 6.032561779022217)\n",
      "Global loss at the end of epoch: 27 : 17.43441031430848\n",
      "--> Global valid accuracy after epoch: 27 --> (0.2094545390456915, 6.5561928369782185)\n",
      "--> Global test accuracy after epoch: 27 --> (0.21346153075305316, 6.661101790574881)\n",
      "\n",
      "----------------------- Epoch 28  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.08152855932712555)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 6.187066555023193)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.0023059421218931675)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.3999999761581421, 4.845578193664551)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.1571994572877884)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 3.4547579288482666)\n",
      "Global loss at the end of epoch: 28 : 16.456818523816764\n",
      "--> Global valid accuracy after epoch: 28 --> (0.20527272068641403, 6.584840491685)\n",
      "--> Global test accuracy after epoch: 28 --> (0.20384614714063132, 6.486540528444143)\n",
      "\n",
      "----------------------- Epoch 29  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.010162021033465862)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.2800000011920929, 2.933471918106079)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.007791462354362011)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 4.820035934448242)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.012640807777643204)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 4.375455379486084)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global loss at the end of epoch: 29 : 13.009663723409176\n",
      "--> Global valid accuracy after epoch: 29 --> (0.21036363003606146, 6.4079818671399895)\n",
      "--> Global test accuracy after epoch: 29 --> (0.20538460778502318, 6.376271639878933)\n",
      "\n",
      "----------------------- Epoch 30  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.24106544256210327)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 2.734457015991211)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9599999785423279, 0.03665706515312195)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 5.6084184646606445)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.017248988151550293)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.1599999964237213, 4.162438869476318)\n",
      "Global loss at the end of epoch: 30 : 12.245634670834988\n",
      "--> Global valid accuracy after epoch: 30 --> (0.2158181754025546, 6.6395984877239576)\n",
      "--> Global test accuracy after epoch: 30 --> (0.2049999926239252, 6.803293769176189)\n",
      "\n",
      "----------------------- Epoch 31  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.06128307804465294)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.2800000011920929, 3.244769334793091)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.0023770451080054045)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.2800000011920929, 4.552642345428467)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.11812326312065125)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 6.121667861938477)\n",
      "Global loss at the end of epoch: 31 : 10.178375512710772\n",
      "--> Global valid accuracy after epoch: 31 --> (0.21599999358030883, 7.489399056001143)\n",
      "--> Global test accuracy after epoch: 31 --> (0.2096153779241901, 7.262583668415363)\n",
      "\n",
      "----------------------- Epoch 32  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.009891191497445107)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 5.134547710418701)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.002560134045779705)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.23999999463558197, 6.723934173583984)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.0938509926199913)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.11999999731779099, 5.786675930023193)\n",
      "Global loss at the end of epoch: 32 : 14.414024464902468\n",
      "--> Global valid accuracy after epoch: 32 --> (0.21309090246531096, 6.831720329414715)\n",
      "--> Global test accuracy after epoch: 32 --> (0.21038460910606843, 6.488408783307443)\n",
      "\n",
      "----------------------- Epoch 33  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8799999952316284, 0.24087776243686676)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.35999998450279236, 7.681417465209961)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9599999785423279, 0.0925406739115715)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.2800000011920929, 6.029357433319092)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.008480314165353775)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 2.7364611625671387)\n",
      "Global loss at the end of epoch: 33 : 20.283384838490747\n",
      "--> Global valid accuracy after epoch: 33 --> (0.20745453898879615, 6.237877674536271)\n",
      "--> Global test accuracy after epoch: 33 --> (0.20692307086518177, 5.943075535389093)\n",
      "\n",
      "----------------------- Epoch 34  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.18326719105243683)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.2800000011920929, 6.015495777130127)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.0264938622713089)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.19999998807907104, 4.908890247344971)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.01595199666917324)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 5.232522010803223)\n",
      "Global loss at the end of epoch: 34 : 19.055766875622794\n",
      "--> Global valid accuracy after epoch: 34 --> (0.20709090320901438, 6.124470243670723)\n",
      "--> Global test accuracy after epoch: 34 --> (0.20923076297801274, 6.034297218689551)\n",
      "\n",
      "----------------------- Epoch 35  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.022009698674082756)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.2800000011920929, 3.7072081565856934)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.011144144460558891)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 6.520577430725098)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.057564157992601395)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 5.835975170135498)\n",
      "Global loss at the end of epoch: 35 : 9.049972014152445\n",
      "--> Global valid accuracy after epoch: 35 --> (0.20818181183527817, 6.433269730481235)\n",
      "--> Global test accuracy after epoch: 35 --> (0.20461537767774785, 6.286576788012798)\n",
      "\n",
      "----------------------- Epoch 36  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.0077365399338305)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 3.3613150119781494)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9199999570846558, 0.49865472316741943)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.3999999761581421, 5.336569309234619)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.08018390834331512)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 5.840219020843506)\n",
      "Global loss at the end of epoch: 36 : 11.237650116439909\n",
      "--> Global valid accuracy after epoch: 36 --> (0.20163635770705612, 6.604276245290583)\n",
      "--> Global test accuracy after epoch: 36 --> (0.20923076276309216, 6.444534978041282)\n",
      "\n",
      "----------------------- Epoch 37  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.016308384016156197)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 4.028909206390381)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9599999785423279, 0.04664008319377899)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 6.415949821472168)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.006472747772932053)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.1599999964237213, 4.5523481369018555)\n",
      "Global loss at the end of epoch: 37 : 10.233323774475139\n",
      "--> Global valid accuracy after epoch: 37 --> (0.20854544896971094, 6.509032997218045)\n",
      "--> Global test accuracy after epoch: 37 --> (0.21499999426305294, 6.489011626977187)\n",
      "\n",
      "----------------------- Epoch 38  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9199999570846558, 0.14798744022846222)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 6.312509536743164)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.004354869015514851)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 5.6278977394104)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.06364300847053528)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 5.7593159675598145)\n",
      "Global loss at the end of epoch: 38 : 9.42726502486039\n",
      "--> Global valid accuracy after epoch: 38 --> (0.21436363004825332, 6.862911547314037)\n",
      "--> Global test accuracy after epoch: 38 --> (0.20961537842567152, 6.846843967070947)\n",
      "\n",
      "----------------------- Epoch 39  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.0035394749138504267)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 4.09318733215332)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.01294199749827385)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.2800000011920929, 4.806127071380615)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.11229096353054047)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 5.361891269683838)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global loss at the end of epoch: 39 : 12.771385071682744\n",
      "--> Global valid accuracy after epoch: 39 --> (0.21527272134341977, 6.44249848018993)\n",
      "--> Global test accuracy after epoch: 39 --> (0.21115384000138596, 6.194011511710974)\n",
      "\n",
      "----------------------- Epoch 40  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.00828428752720356)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 5.661026477813721)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.029441656544804573)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.3999999761581421, 5.473513603210449)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.006433972157537937)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 5.747174263000488)\n",
      "Global loss at the end of epoch: 40 : 13.1910816290183\n",
      "--> Global valid accuracy after epoch: 40 --> (0.2150909027931365, 6.163813605091788)\n",
      "--> Global test accuracy after epoch: 40 --> (0.20653845498768184, 6.110949197640786)\n",
      "\n",
      "----------------------- Epoch 41  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.012017915025353432)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 3.8899049758911133)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.03483293578028679)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 5.009719371795654)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.009876037016510963)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.3199999928474426, 3.7747581005096436)\n",
      "Global loss at the end of epoch: 41 : 9.779141993727535\n",
      "--> Global valid accuracy after epoch: 41 --> (0.21818181218748742, 6.257984381372278)\n",
      "--> Global test accuracy after epoch: 41 --> (0.21192307075342307, 6.29684800024216)\n",
      "\n",
      "----------------------- Epoch 42  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.005017868708819151)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.11999999731779099, 2.981196403503418)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.024973738938570023)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.3999999761581421, 7.645695686340332)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.002879814011976123)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 2.3878872394561768)\n",
      "Global loss at the end of epoch: 42 : 9.220411465736106\n",
      "--> Global valid accuracy after epoch: 42 --> (0.20927272134206512, 6.8927952560511505)\n",
      "--> Global test accuracy after epoch: 42 --> (0.21153845480428293, 6.688844866477526)\n",
      "\n",
      "----------------------- Epoch 43  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.004079625476151705)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 2.841257333755493)\n",
      "Local train acc and loss at the end of set: 100 --> (0.9599999785423279, 0.09013301879167557)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.35999998450279236, 8.890774726867676)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.012964317575097084)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.19999998807907104, 5.384666919708252)\n",
      "Global loss at the end of epoch: 43 : 12.033343759016134\n",
      "--> Global valid accuracy after epoch: 43 --> (0.21181817576289177, 6.3108815529129725)\n",
      "--> Global test accuracy after epoch: 43 --> (0.21076922548504975, 6.1180249349429054)\n",
      "\n",
      "----------------------- Epoch 44  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.15936365723609924)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.2800000011920929, 3.4376959800720215)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.010073990561068058)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.3199999928474426, 7.145690441131592)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.07841906696557999)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 4.632226943969727)\n",
      "Global loss at the end of epoch: 44 : 9.94009657209972\n",
      "--> Global valid accuracy after epoch: 44 --> (0.21581817503002557, 6.622897256504405)\n",
      "--> Global test accuracy after epoch: 44 --> (0.2134615325440581, 6.532914817333221)\n",
      "\n",
      "----------------------- Epoch 45  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.01147087849676609)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 3.1755588054656982)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.001054616179317236)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.3999999761581421, 6.280364513397217)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.01227955799549818)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.2800000011920929, 3.6244237422943115)\n",
      "Global loss at the end of epoch: 45 : 11.573110612574965\n",
      "--> Global valid accuracy after epoch: 45 --> (0.2127272666855292, 6.214266037399119)\n",
      "--> Global test accuracy after epoch: 45 --> (0.21230768612944162, 6.023850832994167)\n",
      "\n",
      "----------------------- Epoch 46  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.06358616054058075)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.23999999463558197, 5.502641201019287)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.0007247328176163137)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.2800000011920929, 6.4356231689453125)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.014325698837637901)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.23999999463558197, 3.941851854324341)\n",
      "Global loss at the end of epoch: 46 : 9.208970719191711\n",
      "--> Global valid accuracy after epoch: 46 --> (0.20509090277959, 6.116189247911627)\n",
      "--> Global test accuracy after epoch: 46 --> (0.20999999401661065, 5.993091131632145)\n",
      "\n",
      "----------------------- Epoch 47  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.01018796768039465)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.19999998807907104, 3.3202757835388184)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.033565450459718704)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.23999999463558197, 8.73107624053955)\n",
      "Local train acc and loss at the end of set: 200 --> (0.9599999785423279, 0.034353841096162796)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.1599999964237213, 4.011901378631592)\n",
      "Global loss at the end of epoch: 47 : 7.63699623697903\n",
      "--> Global valid accuracy after epoch: 47 --> (0.21036363034085795, 6.744465445388447)\n",
      "--> Global test accuracy after epoch: 47 --> (0.2123076862010818, 6.311618947065794)\n",
      "\n",
      "----------------------- Epoch 48  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.0070670912973582745)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.2800000011920929, 3.3455252647399902)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.0014675059355795383)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.2800000011920929, 5.464994430541992)\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.0023178216069936752)\n",
      "Local valid acc and loss at the end of set: 200 --> (0.1599999964237213, 4.5600738525390625)\n",
      "Global loss at the end of epoch: 48 : 5.49779083863541\n",
      "--> Global valid accuracy after epoch: 48 --> (0.2061818123541095, 6.61862992698496)\n",
      "--> Global test accuracy after epoch: 48 --> (0.2096153784973117, 6.293495386838913)\n",
      "\n",
      "----------------------- Epoch 49  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.002071874914690852)\n",
      "Local valid acc and loss at the end of set: 0 --> (0.1599999964237213, 3.881556510925293)\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.0013591664610430598)\n",
      "Local valid acc and loss at the end of set: 100 --> (0.23999999463558197, 4.954127788543701)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 151\u001b[0m\n\u001b[0;32m    146\u001b[0m         mx_adv_test \u001b[38;5;241m=\u001b[39m pgd_attack_data(q_set_test, q_set_lbl_test, mnet, hnet, Ks, kcnn, K, z_space)\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocal adv test acc and loss at the end of set:\u001b[39m\u001b[38;5;124m\"\u001b[39m, l_set_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-->\u001b[39m\u001b[38;5;124m\"\u001b[39m, calc_accuracy_lbls_adv(dataset_l_test, dataset_l_lbl_test, c_lbls, hnet, mnet,\\\n\u001b[0;32m    148\u001b[0m                                                                                Ks, kcnn, mx_adv_test))\n\u001b[1;32m--> 151\u001b[0m \u001b[43mloss_dataset_l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    153\u001b[0m optimizer_s\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# General behavior during trainig:\n",
    "# Using LeNet, it very very slowly (400 epoch) converge to a slightly overfitting case with very bad precision\n",
    "# Using DropResNet it converges faster but very quickly overfit too. Using this, we can achieve around 30% accuracy on\n",
    "# full Omniglot dataset but not more and it is not stable at all during training\n",
    "# Using ResNet, like DropResNet but worse in terms of overfitting of course (especially if we do not do this averaging of the\n",
    "# z-space)\n",
    "# Notice that in any of the 3 cases above, we always observe a pleateau around local loss of 1.6\n",
    "# Test and validation accuracies highly varies depending on the value of q_test\n",
    "\n",
    "# Configure training.\n",
    "nepochs=200\n",
    "# epoch after which adversarial training starts\n",
    "do_adv_train = 10000\n",
    "# K-shot k-way\n",
    "Ks = 5\n",
    "# Length of the embeddings produced by the CNN\n",
    "z_len = 50\n",
    "\n",
    "load_weights = 0\n",
    "continue_training = 0\n",
    "\n",
    "# Amount of sample in query sets during training\n",
    "q_train = 5\n",
    "# Amount of sample in query sets during validation and testing\n",
    "q_test = 5\n",
    "\n",
    "# Array storing statistics (not used for now)\n",
    "accuracies_dataset_0 = []\n",
    "accuracies_dataset_0_adv = []\n",
    "accuracies_dataset_1 = []\n",
    "accuracies_dataset_1_adv = []\n",
    "\n",
    "# Loop in case we want to do statistics (not sued for now)\n",
    "for o in range(1):\n",
    "    print(\"Iteration\", o+1)\n",
    "    \n",
    "    if continue_training == 0:\n",
    "        # Models definition\n",
    "        kcnn = DropResNet(z_len, 0.5).to(device)\n",
    "        # kcnn = LeNet(z_len, p=0.0).to(device)\n",
    "        mnet = MLP(n_in=Ks, n_out=Ks, hidden_layers=[16, 8]).to(device)\n",
    "        hnet = HMLP(mnet.param_shapes, uncond_in_size=Ks**2, cond_in_size=0,\n",
    "                    layers=[32, 32], num_cond_embs=0).to(device)\n",
    "        params = hnet.conditional_params.copy()\n",
    "        hnet.apply_hyperfan_init(mnet=mnet)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # If we want to load weights from anywhere\n",
    "        if load_weights == 1:\n",
    "            file_path = 'models/hnet_20231229022719_49.pth'\n",
    "            hnet.load_state_dict(torch.load(file_path))\n",
    "            file_path = 'models/kcnn_20231229022719_49.pth'\n",
    "            kcnn.load_state_dict(torch.load(file_path))\n",
    "\n",
    "        # The amount of sets of Ks labels we can do during training\n",
    "        n_sets = int(len(lbls_0) / Ks)\n",
    "\n",
    "        # Compute training and validation sets for each of the n_sets labels sets\n",
    "        train_test_sets = []\n",
    "        all_test_sets = np.empty((0, dataset_0.shape[1]))\n",
    "        all_test_sets_lbl = np.empty((0))\n",
    "        for l_set_id in range(n_sets):\n",
    "            c_lbls = lbls_0[l_set_id*Ks:(l_set_id+1)*Ks]\n",
    "            if (l_set_id+1) % 100 == 0:\n",
    "                print(\"Generated train-test split for\", l_set_id+1,\"/\",n_sets)\n",
    "            mask_b = np.isin(dataset_0_lbl, np.array(c_lbls))\n",
    "            dataset_0_b, dataset_0_lbl_b = dataset_0[mask_b], dataset_0_lbl[mask_b]\n",
    "            dataset_0_train, dataset_0_test, dataset_0_lbl_train, dataset_0_lbl_test = \\\n",
    "                            train_test_split(dataset_0_b, dataset_0_lbl_b, random_state=42, test_size=0.5, stratify=dataset_0_lbl_b)\n",
    "            \n",
    "            rotate_m10 = rotate_dataset(dataset_0_train, -10)\n",
    "            rotate_m5 = rotate_dataset(dataset_0_train, -5)\n",
    "            rotate_10 = rotate_dataset(dataset_0_train, 10)\n",
    "            rotate_5 = rotate_dataset(dataset_0_train, 5)\n",
    "            zoom_110 = zoom_dataset(dataset_0_train, 1.10)\n",
    "            zoom_125 = zoom_dataset(dataset_0_train, 1.25)\n",
    "            \n",
    "            assert(rotate_m10.shape == dataset_0_train.shape)\n",
    "            assert(zoom_125.shape == dataset_0_train.shape)\n",
    "            \n",
    "            dataset_0_train = np.concatenate((dataset_0_train, rotate_m10, rotate_m5,\n",
    "                                                                           rotate_5,\n",
    "                                                                           rotate_10,\n",
    "                                                                           zoom_110,\n",
    "                                                                           zoom_125), axis=0)\n",
    "            dataset_0_lbl_train = np.repeat(dataset_0_lbl_train, 7, axis=0)\n",
    "            \n",
    "            \n",
    "            all_test_sets = np.concatenate((all_test_sets, dataset_0_test), axis=0)\n",
    "            all_test_sets_lbl = np.concatenate((all_test_sets_lbl, dataset_0_lbl_test), axis=0)\n",
    "            train_test_sets.append((dataset_0_train, dataset_0_test, dataset_0_lbl_train, dataset_0_lbl_test, c_lbls))\n",
    "    \n",
    "    # Optimizer and scheduler initialization\n",
    "    # TODO-yz: it's correct you only have 2 optimizers here. if you introduce a kernel function f, you'll need a third one though\n",
    "    optimizer = optim.Adam(hnet.parameters(), lr=0.00005)\n",
    "    optimizer_s = optim.Adam(kcnn.parameters(), lr=0.00005)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=int(nepochs / 1), eta_min=0.000001)\n",
    "    scheduler_s = CosineAnnealingLR(optimizer_s, T_max=int(nepochs / 1), eta_min=0.000001)\n",
    "        \n",
    "    # Main training loop\n",
    "    for epoch in range(nepochs): # For each epoch.\n",
    "        print(\"----------------------- Epoch\", epoch, \" -----------------------\")\n",
    "        # Stores the loss over all labels sets\n",
    "        global_loss = 0.0\n",
    "        global_loss_float = 0.0\n",
    "        # We loop over all our sets at each epoch\n",
    "        for l_set_id in range(n_sets):\n",
    "            loss_dataset_l = 0.0\n",
    "            (dataset_l_train, dataset_l_test, dataset_l_lbl_train, dataset_l_lbl_test, c_lbls) = train_test_sets[l_set_id]\n",
    "            \n",
    "            s_set_train, s_set_lbl_train, q_set_train, q_set_lbl_train, z_space, K, all_q_features, all_q_features_lbls = \\\n",
    "            compute_sets_and_features(dataset_l_train, dataset_l_lbl_train, c_lbls, kcnn, Ks, q_train)\n",
    "            \n",
    "            # Formward pass\n",
    "            W_dataset_l = hnet(uncond_input=K.view(1, -1))\n",
    "            dataset_l_P = mnet.forward(all_q_features, weights=W_dataset_l)\n",
    "            prediction_extended = extend_pred_to_nclasses(dataset_l_P, n_classes, c_lbls)\n",
    "            loss_dataset_l += criterion(prediction_extended, all_q_features_lbls.long())\n",
    "\n",
    "            # Adversarial training\n",
    "            # TODO-yz: think it's good to do adversarial querying, this gives a more meaningful baseline result\n",
    "            if epoch == do_adv_train and l_set_id == 0:\n",
    "                print(\"Adversarial training starts.\")\n",
    "            if epoch >= do_adv_train:\n",
    "                mx_adv = pgd_attack_data(q_set_train, q_set_lbl_train, mnet, hnet, Ks, kcnn, K, z_space)\n",
    "                \n",
    "                all_q_features_adv = torch.zeros((q_set_train.shape[0], Ks)).to(device)\n",
    "                for i in range(mx_adv.shape[0]):\n",
    "                    mxx = mx_adv[i].view(-1, q_set_train.shape[1])\n",
    "                    q_sample_features_adv = get_q_sample_features(mxx, kcnn, K, z_space)\n",
    "                    all_q_features_adv[i] = q_sample_features_adv\n",
    "                dataset_l_P_adv = mnet.forward(all_q_features_adv, weights=W_dataset_l)\n",
    "                prediction_extended_adv = extend_pred_to_nclasses(dataset_l_P_adv, n_classes, c_lbls)\n",
    "                loss_dataset_l_adv = criterion(prediction_extended_adv, all_q_features_lbls.long())\n",
    "                loss_dataset_l += loss_dataset_l_adv\n",
    "            \n",
    "            global_loss += loss_dataset_l\n",
    "            global_loss_float += loss_dataset_l.item()\n",
    "            if l_set_id % 100 == 0:\n",
    "                train_metrics = calc_accuracy(dataset_l_train, dataset_l_lbl_train, hnet, mnet, Ks, kcnn, n_classes, q_test)\n",
    "                test_metrics = calc_accuracy(dataset_l_test, dataset_l_lbl_test, hnet, mnet, Ks, kcnn, n_classes, q_test)\n",
    "                print(\"Local train acc and loss at the end of set:\", l_set_id, \"-->\", train_metrics)\n",
    "                print(\"Local valid acc and loss at the end of set:\", l_set_id, \"-->\", test_metrics)\n",
    "                if do_adv_train < nepochs:\n",
    "                    s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test, z_space_tes, K_test, all_q_features_test, all_q_features_lbls_test = \\\n",
    "                    compute_sets_and_features(dataset_l_test, dataset_l_lbl_test, c_lbls, kcnn, Ks, 5)\n",
    "                    mx_adv_test = pgd_attack_data(q_set_test, q_set_lbl_test, mnet, hnet, Ks, kcnn, K, z_space)\n",
    "                    print(\"Local adv test acc and loss at the end of set:\", l_set_id, \"-->\", calc_accuracy_lbls_adv(dataset_l_test, dataset_l_lbl_test, c_lbls, hnet, mnet,\\\n",
    "                                                                                           Ks, kcnn, mx_adv_test))\n",
    "\n",
    "\n",
    "            loss_dataset_l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer_s.step()\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_s.zero_grad()\n",
    "                \n",
    "        scheduler.step()\n",
    "        scheduler_s.step()\n",
    "                \n",
    "  \n",
    "        print(\"Global loss at the end of epoch:\", epoch, \":\", global_loss_float)\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            # Create a file name with the current time\n",
    "            hnet_file = f'models/hnet_{current_time}_{epoch}.pth'\n",
    "            torch.save(hnet.state_dict(), hnet_file)\n",
    "            kcnn_file = f'models/kcnn_{current_time}_{epoch}.pth'\n",
    "            torch.save(kcnn.state_dict(), kcnn_file)\n",
    "            print(\"--> Global valid accuracy after epoch:\", epoch, \"-->\", calc_accuracy(all_test_sets, all_test_sets_lbl,\\\n",
    "                                                                                       hnet, mnet, Ks, kcnn, n_classes, q_test))\n",
    "            print(\"--> Global test accuracy after epoch:\", epoch, \"-->\", calc_accuracy(dataset_1, dataset_1_lbl, \\\n",
    "                                                                                       hnet, mnet, Ks, kcnn, n_classes, q_test))\n",
    "        print()\n",
    "\n",
    "    print(\"END OF ITERATION:\",o+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd33649",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Create a file name with the current time\n",
    "hnet_file = f'models/hnet_{current_time}_best.pth'\n",
    "torch.save(hnet.state_dict(), hnet_file)\n",
    "kcnn_file = f'models/kcnn_{current_time}_best.pth'\n",
    "torch.save(kcnn.state_dict(), kcnn_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145607b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_adv_dataset_1 = pgd_attack_data(dataset_1, dataset_1_lbl, mnet, hnet, z_space_1, K_1, 1)\n",
    "# x_adv_dataset_1_np = x_adv_dataset_1.detach().cpu().numpy()\n",
    "# x_adv_dataset_0_test = pgd_attack_data(dataset_0_test, dataset_0_lbl_test, mnet, hnet, z_space, K, 0)\n",
    "# x_adv_dataset_0_test_np = x_adv_dataset_0_test.detach().cpu().numpy()\n",
    "\n",
    "print(calc_accuracy(all_test_sets, all_test_sets_lbl, hnet, mnet, Ks, kcnn, n_classes, 5))\n",
    "print(calc_accuracy(dataset_1, dataset_1_lbl, hnet, mnet, Ks, kcnn, n_classes, 5))\n",
    "# accuracies_dataset_0_adv.append((calc_accuracy(x_adv_dataset_0_test_np, dataset_0_lbl_test, mnet, W_dataset_0)).detach().cpu())\n",
    "# accuracies_dataset_1_adv.append((calc_accuracy(x_adv_dataset_1_np, dataset_1_lbl, mnet, W_dataset_1)).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045661ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean:\")\n",
    "print(\"dataset 0 accuracy:\", np.mean(np.array(accuracies_dataset_0)))\n",
    "print(\"dataset 1 accuracy:\", np.mean(np.array(accuracies_dataset_1)))\n",
    "print(\"dataset 0 adv accuracy:\", np.mean(np.array(accuracies_dataset_0_adv)))\n",
    "print(\"dataset 1 adv accuracy:\", np.mean(np.array(accuracies_dataset_1_adv)))\n",
    "print()\n",
    "print(\"Standard deviation:\")\n",
    "print(\"dataset 0 accuracy:\", np.std(np.array(accuracies_dataset_0)))\n",
    "print(\"dataset 1 accuracy:\", np.std(np.array(accuracies_dataset_1)))\n",
    "print(\"dataset 0 adv accuracy:\", np.std(np.array(accuracies_dataset_0_adv)))\n",
    "print(\"dataset 1 adv accuracy:\", np.std(np.array(accuracies_dataset_1_adv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2e88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
