{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a3ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use: cuda\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Omniglot\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Will use:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8780cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29600a",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fae8e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading MNIST dataset ...\n",
      "Elapsed time to read dataset: 0.229998 sec\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from hypnettorch.data import FashionMNISTData, MNISTData\n",
    "from hypnettorch.data.dataset import Dataset\n",
    "from hypnettorch.mnets import LeNet\n",
    "from hypnettorch.mnets.resnet import ResNet\n",
    "from hypnettorch.mnets.mlp import MLP\n",
    "from hypnettorch.hnets import HMLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import learn2learn as l2l\n",
    "import copy\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mnist = MNISTData(data_dir, use_one_hot=True, validation_size=0)\n",
    "fmnist = FashionMNISTData(data_dir, use_one_hot=True, validation_size=0)\n",
    "\n",
    "omniglot = l2l.vision.datasets.FullOmniglot(root=data_dir,\n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.Resize(28, interpolation=Image.LANCZOS),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                lambda x: 1.0 - x,\n",
    "                                            ]),\n",
    "                                            download=True)\n",
    "omniglot = l2l.data.MetaDataset(omniglot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e95df",
   "metadata": {},
   "source": [
    "## Convert the dataset to numpy for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf2a597",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension: (32460, 1, 28, 28)\n",
      "Labels dimension: (32460,)\n",
      "0\n",
      "1622\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for batching and shuffling the data\n",
    "batch_size = len(omniglot)  # Set batch size to the total number of examples to load all data at once\n",
    "data_loader = DataLoader(omniglot, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in data_loader:\n",
    "    images, labels = batch\n",
    "    # Convert PyTorch tensors to NumPy arrays\n",
    "    dataset = images.numpy()\n",
    "    dataset_lbl = labels.numpy()    \n",
    "    sizes = dataset.shape\n",
    "    \n",
    "print(\"Dataset dimension:\", dataset.shape)\n",
    "print(\"Labels dimension:\", dataset_lbl.shape)\n",
    "print(np.min(dataset_lbl))\n",
    "print(np.max(dataset_lbl))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48adcc",
   "metadata": {},
   "source": [
    "## Create 2 different datasets for two disjoint set of labels (deterministic for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe5509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0 ... 1622 1622 1622]\n",
      "(32460, 784)\n",
      "(32460,)\n",
      "Shape of the dataset_0: (2000, 784)\n",
      "Shape of the dataset_1: (30460, 784)\n",
      "Some labels in set 1: [0 0 0 0 0 0 0 0 0 0]\n",
      "Some labels in set 2: [100 100 100 100 100 100 100 100 100 100]\n",
      "Minimum and maximum amount of sample per classes in the dataset\n",
      "Each classes contains at least 20 samples\n",
      "Each classes contains at most 20 samples\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of training samples from each data handler.\n",
    "# mnist_inps, mnist_trgts = mnist.next_train_batch(4)\n",
    "# dataset_inps, dataset_trgts = dataset.next_train_batch(4)\n",
    "# dataset_full, dataset_full_lbl = dataset.next_train_batch(60000)\n",
    "print(dataset_lbl)\n",
    "\n",
    "n_classes = len(np.unique(dataset_lbl))\n",
    "dataset_full = dataset.reshape((dataset.shape[0], dataset.shape[2]*dataset.shape[3]))\n",
    "dataset_full_lbl = dataset_lbl\n",
    "\n",
    "print(dataset_full.shape)\n",
    "print(dataset_full_lbl.shape)\n",
    "\n",
    "sep = 100\n",
    "lbls_0 = [i for i in range(sep)]\n",
    "lbls_1 = [i for i in range(sep, n_classes)]\n",
    "\n",
    "mask_0 = np.isin(dataset_full_lbl, np.array(lbls_0))\n",
    "mask_1 = np.isin(dataset_full_lbl, np.array(lbls_1))\n",
    "dataset_0, dataset_0_lbl = dataset_full[mask_0], dataset_full_lbl[mask_0]\n",
    "\n",
    "print(\"Shape of the dataset_0:\",dataset_0.shape)\n",
    "\n",
    "dataset_1, dataset_1_lbl = dataset_full[mask_1], dataset_full_lbl[mask_1]\n",
    "\n",
    "print(\"Shape of the dataset_1:\",dataset_1.shape)\n",
    "\n",
    "print(\"Some labels in set 1:\", dataset_0_lbl[0:10])\n",
    "print(\"Some labels in set 2:\", dataset_1_lbl[0:10])\n",
    "assert(np.all(np.isin(dataset_0_lbl, lbls_0)))\n",
    "assert(np.all(np.isin(dataset_1_lbl, lbls_1)))\n",
    "\n",
    "# mnist.plot_samples('MNIST Examples', mnist_inps, outputs=mnist_trgts)\n",
    "# dataset.plot_samples('FashionMNIST Examples with lbl < sep', dataset_0[0:4], outputs=dataset_0_lbl[0:4])\n",
    "# dataset.plot_samples('FashionMNIST Examples with lbl >= sep', dataset_1[0:4], outputs=dataset_1_lbl[0:4])\n",
    "\n",
    "torch_dataset = torch.tensor(dataset_full_lbl)\n",
    "unique_values, counts = torch.unique(torch_dataset, return_counts=True)\n",
    "\n",
    "print(\"Minimum and maximum amount of sample per classes in the dataset\")\n",
    "print(\"Each classes contains at least\", torch.min(counts).item(), \"samples\")\n",
    "print(\"Each classes contains at most\", torch.max(counts).item(), \"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4e57d",
   "metadata": {},
   "source": [
    "### Compute a pgd attack on test set to assert robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0badd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelCNN(nn.Module):\n",
    "    def __init__(self, z_length):\n",
    "        super(KernelCNN, self).__init__()\n",
    "        self.z_length = z_length\n",
    "        resnet18 = models.resnet18(pretrained=False)\n",
    "        resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), bias=False)\n",
    "        resnet18.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, self.z_length)\n",
    "        self.resnet = resnet18\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return self.resnet(x)\n",
    "\n",
    "def compute_kernel(X, y, cnn, K):\n",
    "    \"\"\"\n",
    "    Compute Hypershot kernel for a support set X and label y\n",
    "    It takes the average of the z's for each label as suggested in the Hypershot paper\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): Support set used to compute the kernel\n",
    "        y (tensor): corresponding labels\n",
    "        cnn : CNN used to compute the embeddings\n",
    "        K: the K of K-shot K-way learning\n",
    "\n",
    "    Returns:\n",
    "        type: embeddings, kernel\n",
    "    \"\"\"\n",
    "    # Obtain the indices that would sort y_test\n",
    "    indices = torch.argsort(y)\n",
    "\n",
    "    # Use the indices to sort the rows of X_test\n",
    "    sorted_X = X[indices].to(device)\n",
    "    sorted_y = y[indices].to(device)\n",
    "    \n",
    "    reshaped_X = sorted_X.view(sorted_X.shape[0], 1, 28, 28).to(device)\n",
    "    nn_X = cnn(reshaped_X)\n",
    "    \n",
    "    mean_X = torch.zeros((int(nn_X.shape[0] / K), nn_X.shape[1])).to(device)\n",
    "    for i in range(K):\n",
    "        mean_X[i] = torch.mean(nn_X[i*K:(i+1)*K], dim = 0)\n",
    "    \n",
    "    assert(nn_X.shape==(sorted_X.shape[0], cnn.z_length))\n",
    "    \n",
    "    return mean_X, torch.matmul(mean_X, torch.t(mean_X))\n",
    "\n",
    "def get_s_and_q_sets(X, y, trgt_lbls, K, q_size):\n",
    "    \"\"\"\n",
    "    Computes a support set for data X for classes in y with K sample per classes\n",
    "    and corresponding query sets of size q_size.\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): Data used to compute the sets (can contain label you do not want for your sets)\n",
    "        y (tensor): corresponding labels\n",
    "        trgt_lbls : the labels that end up in the sets\n",
    "        K: the K of K-shot K-way learning\n",
    "        q_size: amount of sample per classes in query set\n",
    "\n",
    "    Returns:\n",
    "        type: support set, support set labels, query set, query set labels\n",
    "    \"\"\"\n",
    "    \n",
    "    s_set = np.zeros((len(trgt_lbls) * K, X.shape[1]))\n",
    "    s_set_lbl = np.zeros((len(trgt_lbls) * K))\n",
    "    \n",
    "    q_set = np.zeros((len(trgt_lbls) * q_size, X.shape[1]))\n",
    "    q_set_lbl = np.zeros((len(trgt_lbls) * q_size))\n",
    "    \n",
    "    for j, l in enumerate(trgt_lbls):\n",
    "        mask = (y == l)\n",
    "        masked_data = X[mask]\n",
    "        masked_lbls = y[mask]\n",
    "        s_set[j*K:(j+1)*K] = masked_data[0:K]\n",
    "        s_set_lbl[j*K:(j+1)*K] = masked_lbls[0:K]\n",
    "        q_set[j*q_size:(j+1)*q_size] = masked_data[K:K+q_size]\n",
    "        q_set_lbl[j*q_size:(j+1)*q_size] = masked_lbls[K:K+q_size]\n",
    "    \n",
    "    s_set = torch.tensor(s_set, requires_grad=True).to(device).float()\n",
    "    s_set_lbl = torch.tensor(s_set_lbl, requires_grad=True).to(device).float()\n",
    "    q_set = torch.tensor(q_set, requires_grad=True).to(device).float()\n",
    "    q_set_lbl = torch.tensor(q_set_lbl, requires_grad=True).to(device).float()\n",
    "    \n",
    "    return s_set, s_set_lbl, q_set, q_set_lbl\n",
    "\n",
    "def get_q_sample_features(X, cnn, kernel, zs):\n",
    "    \"\"\"\n",
    "    Computes the final features used for classification, given a query sample mx\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): query sample \n",
    "        cnn: the cnn trained to compute the desired features\n",
    "        kernel: the kernel corresponding to the corresponding X's support set\n",
    "        zs: z space of the support set corresponding to the query sample\n",
    "\n",
    "    Returns:\n",
    "        type: final flattened features use by the main network\n",
    "    \"\"\"\n",
    "    X = X.view(1, 28, 28)\n",
    "    zs_q = cnn(X)\n",
    "    zs_q_m = torch.matmul(zs, torch.t(zs_q))\n",
    "    # This could be modified, the features are just the concatenation of the kernel and the q_sample multiplied\n",
    "    # by the z_space of the support set\n",
    "    q_features = zs_q_m.flatten()\n",
    "    return q_features\n",
    "\n",
    "def compute_sets_and_features(X, y, trgt_lbls, cnn, K, q_size):\n",
    "    s_set, s_set_lbl, q_set, q_set_lbl = get_s_and_q_sets(X, y, trgt_lbls, K, q_size)\n",
    "\n",
    "    # Kernel computation\n",
    "    z_space, kernel = compute_kernel(s_set, s_set_lbl, cnn, K)\n",
    "\n",
    "    # Gather features for all samples in query training set\n",
    "    all_q_features = torch.zeros((q_set.shape[0], K)).to(device)\n",
    "    all_q_features_lbls = torch.zeros((q_set.shape[0])).to(device)\n",
    "    for i in range(q_set.shape[0]):\n",
    "        x = q_set[i].view(1, -1)\n",
    "        q_sample_features = get_q_sample_features(x, cnn, kernel, z_space)\n",
    "        all_q_features[i] = q_sample_features\n",
    "        all_q_features_lbls[i] = q_set_lbl[i]\n",
    "        \n",
    "    return s_set, s_set_lbl, q_set, q_set_lbl, z_space, kernel, all_q_features, all_q_features_lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bafc56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(x_adv, x_orig):\n",
    "    epsilon = 8/255.0\n",
    "    x_adv_eps = torch.minimum(torch.maximum(x_adv, x_orig-epsilon), x_orig+epsilon)\n",
    "    return torch.clamp(x_adv_eps, 0, 1)\n",
    "\n",
    "def pgd_attack_data(X, y, t_mnet, t_hnet, K, cnn, kernel, zs):\n",
    "    criterion = nn.CrossEntropyLoss()    \n",
    "    x_adv = torch.clone(X).detach()\n",
    "   \n",
    "    for i in range(20):\n",
    "        x_adv = x_adv.requires_grad_(True)\n",
    "        x_features = torch.zeros((x_adv.shape[0], K)).to(device)\n",
    "        x_features_lbls = torch.zeros((x_adv.shape[0])).to(device)\n",
    "        for j in range(x_adv.shape[0]):\n",
    "            mx = x_adv[j].view(-1, X.shape[1])\n",
    "            x_sample_features = get_q_sample_features(mx, cnn, kernel, zs)\n",
    "            x_features[j] = x_sample_features\n",
    "            x_features_lbls[j] = y[j]\n",
    "            \n",
    "        # Apply to test set\n",
    "        W_mnet = t_hnet(cond_id=0)\n",
    "        logits = t_mnet.forward(x_features, weights=W_mnet)\n",
    "        loss_adv = criterion(logits, x_features_lbls.long())\n",
    "        loss_adv.backward(retain_graph=True)\n",
    "        \n",
    "        grad = x_adv.grad.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_adv = x_adv + 0.1 * torch.sign(grad)  # take a gradient update step to minimize the objective\n",
    "            x_adv = project(x_adv, X)               # ensure we stay in the allowed range\n",
    "            \n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbdcd4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_lbls(X_test, y_test, test_classes, hnet, mnet, Ks, s_cnn):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy for the sample with label test_classes in X_test.\n",
    "    Mainly used as utility for the calc_accuracy function below.\n",
    "    \n",
    "    Args:\n",
    "        X_test (tensor): entire test set\n",
    "        y_test (tensor): corresponding labels\n",
    "        test_classes: the classes we want to consider for testing accuracies (should contain Ks classes)\n",
    "        mnet : main net trained by the hypernetwork\n",
    "        Ks: the K of K-shot K-way\n",
    "        s_cnn: the cnn trained to compute the desired features\n",
    "\n",
    "    Returns:\n",
    "        type: accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test = get_s_and_q_sets(X_test, y_test, \\\n",
    "                                                                                 test_classes, Ks, 5)  \n",
    "        z_space, K = compute_kernel(s_set_test, s_set_lbl_test, s_cnn, Ks)\n",
    "        \n",
    "        # Accuracy\n",
    "        all_q_features = torch.zeros((q_set_test.shape[0], Ks)).to(device)\n",
    "        all_q_features_lbls = torch.zeros((q_set_test.shape[0])).to(device)\n",
    "        for i in range(q_set_test.shape[0]):\n",
    "            mx = q_set_test[i].view(-1, q_set_test.shape[1])\n",
    "            q_sample_features = get_q_sample_features(mx, s_cnn, K, z_space)\n",
    "            all_q_features[i] = q_sample_features\n",
    "            all_q_features_lbls[i] = q_set_lbl_test[i]\n",
    "\n",
    "        W_dataset_l = W_dataset_l = hnet(uncond_input=K.view(1, -1))\n",
    "        dataset_l_P = mnet.forward(all_q_features, weights=W_dataset_l)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(dataset_l_P, all_q_features_lbls.long())\n",
    "        accuracy = (torch.argmax(dataset_l_P,dim=1) == all_q_features_lbls.long()).float().mean().item()\n",
    "        # print(\"Correctly predicted samples had labels:\", all_q_features_lbls[torch.argmax(dataset_l_P,dim=1) == all_q_features_lbls.long()])\n",
    "    return accuracy, loss.item()\n",
    "\n",
    "\n",
    "def calc_accuracy(X_test, y_test, hnet, mnet, Ks, s_cnn):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy for the entire X_test test set.\n",
    "    \n",
    "    Args:\n",
    "        X_test (tensor): entire test set\n",
    "        y_test (tensor): corresponding labels\n",
    "        mnet : main net trained by the hypernetwork\n",
    "        Ks: the K of K-shot K-way\n",
    "        s_cnn: the cnn trained to compute the desired features\n",
    "\n",
    "    Returns:\n",
    "        type: average accuracy over all the label batch (of Ks different labels each time)\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(X_test):\n",
    "        X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    else: \n",
    "        X_test_t = torch.clone(X_test)\n",
    "        \n",
    "    if not torch.is_tensor(y_test):\n",
    "        y_test_t = torch.FloatTensor(y_test).to(device)\n",
    "    else:\n",
    "        y_test_t = torch.clone(y_test)\n",
    "        \n",
    "    diff_classes = torch.unique(y_test_t)\n",
    "    n_diff_classes = diff_classes.shape[0]\n",
    "    n_sets = int(n_diff_classes / Ks)\n",
    "    acc, loss = 0.0, 0.0\n",
    "    for i in range(n_sets):\n",
    "        lbls = diff_classes[i*Ks:(i+1)*Ks].tolist()\n",
    "        d_acc, d_loss = calc_accuracy_lbls(X_test, y_test, lbls, hnet, mnet, Ks, s_cnn)\n",
    "        acc += d_acc\n",
    "        loss += d_loss\n",
    "    acc = acc / n_sets\n",
    "    loss = loss / n_sets\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "437afb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_lbls_adv(X_test, y_test, test_classes, mnet, Ks, s_cnn, q_set_test_adv):\n",
    "    \"\"\"\n",
    "    Same as the calc_accuracy_lbls function but replace the query set with an attacked version of itself.\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test = get_s_and_q_sets(X_test, y_test, \\\n",
    "                                                                                 test_classes, Ks, 5) \n",
    "        q_set_test = q_set_test_adv\n",
    "        z_space, K = compute_kernel(s_set_test, s_set_lbl_test, s_cnn, Ks)\n",
    "        \n",
    "        # Accuracy\n",
    "        all_q_features = torch.zeros((q_set_test.shape[0], Ks)).to(device)\n",
    "        all_q_features_lbls = torch.zeros((q_set_test.shape[0])).to(device)\n",
    "        for i in range(q_set_test.shape[0]):\n",
    "            mx = q_set_test[i].view(-1, q_set_test.shape[1])\n",
    "            my = torch.argmax(q_set_lbl_test[i])\n",
    "            q_sample_features = get_q_sample_features(mx, s_cnn, K, z_space)\n",
    "            all_q_features[i] = q_sample_features\n",
    "            all_q_features_lbls[i] = my\n",
    "\n",
    "        W_dataset_l = hnet(cond_id=0)\n",
    "        dataset_l_P = mnet.forward(all_q_features, weights=W_dataset_l)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(dataset_l_P, all_q_features_lbls.long())\n",
    "        accuracy = (torch.argmax(dataset_l_P,dim=1) == all_q_features_lbls.long()).float().mean().item()\n",
    "    return accuracy, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd4014d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Creating an MLP with 30 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 1570 weights and 30 outputs (compression ratio: 52.33).\n",
      "The network consists of 1570 unconditional weights (1570 internally maintained) and 0 conditional weights (0 internally maintained).\n",
      "----------------------- Epoch 0  -----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quent\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\quent\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 26.457304000854492)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 24.187946319580078)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 100\u001b[0m\n\u001b[0;32m     97\u001b[0m     loss_dataset_l_adv \u001b[38;5;241m=\u001b[39m criterion(dataset_l_P_adv, all_q_features_lbls\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     98\u001b[0m     loss_dataset_l \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_dataset_l_adv\n\u001b[1;32m--> 100\u001b[0m global_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_dataset_l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l_set_id \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocal train acc and loss at the end of set:\u001b[39m\u001b[38;5;124m\"\u001b[39m, l_set_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-->\u001b[39m\u001b[38;5;124m\"\u001b[39m, calc_accuracy(dataset_l_train, dataset_l_lbl_train, hnet, mnet,\\\n\u001b[0;32m    103\u001b[0m                                                                            Ks, kcnn))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure training.\n",
    "nepochs=250\n",
    "# epoch after which adversarial training starts\n",
    "do_adv_train = 10000\n",
    "# K-shot k-way\n",
    "Ks = 5\n",
    "# Length of the embeddings produced by the CNN\n",
    "z_len = 10\n",
    "\n",
    "load_weights = 0\n",
    "continue_training = 0\n",
    "\n",
    "# Array storing statistics (not used for now)\n",
    "accuracies_dataset_0 = []\n",
    "accuracies_dataset_0_adv = []\n",
    "accuracies_dataset_1 = []\n",
    "accuracies_dataset_1_adv = []\n",
    "\n",
    "# Loop in case we want to do statistics (not sued for now)\n",
    "for o in range(1):\n",
    "    print(\"Iteration\", o+1)\n",
    "    \n",
    "    if continue_training == 0:\n",
    "        # Models definition\n",
    "        mnet = MLP(n_in=Ks, n_out=n_classes, hidden_layers=[]).to(device)\n",
    "        hnet = HMLP(mnet.param_shapes, uncond_in_size=Ks*Ks, cond_in_size=0,\n",
    "                    layers=[20, 20], num_cond_embs=0).to(device)\n",
    "        params = hnet.conditional_params.copy()\n",
    "        hnet.apply_hyperfan_init(mnet=mnet)\n",
    "        kcnn = KernelCNN(z_len).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # If we want to load weights from anywhere\n",
    "        if load_weights == 1:\n",
    "            file_path = 'models/hnet_20231229022719_49.pth'\n",
    "            hnet.load_state_dict(torch.load(file_path))\n",
    "            file_path = 'models/kcnn_20231229022719_49.pth'\n",
    "            kcnn.load_state_dict(torch.load(file_path))\n",
    "\n",
    "        # The amount of sets of Ks labels we can do during training\n",
    "        n_sets = int(len(lbls_0) / Ks)\n",
    "\n",
    "        # Compute training and validation sets for each of the n_sets labels sets\n",
    "        train_test_sets = []\n",
    "        all_test_sets = np.empty((0, dataset_0.shape[1]))\n",
    "        all_test_sets_lbl = np.empty((0))\n",
    "        for l_set_id in range(n_sets):\n",
    "            c_lbls = lbls_0[l_set_id*Ks:(l_set_id+1)*Ks]\n",
    "            if (l_set_id+1) % 100 == 0:\n",
    "                print(\"Generated train-test split for\", l_set_id+1,\"/\",n_sets)\n",
    "            mask_b = np.isin(dataset_0_lbl, np.array(c_lbls))\n",
    "            dataset_0_b, dataset_0_lbl_b = dataset_0[mask_b], dataset_0_lbl[mask_b]\n",
    "            dataset_0_train, dataset_0_test, dataset_0_lbl_train, dataset_0_lbl_test = \\\n",
    "                            train_test_split(dataset_0_b, dataset_0_lbl_b, random_state=42, test_size=0.5, stratify=dataset_0_lbl_b)\n",
    "            all_test_sets = np.concatenate((all_test_sets, dataset_0_test), axis=0)\n",
    "            all_test_sets_lbl = np.concatenate((all_test_sets_lbl, dataset_0_lbl_test), axis=0)\n",
    "            train_test_sets.append((dataset_0_train, dataset_0_test, dataset_0_lbl_train, dataset_0_lbl_test, c_lbls))\n",
    "    \n",
    "    # Optimizer and scheduler initialization\n",
    "    optimizer = optim.Adam(hnet.parameters(), lr=0.00001)\n",
    "    optimizer_s = optim.Adam(kcnn.parameters(), lr=0.00001)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=nepochs, eta_min=0.000001)\n",
    "    scheduler_s = CosineAnnealingLR(optimizer_s, T_max=nepochs, eta_min=0.000001)\n",
    "        \n",
    "    # Main training loop\n",
    "    for epoch in range(nepochs): # For each epoch.\n",
    "        print(\"----------------------- Epoch\", epoch, \" -----------------------\")\n",
    "        # Stores the loss over all labels sets\n",
    "        global_loss = 0.0\n",
    "        # We loop over all our sets at each epoch\n",
    "        for l_set_id in range(n_sets):\n",
    "            (dataset_l_train, dataset_l_test, dataset_l_lbl_train, dataset_l_lbl_test, c_lbls) = train_test_sets[l_set_id]\n",
    "            \n",
    "            s_set_train, s_set_lbl_train, q_set_train, q_set_lbl_train, z_space, K, all_q_features, all_q_features_lbls = \\\n",
    "            compute_sets_and_features(dataset_l_train, dataset_l_lbl_train, c_lbls, kcnn, Ks, 5)\n",
    "            \n",
    "            # Formward pass\n",
    "            W_dataset_l = hnet(uncond_input=K.view(1, -1))\n",
    "            dataset_l_P = mnet.forward(all_q_features, weights=W_dataset_l)\n",
    "            loss_dataset_l = criterion(dataset_l_P, all_q_features_lbls.long())\n",
    "\n",
    "            # Adversarial training\n",
    "            if epoch == do_adv_train and l_set_id == 0:\n",
    "                print(\"Adversarial training starts.\")\n",
    "            if epoch >= do_adv_train:\n",
    "                mx_adv = pgd_attack_data(q_set_train, q_set_lbl_train, mnet, hnet, Ks, kcnn, K, z_space)\n",
    "                \n",
    "                all_q_features_adv = torch.zeros((q_set_train.shape[0], Ks)).to(device)\n",
    "                for i in range(mx_adv.shape[0]):\n",
    "                    mxx = mx_adv[i].view(-1, q_set_train.shape[1])\n",
    "                    q_sample_features_adv = get_q_sample_features(mxx, kcnn, K, z_space)\n",
    "                    all_q_features_adv[i] = q_sample_features_adv\n",
    "                \n",
    "                dataset_l_P_adv = mnet.forward(all_q_features_adv, weights=W_dataset_l)\n",
    "                loss_dataset_l_adv = criterion(dataset_l_P_adv, all_q_features_lbls.long())\n",
    "                loss_dataset_l += loss_dataset_l_adv\n",
    "            \n",
    "            global_loss += loss_dataset_l.item()\n",
    "            if l_set_id % 50 == 0:\n",
    "                print(\"Local train acc and loss at the end of set:\", l_set_id, \"-->\", calc_accuracy(dataset_l_train, dataset_l_lbl_train, hnet, mnet,\\\n",
    "                                                                                       Ks, kcnn))\n",
    "                print(\"Local test acc and loss at the end of set:\", l_set_id, \"-->\", calc_accuracy(dataset_l_test, dataset_l_lbl_test, hnet, mnet,\\\n",
    "                                                                                       Ks, kcnn))\n",
    "                if do_adv_train < nepochs:\n",
    "                    s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test, z_space_tes, K_test, all_q_features_test, all_q_features_lbls_test = \\\n",
    "                    compute_sets_and_features(dataset_l_test, dataset_l_lbl_test, c_lbls, kcnn, Ks, 5)\n",
    "                    mx_adv_test = pgd_attack_data(q_set_test, q_set_lbl_test, mnet, hnet, Ks, kcnn, K, z_space)\n",
    "                    print(\"Local adv test acc and loss at the end of set:\", l_set_id, \"-->\", calc_accuracy_lbls_adv(dataset_l_test, dataset_l_lbl_test, c_lbls, hnet, mnet,\\\n",
    "                                                                                           Ks, kcnn, mx_adv_test))\n",
    "            loss_dataset_l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer_s.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            scheduler_s.step()\n",
    "                \n",
    "  \n",
    "        print(\"Global loss at the end of epoch:\", epoch, \":\", global_loss)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            # Create a file name with the current time\n",
    "            hnet_file = f'models/hnet_{current_time}_{epoch}.pth'\n",
    "            torch.save(hnet.state_dict(), hnet_file)\n",
    "            kcnn_file = f'models/kcnn_{current_time}_{epoch}.pth'\n",
    "            torch.save(kcnn.state_dict(), kcnn_file)\n",
    "            print(\"--> Global test accuracy after epoch:\", epoch, \"-->\", calc_accuracy(all_test_sets, all_test_sets_lbl,\\\n",
    "                                                                                       hnet, mnet, Ks, kcnn))\n",
    "        print()\n",
    "\n",
    "    print(\"END OF ITERATION:\",o+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd33649",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Create a file name with the current time\n",
    "hnet_file = f'models/hnet_{current_time}.pth'\n",
    "torch.save(hnet.state_dict(), hnet_file)\n",
    "kcnn_file = f'models/kcnn_{current_time}.pth'\n",
    "torch.save(kcnn.state_dict(), kcnn_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145607b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_adv_dataset_1 = pgd_attack_data(dataset_1, dataset_1_lbl, mnet, hnet, z_space_1, K_1, 1)\n",
    "# x_adv_dataset_1_np = x_adv_dataset_1.detach().cpu().numpy()\n",
    "# x_adv_dataset_0_test = pgd_attack_data(dataset_0_test, dataset_0_lbl_test, mnet, hnet, z_space, K, 0)\n",
    "# x_adv_dataset_0_test_np = x_adv_dataset_0_test.detach().cpu().numpy()\n",
    "\n",
    "print(calc_accuracy(dataset_0_test, dataset_0_lbl_test, mnet, W_dataset_0, Ks, s_cnn))\n",
    "print(calc_accuracy(dataset_1, dataset_1_lbl, mnet, W_dataset_0, Ks, s_cnn))\n",
    "# accuracies_dataset_0_adv.append((calc_accuracy(x_adv_dataset_0_test_np, dataset_0_lbl_test, mnet, W_dataset_0)).detach().cpu())\n",
    "# accuracies_dataset_1_adv.append((calc_accuracy(x_adv_dataset_1_np, dataset_1_lbl, mnet, W_dataset_1)).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045661ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean:\")\n",
    "print(\"dataset 0 accuracy:\", np.mean(np.array(accuracies_dataset_0)))\n",
    "print(\"dataset 1 accuracy:\", np.mean(np.array(accuracies_dataset_1)))\n",
    "print(\"dataset 0 adv accuracy:\", np.mean(np.array(accuracies_dataset_0_adv)))\n",
    "print(\"dataset 1 adv accuracy:\", np.mean(np.array(accuracies_dataset_1_adv)))\n",
    "print()\n",
    "print(\"Standard deviation:\")\n",
    "print(\"dataset 0 accuracy:\", np.std(np.array(accuracies_dataset_0)))\n",
    "print(\"dataset 1 accuracy:\", np.std(np.array(accuracies_dataset_1)))\n",
    "print(\"dataset 0 adv accuracy:\", np.std(np.array(accuracies_dataset_0_adv)))\n",
    "print(\"dataset 1 adv accuracy:\", np.std(np.array(accuracies_dataset_1_adv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2e88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
