{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a3ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use: cuda\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Omniglot\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Will use:\", device)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8780cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29600a",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fae8e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading MNIST dataset ...\n",
      "Elapsed time to read dataset: 0.122000 sec\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from hypnettorch.data import FashionMNISTData, MNISTData\n",
    "from hypnettorch.data.dataset import Dataset\n",
    "from hypnettorch.mnets import LeNet\n",
    "from hypnettorch.mnets.resnet import ResNet\n",
    "from hypnettorch.mnets.mlp import MLP\n",
    "from hypnettorch.hnets import HMLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import learn2learn as l2l\n",
    "import copy\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mnist = MNISTData(data_dir, use_one_hot=True, validation_size=0)\n",
    "fmnist = FashionMNISTData(data_dir, use_one_hot=True, validation_size=0)\n",
    "\n",
    "omniglot = l2l.vision.datasets.FullOmniglot(root=data_dir,\n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.Resize(28, interpolation=Image.LANCZOS),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                lambda x: 1.0 - x,\n",
    "                                            ]),\n",
    "                                            download=True)\n",
    "omniglot = l2l.data.MetaDataset(omniglot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e95df",
   "metadata": {},
   "source": [
    "## Convert the dataset to numpy for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf2a597",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension: (32460, 1, 28, 28)\n",
      "Labels dimension: (32460,)\n",
      "0\n",
      "1622\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for batching and shuffling the data\n",
    "batch_size = len(omniglot)  # Set batch size to the total number of examples to load all data at once\n",
    "data_loader = DataLoader(omniglot, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in data_loader:\n",
    "    images, labels = batch\n",
    "    # Convert PyTorch tensors to NumPy arrays\n",
    "    dataset = images.numpy()\n",
    "    dataset_lbl = labels.numpy()    \n",
    "    sizes = dataset.shape\n",
    "    \n",
    "print(\"Dataset dimension:\", dataset.shape)\n",
    "print(\"Labels dimension:\", dataset_lbl.shape)\n",
    "print(np.min(dataset_lbl))\n",
    "print(np.max(dataset_lbl))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48adcc",
   "metadata": {},
   "source": [
    "## Create 2 different datasets for two disjoint set of labels (deterministic for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe5509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0 ... 1622 1622 1622]\n",
      "(32460, 784)\n",
      "(32460,)\n",
      "Shape of the dataset_0: (2000, 784)\n",
      "Shape of the dataset_1: (30460, 784)\n",
      "Some labels in set 1: [0 0 0 0 0 0 0 0 0 0]\n",
      "Some labels in set 2: [100 100 100 100 100 100 100 100 100 100]\n",
      "Minimum and maximum amount of sample per classes in the dataset\n",
      "Each classes contains at least 20 samples\n",
      "Each classes contains at most 20 samples\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of training samples from each data handler.\n",
    "# mnist_inps, mnist_trgts = mnist.next_train_batch(4)\n",
    "# dataset_inps, dataset_trgts = dataset.next_train_batch(4)\n",
    "# dataset_full, dataset_full_lbl = dataset.next_train_batch(60000)\n",
    "print(dataset_lbl)\n",
    "\n",
    "n_classes = len(np.unique(dataset_lbl))\n",
    "dataset_full = dataset.reshape((dataset.shape[0], dataset.shape[2]*dataset.shape[3]))\n",
    "dataset_full_lbl = dataset_lbl\n",
    "\n",
    "print(dataset_full.shape)\n",
    "print(dataset_full_lbl.shape)\n",
    "\n",
    "sep = 100\n",
    "lbls_0 = [i for i in range(sep)]\n",
    "lbls_1 = [i for i in range(sep, n_classes)]\n",
    "\n",
    "mask_0 = np.isin(dataset_full_lbl, np.array(lbls_0))\n",
    "mask_1 = np.isin(dataset_full_lbl, np.array(lbls_1))\n",
    "dataset_0, dataset_0_lbl = dataset_full[mask_0], dataset_full_lbl[mask_0]\n",
    "\n",
    "print(\"Shape of the dataset_0:\",dataset_0.shape)\n",
    "\n",
    "dataset_1, dataset_1_lbl = dataset_full[mask_1], dataset_full_lbl[mask_1]\n",
    "\n",
    "print(\"Shape of the dataset_1:\",dataset_1.shape)\n",
    "\n",
    "print(\"Some labels in set 1:\", dataset_0_lbl[0:10])\n",
    "print(\"Some labels in set 2:\", dataset_1_lbl[0:10])\n",
    "assert(np.all(np.isin(dataset_0_lbl, lbls_0)))\n",
    "assert(np.all(np.isin(dataset_1_lbl, lbls_1)))\n",
    "\n",
    "# mnist.plot_samples('MNIST Examples', mnist_inps, outputs=mnist_trgts)\n",
    "# dataset.plot_samples('FashionMNIST Examples with lbl < sep', dataset_0[0:4], outputs=dataset_0_lbl[0:4])\n",
    "# dataset.plot_samples('FashionMNIST Examples with lbl >= sep', dataset_1[0:4], outputs=dataset_1_lbl[0:4])\n",
    "\n",
    "torch_dataset = torch.tensor(dataset_full_lbl)\n",
    "unique_values, counts = torch.unique(torch_dataset, return_counts=True)\n",
    "\n",
    "print(\"Minimum and maximum amount of sample per classes in the dataset\")\n",
    "print(\"Each classes contains at least\", torch.min(counts).item(), \"samples\")\n",
    "print(\"Each classes contains at most\", torch.max(counts).item(), \"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4e57d",
   "metadata": {},
   "source": [
    "### Compute a pgd attack on test set to assert robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0badd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelCNN(nn.Module):\n",
    "    def __init__(self, z_length):\n",
    "        super(KernelCNN, self).__init__()\n",
    "        self.z_length = z_length\n",
    "        resnet18 = models.resnet18(pretrained=False)\n",
    "        resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), bias=False)\n",
    "        resnet18.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, self.z_length)\n",
    "        self.resnet = resnet18\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return self.resnet(x)\n",
    "    \n",
    "def standardize(t):\n",
    "    mean = torch.mean(t)\n",
    "    std_dev = torch.std(t)\n",
    "    return (t - mean) / std_dev\n",
    "\n",
    "def compute_kernel(X, y, cnn, K):\n",
    "    \"\"\"\n",
    "    Compute Hypershot kernel for a support set X and label y\n",
    "    It takes the average of the z's for each label as suggested in the Hypershot paper\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): Support set used to compute the kernel\n",
    "        y (tensor): corresponding labels\n",
    "        cnn : CNN used to compute the embeddings\n",
    "        K: the K of K-shot K-way learning\n",
    "\n",
    "    Returns:\n",
    "        type: embeddings, kernel\n",
    "    \"\"\"\n",
    "    # Obtain the indices that would sort y_test\n",
    "    indices = torch.argsort(y)\n",
    "\n",
    "    # Use the indices to sort the rows of X_test\n",
    "    sorted_X = X[indices].to(device)\n",
    "    sorted_y = y[indices].to(device)\n",
    "    \n",
    "    reshaped_X = sorted_X.view(sorted_X.shape[0], 1, 28, 28).to(device)\n",
    "    nn_X = cnn(reshaped_X)\n",
    "    \n",
    "    mean_X = torch.zeros((int(nn_X.shape[0] / K), nn_X.shape[1])).to(device)\n",
    "    for i in range(K):\n",
    "        mean_X[i] = torch.mean(nn_X[i*K:(i+1)*K], dim = 0)\n",
    "    \n",
    "    assert(nn_X.shape==(sorted_X.shape[0], cnn.z_length))\n",
    "    \n",
    "    return standardize(mean_X), standardize(torch.matmul(mean_X, torch.t(mean_X)))\n",
    "\n",
    "def get_s_and_q_sets(X, y, trgt_lbls, K, q_size):\n",
    "    \"\"\"\n",
    "    Computes a support set for data X for classes in y with K sample per classes\n",
    "    and corresponding query sets of size q_size.\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): Data used to compute the sets (can contain label you do not want for your sets)\n",
    "        y (tensor): corresponding labels\n",
    "        trgt_lbls : the labels that end up in the sets\n",
    "        K: the K of K-shot K-way learning\n",
    "        q_size: amount of sample per classes in query set\n",
    "\n",
    "    Returns:\n",
    "        type: support set, support set labels, query set, query set labels\n",
    "    \"\"\"\n",
    "    \n",
    "    s_set = np.zeros((len(trgt_lbls) * K, X.shape[1]))\n",
    "    s_set_lbl = np.zeros((len(trgt_lbls) * K))\n",
    "    \n",
    "    q_set = np.zeros((len(trgt_lbls) * q_size, X.shape[1]))\n",
    "    q_set_lbl = np.zeros((len(trgt_lbls) * q_size))\n",
    "    \n",
    "    for j, l in enumerate(trgt_lbls):\n",
    "        mask = (y == l)\n",
    "        masked_data = X[mask]\n",
    "        masked_lbls = y[mask]\n",
    "        s_set[j*K:(j+1)*K] = masked_data[0:K]\n",
    "        s_set_lbl[j*K:(j+1)*K] = masked_lbls[0:K]\n",
    "        q_set[j*q_size:(j+1)*q_size] = masked_data[K:K+q_size]\n",
    "        q_set_lbl[j*q_size:(j+1)*q_size] = masked_lbls[K:K+q_size]\n",
    "    \n",
    "    s_set = torch.tensor(s_set, requires_grad=True).to(device).float()\n",
    "    s_set_lbl = torch.tensor(s_set_lbl, requires_grad=True).to(device).float()\n",
    "    q_set = torch.tensor(q_set, requires_grad=True).to(device).float()\n",
    "    q_set_lbl = torch.tensor(q_set_lbl, requires_grad=True).to(device).float()\n",
    "    \n",
    "    return s_set, s_set_lbl, q_set, q_set_lbl\n",
    "\n",
    "def get_q_sample_features(X, cnn, kernel, zs):\n",
    "    \"\"\"\n",
    "    Computes the final features used for classification, given a query sample mx\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): query sample \n",
    "        cnn: the cnn trained to compute the desired features\n",
    "        kernel: the kernel corresponding to the corresponding X's support set\n",
    "        zs: z space of the support set corresponding to the query sample\n",
    "\n",
    "    Returns:\n",
    "        type: final flattened features use by the main network\n",
    "    \"\"\"\n",
    "    X = X.view(1, 28, 28)\n",
    "    zs_q = cnn(X)\n",
    "    zs_q_m = standardize(torch.matmul(zs, torch.t(zs_q)))\n",
    "    # This could be modified, the features are just the concatenation of the kernel and the q_sample multiplied\n",
    "    # by the z_space of the support set\n",
    "    q_features = zs_q_m.flatten()\n",
    "    return q_features\n",
    "\n",
    "def compute_sets_and_features(X, y, trgt_lbls, cnn, K, q_size):\n",
    "    s_set, s_set_lbl, q_set, q_set_lbl = get_s_and_q_sets(X, y, trgt_lbls, K, q_size)\n",
    "\n",
    "    # Kernel computation\n",
    "    z_space, kernel = compute_kernel(s_set, s_set_lbl, cnn, K)\n",
    "\n",
    "    # Gather features for all samples in query training set\n",
    "    all_q_features = torch.zeros((q_set.shape[0], K)).to(device)\n",
    "    all_q_features_lbls = torch.zeros((q_set.shape[0])).to(device)\n",
    "    for i in range(q_set.shape[0]):\n",
    "        x = q_set[i].view(1, -1)\n",
    "        q_sample_features = get_q_sample_features(x, cnn, kernel, z_space)\n",
    "        all_q_features[i] = q_sample_features\n",
    "        all_q_features_lbls[i] = q_set_lbl[i]\n",
    "        \n",
    "    return s_set, s_set_lbl, q_set, q_set_lbl, z_space, kernel, all_q_features, all_q_features_lbls\n",
    "\n",
    "def extend_pred_to_nclasses(pred, n_c, lbls):\n",
    "    out = torch.zeros((pred.shape[0], n_classes)).to(device)\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i][c_lbls] = pred[i]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bafc56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(x_adv, x_orig):\n",
    "    epsilon = 8/255.0\n",
    "    x_adv_eps = torch.minimum(torch.maximum(x_adv, x_orig-epsilon), x_orig+epsilon)\n",
    "    return torch.clamp(x_adv_eps, 0, 1)\n",
    "\n",
    "def pgd_attack_data(X, y, t_mnet, t_hnet, K, cnn, kernel, zs):\n",
    "    criterion = nn.CrossEntropyLoss()    \n",
    "    x_adv = torch.clone(X).detach()\n",
    "   \n",
    "    for i in range(20):\n",
    "        x_adv = x_adv.requires_grad_(True)\n",
    "        x_features = torch.zeros((x_adv.shape[0], K)).to(device)\n",
    "        x_features_lbls = torch.zeros((x_adv.shape[0])).to(device)\n",
    "        for j in range(x_adv.shape[0]):\n",
    "            mx = x_adv[j].view(-1, X.shape[1])\n",
    "            x_sample_features = get_q_sample_features(mx, cnn, kernel, zs)\n",
    "            x_features[j] = x_sample_features\n",
    "            x_features_lbls[j] = y[j]\n",
    "            \n",
    "        # Apply to test set\n",
    "        W_mnet = t_hnet(cond_id=0)\n",
    "        logits = t_mnet.forward(x_features, weights=W_mnet)\n",
    "        loss_adv = criterion(logits, x_features_lbls.long())\n",
    "        loss_adv.backward(retain_graph=True)\n",
    "        \n",
    "        grad = x_adv.grad.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_adv = x_adv + 0.1 * torch.sign(grad)  # take a gradient update step to minimize the objective\n",
    "            x_adv = project(x_adv, X)               # ensure we stay in the allowed range\n",
    "            \n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bbdcd4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_lbls(X_test, y_test, test_classes, hnet, mnet, Ks, cnn, n_c, q_size):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy for the sample with label test_classes in X_test.\n",
    "    Mainly used as utility for the calc_accuracy function below.\n",
    "    \n",
    "    Args:\n",
    "        X_test (tensor): entire test set\n",
    "        y_test (tensor): corresponding labels\n",
    "        test_classes: the classes we want to consider for testing accuracies (should contain Ks classes)\n",
    "        mnet : main net trained by the hypernetwork\n",
    "        Ks: the K of K-shot K-way\n",
    "        s_cnn: the cnn trained to compute the desired features\n",
    "\n",
    "    Returns:\n",
    "        type: accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test = get_s_and_q_sets(X_test, y_test, \\\n",
    "                                                                                 test_classes, Ks, q_size)\n",
    "        z_space, K = compute_kernel(s_set_test, s_set_lbl_test, cnn, Ks)\n",
    "        \n",
    "        # Accuracy\n",
    "        all_q_features = torch.zeros((q_set_test.shape[0], Ks)).to(device)\n",
    "        all_q_features_lbls = torch.zeros((q_set_test.shape[0])).to(device)\n",
    "        for i in range(q_set_test.shape[0]):\n",
    "            mx = q_set_test[i].view(-1, q_set_test.shape[1])\n",
    "            q_sample_features = get_q_sample_features(mx, cnn, K, z_space)\n",
    "            all_q_features[i] = q_sample_features\n",
    "            all_q_features_lbls[i] = q_set_lbl_test[i]\n",
    "\n",
    "        W_dataset_l_acc =  hnet(uncond_input=K.view(1, -1))\n",
    "        dataset_l_P_acc = mnet.forward(all_q_features, weights=W_dataset_l_acc)\n",
    "        prediction_extended_acc = extend_pred_to_nclasses(dataset_l_P_acc, n_c, test_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(prediction_extended_acc, all_q_features_lbls.long())\n",
    "        accuracy = (torch.argmax(prediction_extended_acc,dim=1) == all_q_features_lbls.long()).float().mean().item()\n",
    "        print(\"Correctly predicted samples had labels:\", all_q_features_lbls[torch.argmax(prediction_extended_acc,dim=1) == all_q_features_lbls.long()])\n",
    "    return accuracy, loss.item()\n",
    "\n",
    "\n",
    "def calc_accuracy(X_test, y_test, hnet, mnet, Ks, cnn, n_c, q_size):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy for the entire X_test test set.\n",
    "    \n",
    "    Args:\n",
    "        X_test (tensor): entire test set\n",
    "        y_test (tensor): corresponding labels\n",
    "        mnet : main net trained by the hypernetwork\n",
    "        Ks: the K of K-shot K-way\n",
    "        s_cnn: the cnn trained to compute the desired features\n",
    "\n",
    "    Returns:\n",
    "        type: average accuracy over all the label batch (of Ks different labels each time)\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(X_test):\n",
    "        X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    else: \n",
    "        X_test_t = torch.clone(X_test)\n",
    "        \n",
    "    if not torch.is_tensor(y_test):\n",
    "        y_test_t = torch.FloatTensor(y_test).to(device)\n",
    "    else:\n",
    "        y_test_t = torch.clone(y_test)\n",
    "        \n",
    "    diff_classes = torch.unique(y_test_t)\n",
    "    n_diff_classes = diff_classes.shape[0]\n",
    "    n_sets = int(n_diff_classes / Ks)\n",
    "    acc, loss = 0.0, 0.0\n",
    "    for i in range(n_sets):\n",
    "        lbls = diff_classes[i*Ks:(i+1)*Ks].tolist()\n",
    "        d_acc, d_loss = calc_accuracy_lbls(X_test, y_test, lbls, hnet, mnet, Ks, cnn, n_c, q_size)\n",
    "        acc += d_acc\n",
    "        loss += d_loss\n",
    "    acc = acc / n_sets\n",
    "    loss = loss / n_sets\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "437afb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_lbls_adv(X_test, y_test, test_classes, mnet, Ks, s_cnn, q_set_test_adv):\n",
    "    \"\"\"\n",
    "    Same as the calc_accuracy_lbls function but replace the query set with an attacked version of itself.\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test = get_s_and_q_sets(X_test, y_test, \\\n",
    "                                                                                 test_classes, Ks, 5) \n",
    "        q_set_test = q_set_test_adv\n",
    "        z_space, K = compute_kernel(s_set_test, s_set_lbl_test, s_cnn, Ks)\n",
    "        \n",
    "        # Accuracy\n",
    "        all_q_features = torch.zeros((q_set_test.shape[0], Ks)).to(device)\n",
    "        all_q_features_lbls = torch.zeros((q_set_test.shape[0])).to(device)\n",
    "        for i in range(q_set_test.shape[0]):\n",
    "            mx = q_set_test[i].view(-1, q_set_test.shape[1])\n",
    "            my = torch.argmax(q_set_lbl_test[i])\n",
    "            q_sample_features = get_q_sample_features(mx, s_cnn, K, z_space)\n",
    "            all_q_features[i] = q_sample_features\n",
    "            all_q_features_lbls[i] = my\n",
    "\n",
    "        W_dataset_l = hnet(cond_id=0)\n",
    "        dataset_l_P = mnet.forward(all_q_features, weights=W_dataset_l)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(dataset_l_P, all_q_features_lbls.long())\n",
    "        accuracy = (torch.argmax(dataset_l_P,dim=1) == all_q_features_lbls.long()).float().mean().item()\n",
    "        print(\"Correctly predicted labels:\", all_q_features_lbls.long()[torch.argmax(dataset_l_P,dim=1) == all_q_features_lbls.long()])\n",
    "    return accuracy, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dd4014d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Creating an MLP with 90 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 21790 weights and 90 outputs (compression ratio: 242.11).\n",
      "The network consists of 21790 unconditional weights (21790 internally maintained) and 0 conditional weights (0 internally maintained).\n",
      "----------------------- Epoch 0  -----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quent\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\quent\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly predicted samples had labels: tensor([3., 3., 3., 3., 3.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 7.757396697998047)\n",
      "Correctly predicted samples had labels: tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 7.095114707946777)\n",
      "Correctly predicted samples had labels: tensor([53., 53., 53., 53., 53.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 10 --> (0.19999998807907104, 8.098657608032227)\n",
      "Correctly predicted samples had labels: tensor([53., 53., 53., 53., 53.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 10 --> (0.19999998807907104, 8.064777374267578)\n",
      "Global loss at the end of epoch: 0 : 150.23689079284668\n",
      "\n",
      "----------------------- Epoch 1  -----------------------\n",
      "Correctly predicted samples had labels: tensor([2., 2., 2., 2., 2.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 6.914384841918945)\n",
      "Correctly predicted samples had labels: tensor([4., 4., 4., 4., 4.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 7.517232894897461)\n",
      "Correctly predicted samples had labels: tensor([52., 52., 52., 52., 52.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 10 --> (0.19999998807907104, 6.910940647125244)\n",
      "Correctly predicted samples had labels: tensor([52., 52., 52., 52., 52.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 10 --> (0.19999998807907104, 7.52752685546875)\n",
      "Global loss at the end of epoch: 1 : 142.1547408103943\n",
      "\n",
      "----------------------- Epoch 2  -----------------------\n",
      "Correctly predicted samples had labels: tensor([2., 2., 2., 2., 2.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 6.577121734619141)\n",
      "Correctly predicted samples had labels: tensor([4., 4., 4., 4., 4.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 7.058077335357666)\n",
      "Correctly predicted samples had labels: tensor([50., 50., 50., 50., 50.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 10 --> (0.19999998807907104, 6.710321426391602)\n",
      "Correctly predicted samples had labels: tensor([52., 52., 52., 52., 52.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 10 --> (0.19999998807907104, 7.723903656005859)\n",
      "Global loss at the end of epoch: 2 : 138.43911695480347\n",
      "\n",
      "----------------------- Epoch 3  -----------------------\n",
      "Correctly predicted samples had labels: tensor([2., 2., 2., 2., 2.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 6.188531875610352)\n",
      "Correctly predicted samples had labels: tensor([4., 4., 4., 4., 4.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 6.36351203918457)\n",
      "Correctly predicted samples had labels: tensor([50., 50., 50., 50., 50.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 10 --> (0.19999998807907104, 6.707014560699463)\n",
      "Correctly predicted samples had labels: tensor([52., 52., 52., 52., 52.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 10 --> (0.19999998807907104, 7.964343070983887)\n",
      "Global loss at the end of epoch: 3 : 135.3987216949463\n",
      "\n",
      "----------------------- Epoch 4  -----------------------\n",
      "Correctly predicted samples had labels: tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 5.767220497131348)\n",
      "Correctly predicted samples had labels: tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 5.800334453582764)\n",
      "Correctly predicted samples had labels: tensor([50., 50., 50., 50., 50.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 10 --> (0.19999998807907104, 6.610395908355713)\n",
      "Correctly predicted samples had labels: tensor([52., 52., 52., 52., 52.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 10 --> (0.19999998807907104, 8.132364273071289)\n",
      "Global loss at the end of epoch: 4 : 130.86526536941528\n",
      "\n",
      "----------------------- Epoch 5  -----------------------\n",
      "Correctly predicted samples had labels: tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 5.240825653076172)\n",
      "Correctly predicted samples had labels: tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 5.5723795890808105)\n",
      "Correctly predicted samples had labels: tensor([52., 52., 52., 52., 52.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 10 --> (0.19999998807907104, 6.567774772644043)\n",
      "Correctly predicted samples had labels: tensor([53., 53., 53., 53., 53.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 10 --> (0.19999998807907104, 7.615781307220459)\n",
      "Global loss at the end of epoch: 5 : 127.40489292144775\n",
      "\n",
      "----------------------- Epoch 6  -----------------------\n",
      "Correctly predicted samples had labels: tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 5.446822643280029)\n",
      "Correctly predicted samples had labels: tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 5.643641471862793)\n",
      "Correctly predicted samples had labels: tensor([52., 52., 52., 52., 52.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 10 --> (0.19999998807907104, 6.592204570770264)\n",
      "Correctly predicted samples had labels: tensor([53., 53., 53., 53., 53.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 10 --> (0.19999998807907104, 7.368525981903076)\n",
      "Global loss at the end of epoch: 6 : 125.34780597686768\n",
      "\n",
      "----------------------- Epoch 7  -----------------------\n",
      "Correctly predicted samples had labels: tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 4.601591110229492)\n",
      "Correctly predicted samples had labels: tensor([1., 1., 1., 1., 1.], device='cuda:0')\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 5.078479766845703)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m global_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_dataset_l\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l_set_id \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocal train acc and loss at the end of set:\u001b[39m\u001b[38;5;124m\"\u001b[39m, l_set_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-->\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcalc_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_l_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_l_lbl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m                                                                           \u001b[49m\u001b[43mKs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLocal test acc and loss at the end of set:\u001b[39m\u001b[38;5;124m\"\u001b[39m, l_set_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-->\u001b[39m\u001b[38;5;124m\"\u001b[39m, calc_accuracy(dataset_l_test, dataset_l_lbl_test, hnet, mnet,\\\n\u001b[0;32m    106\u001b[0m                                                                            Ks, kcnn, n_classes, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_adv_train \u001b[38;5;241m<\u001b[39m nepochs:\n",
      "Cell \u001b[1;32mIn[73], line 72\u001b[0m, in \u001b[0;36mcalc_accuracy\u001b[1;34m(X_test, y_test, hnet, mnet, Ks, cnn, n_c, q_size)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_sets):\n\u001b[0;32m     71\u001b[0m     lbls \u001b[38;5;241m=\u001b[39m diff_classes[i\u001b[38;5;241m*\u001b[39mKs:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mKs]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 72\u001b[0m     d_acc, d_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_accuracy_lbls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_acc\n\u001b[0;32m     74\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss\n",
      "Cell \u001b[1;32mIn[73], line 28\u001b[0m, in \u001b[0;36mcalc_accuracy_lbls\u001b[1;34m(X_test, y_test, test_classes, hnet, mnet, Ks, cnn, n_c, q_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(q_set_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     27\u001b[0m     mx \u001b[38;5;241m=\u001b[39m q_set_test[i]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, q_set_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 28\u001b[0m     q_sample_features \u001b[38;5;241m=\u001b[39m \u001b[43mget_q_sample_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     all_q_features[i] \u001b[38;5;241m=\u001b[39m q_sample_features\n\u001b[0;32m     30\u001b[0m     all_q_features_lbls[i] \u001b[38;5;241m=\u001b[39m q_set_lbl_test[i]\n",
      "Cell \u001b[1;32mIn[62], line 104\u001b[0m, in \u001b[0;36mget_q_sample_features\u001b[1;34m(X, cnn, kernel, zs)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03mComputes the final features used for classification, given a query sample mx\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    type: final flattened features use by the main network\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    103\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m--> 104\u001b[0m zs_q \u001b[38;5;241m=\u001b[39m \u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m zs_q_m \u001b[38;5;241m=\u001b[39m standardize(torch\u001b[38;5;241m.\u001b[39mmatmul(zs, torch\u001b[38;5;241m.\u001b[39mt(zs_q)))\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# This could be modified, the features are just the concatenation of the kernel and the q_sample multiplied\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# by the z_space of the support set\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[62], line 13\u001b[0m, in \u001b[0;36mKernelCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torchvision\\models\\resnet.py:276\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m--> 276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    279\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torchvision\\models\\resnet.py:96\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m---> 96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure training.\n",
    "nepochs=250\n",
    "# epoch after which adversarial training starts\n",
    "do_adv_train = 10000\n",
    "# K-shot k-way\n",
    "Ks = 5\n",
    "# Length of the embeddings produced by the CNN\n",
    "z_len = 10\n",
    "\n",
    "load_weights = 0\n",
    "continue_training = 0\n",
    "\n",
    "# Array storing statistics (not used for now)\n",
    "accuracies_dataset_0 = []\n",
    "accuracies_dataset_0_adv = []\n",
    "accuracies_dataset_1 = []\n",
    "accuracies_dataset_1_adv = []\n",
    "\n",
    "# Loop in case we want to do statistics (not sued for now)\n",
    "for o in range(1):\n",
    "    print(\"Iteration\", o+1)\n",
    "    \n",
    "    if continue_training == 0:\n",
    "        # Models definition\n",
    "        kcnn = KernelCNN(z_len).to(device)\n",
    "        mnet = MLP(n_in=Ks, n_out=Ks, hidden_layers=[5, 5]).to(device)\n",
    "        hnet = HMLP(mnet.param_shapes, uncond_in_size=Ks*Ks, cond_in_size=0,\n",
    "                    layers=[100, 100], num_cond_embs=0).to(device)\n",
    "        params = hnet.conditional_params.copy()\n",
    "        hnet.apply_hyperfan_init(mnet=mnet)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # If we want to load weights from anywhere\n",
    "        if load_weights == 1:\n",
    "            file_path = 'models/hnet_20231229022719_49.pth'\n",
    "            hnet.load_state_dict(torch.load(file_path))\n",
    "            file_path = 'models/kcnn_20231229022719_49.pth'\n",
    "            kcnn.load_state_dict(torch.load(file_path))\n",
    "\n",
    "        # The amount of sets of Ks labels we can do during training\n",
    "        n_sets = int(len(lbls_0) / Ks)\n",
    "\n",
    "        # Compute training and validation sets for each of the n_sets labels sets\n",
    "        train_test_sets = []\n",
    "        all_test_sets = np.empty((0, dataset_0.shape[1]))\n",
    "        all_test_sets_lbl = np.empty((0))\n",
    "        for l_set_id in range(n_sets):\n",
    "            c_lbls = lbls_0[l_set_id*Ks:(l_set_id+1)*Ks]\n",
    "            if (l_set_id+1) % 100 == 0:\n",
    "                print(\"Generated train-test split for\", l_set_id+1,\"/\",n_sets)\n",
    "            mask_b = np.isin(dataset_0_lbl, np.array(c_lbls))\n",
    "            dataset_0_b, dataset_0_lbl_b = dataset_0[mask_b], dataset_0_lbl[mask_b]\n",
    "            dataset_0_train, dataset_0_test, dataset_0_lbl_train, dataset_0_lbl_test = \\\n",
    "                            train_test_split(dataset_0_b, dataset_0_lbl_b, random_state=42, test_size=0.5, stratify=dataset_0_lbl_b)\n",
    "            all_test_sets = np.concatenate((all_test_sets, dataset_0_test), axis=0)\n",
    "            all_test_sets_lbl = np.concatenate((all_test_sets_lbl, dataset_0_lbl_test), axis=0)\n",
    "            train_test_sets.append((dataset_0_train, dataset_0_test, dataset_0_lbl_train, dataset_0_lbl_test, c_lbls))\n",
    "    \n",
    "    # Optimizer and scheduler initialization\n",
    "    optimizer = optim.Adam(hnet.parameters(), lr=0.00001)\n",
    "    optimizer_s = optim.Adam(kcnn.parameters(), lr=0.00001)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=nepochs, eta_min=0.00001)\n",
    "    scheduler_s = CosineAnnealingLR(optimizer_s, T_max=nepochs, eta_min=0.00001)\n",
    "        \n",
    "    # Main training loop\n",
    "    for epoch in range(nepochs): # For each epoch.\n",
    "        print(\"----------------------- Epoch\", epoch, \" -----------------------\")\n",
    "        # Stores the loss over all labels sets\n",
    "        global_loss = 0.0\n",
    "        # We loop over all our sets at each epoch\n",
    "        for l_set_id in range(n_sets):\n",
    "            (dataset_l_train, dataset_l_test, dataset_l_lbl_train, dataset_l_lbl_test, c_lbls) = train_test_sets[l_set_id]\n",
    "            \n",
    "            s_set_train, s_set_lbl_train, q_set_train, q_set_lbl_train, z_space, K, all_q_features, all_q_features_lbls = \\\n",
    "            compute_sets_and_features(dataset_l_train, dataset_l_lbl_train, c_lbls, kcnn, Ks, 1)\n",
    "            \n",
    "            # Formward pass\n",
    "            W_dataset_l = hnet(uncond_input=K.view(1, -1))\n",
    "            dataset_l_P = mnet.forward(all_q_features, weights=W_dataset_l)\n",
    "            prediction_extended = extend_pred_to_nclasses(dataset_l_P, n_classes, c_lbls)\n",
    "            loss_dataset_l = criterion(prediction_extended, all_q_features_lbls.long())\n",
    "\n",
    "            # Adversarial training\n",
    "            if epoch == do_adv_train and l_set_id == 0:\n",
    "                print(\"Adversarial training starts.\")\n",
    "            if epoch >= do_adv_train:\n",
    "                mx_adv = pgd_attack_data(q_set_train, q_set_lbl_train, mnet, hnet, Ks, kcnn, K, z_space)\n",
    "                \n",
    "                all_q_features_adv = torch.zeros((q_set_train.shape[0], Ks)).to(device)\n",
    "                for i in range(mx_adv.shape[0]):\n",
    "                    mxx = mx_adv[i].view(-1, q_set_train.shape[1])\n",
    "                    q_sample_features_adv = get_q_sample_features(mxx, kcnn, K, z_space)\n",
    "                    all_q_features_adv[i] = q_sample_features_adv\n",
    "                dataset_l_P_adv = mnet.forward(all_q_features_adv, weights=W_dataset_l)\n",
    "                prediction_extended_adv = extend_pred_to_nclasses(dataset_l_P_adv, n_classes, c_lbls)\n",
    "                loss_dataset_l_adv = criterion(prediction_extended_adv, all_q_features_lbls.long())\n",
    "                loss_dataset_l += loss_dataset_l_adv\n",
    "            \n",
    "            global_loss += loss_dataset_l.item()\n",
    "            if l_set_id % 10 == 0:\n",
    "                print(\"Local train acc and loss at the end of set:\", l_set_id, \"-->\", calc_accuracy(dataset_l_train, dataset_l_lbl_train, hnet, mnet,\\\n",
    "                                                                                       Ks, kcnn, n_classes, 5))\n",
    "                print(\"Local test acc and loss at the end of set:\", l_set_id, \"-->\", calc_accuracy(dataset_l_test, dataset_l_lbl_test, hnet, mnet,\\\n",
    "                                                                                       Ks, kcnn, n_classes, 5))\n",
    "                if do_adv_train < nepochs:\n",
    "                    s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test, z_space_tes, K_test, all_q_features_test, all_q_features_lbls_test = \\\n",
    "                    compute_sets_and_features(dataset_l_test, dataset_l_lbl_test, c_lbls, kcnn, Ks, 5)\n",
    "                    mx_adv_test = pgd_attack_data(q_set_test, q_set_lbl_test, mnet, hnet, Ks, kcnn, K, z_space)\n",
    "                    print(\"Local adv test acc and loss at the end of set:\", l_set_id, \"-->\", calc_accuracy_lbls_adv(dataset_l_test, dataset_l_lbl_test, c_lbls, hnet, mnet,\\\n",
    "                                                                                           Ks, kcnn, mx_adv_test))\n",
    "            loss_dataset_l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer_s.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            scheduler_s.step()\n",
    "                \n",
    "  \n",
    "        print(\"Global loss at the end of epoch:\", epoch, \":\", global_loss)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            # Create a file name with the current time\n",
    "            hnet_file = f'models/hnet_{current_time}_{epoch}.pth'\n",
    "            torch.save(hnet.state_dict(), hnet_file)\n",
    "            kcnn_file = f'models/kcnn_{current_time}_{epoch}.pth'\n",
    "            torch.save(kcnn.state_dict(), kcnn_file)\n",
    "            print(\"--> Global test accuracy after epoch:\", epoch, \"-->\", calc_accuracy(all_test_sets, all_test_sets_lbl,\\\n",
    "                                                                                       hnet, mnet, Ks, kcnn, n_classes, 5))\n",
    "        print()\n",
    "\n",
    "    print(\"END OF ITERATION:\",o+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd33649",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Create a file name with the current time\n",
    "hnet_file = f'models/hnet_{current_time}.pth'\n",
    "torch.save(hnet.state_dict(), hnet_file)\n",
    "kcnn_file = f'models/kcnn_{current_time}.pth'\n",
    "torch.save(kcnn.state_dict(), kcnn_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145607b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_adv_dataset_1 = pgd_attack_data(dataset_1, dataset_1_lbl, mnet, hnet, z_space_1, K_1, 1)\n",
    "# x_adv_dataset_1_np = x_adv_dataset_1.detach().cpu().numpy()\n",
    "# x_adv_dataset_0_test = pgd_attack_data(dataset_0_test, dataset_0_lbl_test, mnet, hnet, z_space, K, 0)\n",
    "# x_adv_dataset_0_test_np = x_adv_dataset_0_test.detach().cpu().numpy()\n",
    "\n",
    "print(calc_accuracy(dataset_0_test, dataset_0_lbl_test, mnet, W_dataset_0, Ks, s_cnn))\n",
    "print(calc_accuracy(dataset_1, dataset_1_lbl, mnet, W_dataset_0, Ks, s_cnn))\n",
    "# accuracies_dataset_0_adv.append((calc_accuracy(x_adv_dataset_0_test_np, dataset_0_lbl_test, mnet, W_dataset_0)).detach().cpu())\n",
    "# accuracies_dataset_1_adv.append((calc_accuracy(x_adv_dataset_1_np, dataset_1_lbl, mnet, W_dataset_1)).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045661ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean:\")\n",
    "print(\"dataset 0 accuracy:\", np.mean(np.array(accuracies_dataset_0)))\n",
    "print(\"dataset 1 accuracy:\", np.mean(np.array(accuracies_dataset_1)))\n",
    "print(\"dataset 0 adv accuracy:\", np.mean(np.array(accuracies_dataset_0_adv)))\n",
    "print(\"dataset 1 adv accuracy:\", np.mean(np.array(accuracies_dataset_1_adv)))\n",
    "print()\n",
    "print(\"Standard deviation:\")\n",
    "print(\"dataset 0 accuracy:\", np.std(np.array(accuracies_dataset_0)))\n",
    "print(\"dataset 1 accuracy:\", np.std(np.array(accuracies_dataset_1)))\n",
    "print(\"dataset 0 adv accuracy:\", np.std(np.array(accuracies_dataset_0_adv)))\n",
    "print(\"dataset 1 adv accuracy:\", np.std(np.array(accuracies_dataset_1_adv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2e88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
