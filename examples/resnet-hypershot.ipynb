{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a3ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use: cuda\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Omniglot\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Will use:\", device)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8780cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29600a",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fae8e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading MNIST dataset ...\n",
      "Elapsed time to read dataset: 0.339049 sec\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from hypnettorch.data import FashionMNISTData, MNISTData\n",
    "from hypnettorch.data.dataset import Dataset\n",
    "from hypnettorch.mnets import LeNet\n",
    "from hypnettorch.mnets.resnet import ResNet\n",
    "from hypnettorch.mnets.mlp import MLP\n",
    "from hypnettorch.hnets import HMLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import learn2learn as l2l\n",
    "import copy\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mnist = MNISTData(data_dir, use_one_hot=True, validation_size=0)\n",
    "fmnist = FashionMNISTData(data_dir, use_one_hot=True, validation_size=0)\n",
    "\n",
    "omniglot = l2l.vision.datasets.FullOmniglot(root=data_dir,\n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.Resize(28, interpolation=Image.LANCZOS),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                lambda x: 1.0 - x,\n",
    "                                            ]),\n",
    "                                            download=True)\n",
    "omniglot = l2l.data.MetaDataset(omniglot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e95df",
   "metadata": {},
   "source": [
    "## Convert the dataset to numpy for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf2a597",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension: (32460, 1, 28, 28)\n",
      "Labels dimension: (32460,)\n",
      "0\n",
      "1622\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for batching and shuffling the data\n",
    "batch_size = len(omniglot)  # Set batch size to the total number of examples to load all data at once\n",
    "data_loader = DataLoader(omniglot, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in data_loader:\n",
    "    images, labels = batch\n",
    "    # Convert PyTorch tensors to NumPy arrays\n",
    "    dataset = images.numpy()\n",
    "    dataset_lbl = labels.numpy()    \n",
    "    sizes = dataset.shape\n",
    "    \n",
    "print(\"Dataset dimension:\", dataset.shape)\n",
    "print(\"Labels dimension:\", dataset_lbl.shape)\n",
    "print(np.min(dataset_lbl))\n",
    "print(np.max(dataset_lbl))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48adcc",
   "metadata": {},
   "source": [
    "## Create 2 different datasets for two disjoint set of labels (deterministic for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe5509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0 ... 1622 1622 1622]\n",
      "(32460, 784)\n",
      "(32460,)\n",
      "Shape of the dataset_0: (2000, 784)\n",
      "Shape of the dataset_1: (30460, 784)\n",
      "Some labels in set 1: [0 0 0 0 0 0 0 0 0 0]\n",
      "Some labels in set 2: [100 100 100 100 100 100 100 100 100 100]\n",
      "Minimum and maximum amount of sample per classes in the dataset\n",
      "Each classes contains at least 20 samples\n",
      "Each classes contains at most 20 samples\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of training samples from each data handler.\n",
    "# mnist_inps, mnist_trgts = mnist.next_train_batch(4)\n",
    "# dataset_inps, dataset_trgts = dataset.next_train_batch(4)\n",
    "# dataset_full, dataset_full_lbl = dataset.next_train_batch(60000)\n",
    "print(dataset_lbl)\n",
    "\n",
    "n_classes = len(np.unique(dataset_lbl))\n",
    "dataset_full = dataset.reshape((dataset.shape[0], dataset.shape[2]*dataset.shape[3]))\n",
    "dataset_full_lbl = dataset_lbl\n",
    "\n",
    "print(dataset_full.shape)\n",
    "print(dataset_full_lbl.shape)\n",
    "\n",
    "# TODO-yz: you will need to use the same split as is used in evaluation (pull and merge with main to get access to datasets.get_benchmark_tasksets which gives you train/val/test split over classes)\n",
    "sep = 100\n",
    "lbls_0 = [i for i in range(sep)]\n",
    "lbls_1 = [i for i in range(sep, n_classes)]\n",
    "\n",
    "mask_0 = np.isin(dataset_full_lbl, np.array(lbls_0))\n",
    "mask_1 = np.isin(dataset_full_lbl, np.array(lbls_1))\n",
    "dataset_0, dataset_0_lbl = dataset_full[mask_0], dataset_full_lbl[mask_0]\n",
    "\n",
    "print(\"Shape of the dataset_0:\",dataset_0.shape)\n",
    "\n",
    "dataset_1, dataset_1_lbl = dataset_full[mask_1], dataset_full_lbl[mask_1]\n",
    "\n",
    "print(\"Shape of the dataset_1:\",dataset_1.shape)\n",
    "\n",
    "print(\"Some labels in set 1:\", dataset_0_lbl[0:10])\n",
    "print(\"Some labels in set 2:\", dataset_1_lbl[0:10])\n",
    "assert(np.all(np.isin(dataset_0_lbl, lbls_0)))\n",
    "assert(np.all(np.isin(dataset_1_lbl, lbls_1)))\n",
    "\n",
    "# mnist.plot_samples('MNIST Examples', mnist_inps, outputs=mnist_trgts)\n",
    "# dataset.plot_samples('FashionMNIST Examples with lbl < sep', dataset_0[0:4], outputs=dataset_0_lbl[0:4])\n",
    "# dataset.plot_samples('FashionMNIST Examples with lbl >= sep', dataset_1[0:4], outputs=dataset_1_lbl[0:4])\n",
    "\n",
    "torch_dataset = torch.tensor(dataset_full_lbl)\n",
    "unique_values, counts = torch.unique(torch_dataset, return_counts=True)\n",
    "\n",
    "print(\"Minimum and maximum amount of sample per classes in the dataset\")\n",
    "print(\"Each classes contains at least\", torch.min(counts).item(), \"samples\")\n",
    "print(\"Each classes contains at most\", torch.max(counts).item(), \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7356d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom, rotate\n",
    "from scipy.interpolate import interp2d\n",
    "\n",
    "def rotate_dataset(dataset, angle):\n",
    "    dataset_unflatten = dataset.reshape(-1, 1, 28, 28)\n",
    "    rotated_data = rotate(dataset_unflatten, angle, axes=(2, 3), reshape=False)\n",
    "    return rotated_data.reshape(-1, 784)\n",
    "\n",
    "def zoom_dataset(dataset, zoom_factor):\n",
    "    dataset_unflatten = dataset.reshape(-1, 1, 28, 28)\n",
    "    zoomed_dataset = zoom(dataset_unflatten, (1, 1, zoom_factor, zoom_factor), order=1)\n",
    "    \n",
    "    original_size = dataset_unflatten.shape\n",
    "    zoomed_size = zoomed_dataset.shape\n",
    "    diff = int((zoomed_size[2] - original_size[2])/2)\n",
    "    interpolated_data = zoomed_dataset[:,:,diff:diff+original_size[2], diff:diff+original_size[2]]\n",
    "    return interpolated_data.reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4e57d",
   "metadata": {},
   "source": [
    "### Compute a pgd attack on test set to assert robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc33a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, z_length):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.z_length = z_length\n",
    "        resnet18 = models.resnet18(pretrained=False)\n",
    "        resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), bias=False)\n",
    "        resnet18.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, self.z_length)\n",
    "        self.resnet = resnet18\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return self.resnet(x)\n",
    "    \n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, z_length):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.z_length = z_length\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(16 * 16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, z_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0badd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_kernel(X, y, cnn, K):\n",
    "    \"\"\"\n",
    "    Compute Hypershot kernel for a support set X and label y\n",
    "    It takes the average of the z's for each label as suggested in the Hypershot paper\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): Support set used to compute the kernel\n",
    "        y (tensor): corresponding labels\n",
    "        cnn : CNN used to compute the embeddings\n",
    "        K: the K of K-shot K-way learning\n",
    "\n",
    "    Returns:\n",
    "        type: embeddings, kernel\n",
    "    \"\"\"\n",
    "    # Obtain the indices that would sort y_test\n",
    "    indices = torch.argsort(y)\n",
    "\n",
    "    # Use the indices to sort the rows of X_test\n",
    "    sorted_X = X[indices].to(device)\n",
    "    sorted_y = y[indices].to(device)\n",
    "    \n",
    "    reshaped_X = sorted_X.view(sorted_X.shape[0], 1, 28, 28).to(device)\n",
    "    nn_X = cnn(reshaped_X)\n",
    "    \n",
    "    # TODO-yz: think this can be turned into a one-liner to make things faster. Sorting looks correct to me btw\n",
    "    mean_X = torch.zeros((int(nn_X.shape[0] / K), nn_X.shape[1])).to(device)\n",
    "    for i in range(K):\n",
    "        mean_X[i] = torch.mean(nn_X[i*K:(i+1)*K], dim = 0)\n",
    "    norm_X = F.normalize(mean_X, p=2, dim=1)\n",
    "    \n",
    "    assert(nn_X.shape==(sorted_X.shape[0], cnn.z_length))\n",
    "    \n",
    "    # TODO-yz: in the paper they used normalized dot product which is not the same as making the features 0-1-gaussian. See formula (6) in paper. Think it is okay to have f as the identity (its essentially the same as extending the feature extractor)\n",
    "    return norm_X, torch.matmul(norm_X, torch.t(norm_X))\n",
    "\n",
    "def get_s_and_q_sets(X, y, trgt_lbls, K, q_size):\n",
    "    # TODO-yz: this should become much easier if you use the datasets.py\n",
    "    \"\"\"\n",
    "    Computes a support set for data X for classes in y with K sample per classes\n",
    "    and corresponding query sets of size q_size.\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): Data used to compute the sets (can contain label you do not want for your sets)\n",
    "        y (tensor): corresponding labels\n",
    "        trgt_lbls : the labels that end up in the sets\n",
    "        K: the K of K-shot K-way learning\n",
    "        q_size: amount of sample per classes in query set\n",
    "\n",
    "    Returns:\n",
    "        type: support set, support set labels, query set, query set labels\n",
    "    \"\"\"\n",
    "    s_set = np.zeros((len(trgt_lbls) * K, X.shape[1]))\n",
    "    s_set_lbl = np.zeros((len(trgt_lbls) * K))\n",
    "    \n",
    "    q_set = np.zeros((len(trgt_lbls) * q_size, X.shape[1]))\n",
    "    q_set_lbl = np.zeros((len(trgt_lbls) * q_size))\n",
    "    \n",
    "    for j, l in enumerate(trgt_lbls):\n",
    "        mask = (y == l)\n",
    "        masked_data = X[mask]\n",
    "        masked_lbls = y[mask]\n",
    "        s_set[j*K:(j+1)*K] = masked_data[0:K]\n",
    "        s_set_lbl[j*K:(j+1)*K] = masked_lbls[0:K]\n",
    "        q_set[j*q_size:(j+1)*q_size] = masked_data[K:K+q_size]\n",
    "        q_set_lbl[j*q_size:(j+1)*q_size] = masked_lbls[K:K+q_size]\n",
    "    \n",
    "    s_set = torch.tensor(s_set, requires_grad=True).to(device).float()\n",
    "    s_set_lbl = torch.tensor(s_set_lbl, requires_grad=True).to(device).float()\n",
    "    q_set = torch.tensor(q_set, requires_grad=True).to(device).float()\n",
    "    q_set_lbl = torch.tensor(q_set_lbl, requires_grad=True).to(device).float()\n",
    "    \n",
    "    return s_set, s_set_lbl, q_set, q_set_lbl\n",
    "\n",
    "def get_q_sample_features(X, cnn, kernel, zs):\n",
    "    \"\"\"\n",
    "    Computes the final features used for classification, given a query sample mx\n",
    "    \n",
    "    Args:\n",
    "        X (tensor): query sample \n",
    "        cnn: the cnn trained to compute the desired features\n",
    "        kernel: the kernel corresponding to the corresponding X's support set\n",
    "        zs: z space of the support set corresponding to the query sample\n",
    "\n",
    "    Returns:\n",
    "        type: final flattened features use by the main network\n",
    "    \"\"\"\n",
    "    # TODO-yz: you could pass the entire query set at once instead of iterating over every i in range(q_set.shape[0]). But you'd need to make sure that the first dimension corresponds to the images of the query set and the second dimension to all the ways. Also flattening would have to be done starting at dim 1\n",
    "    X = X.view(1, 28, 28)\n",
    "    # TODO-yz: not sure why you don't need to unsqueeze here (1,28,28) has no batch_dim\n",
    "    zs_q = cnn(X.view(X.shape[0], 1, 28, -1))\n",
    "    zs_q = F.normalize(zs_q, p=2, dim=1)\n",
    "    zs_q_m = torch.matmul(zs, torch.t(zs_q))\n",
    "    # This could be modified, the features are just the concatenation of the kernel and the q_sample multiplied\n",
    "    # by the z_space of the support set\n",
    "    q_features = zs_q_m.flatten()\n",
    "    return q_features\n",
    "\n",
    "def compute_sets_and_features(X, y, trgt_lbls, cnn, K, q_size):\n",
    "    s_set, s_set_lbl, q_set, q_set_lbl = get_s_and_q_sets(X, y, trgt_lbls, K, q_size)\n",
    "\n",
    "    # Kernel computation\n",
    "    z_space, kernel = compute_kernel(s_set, s_set_lbl, cnn, K)\n",
    "\n",
    "    # Gather features for all samples in query training set\n",
    "    all_q_features = torch.zeros((q_set.shape[0], K)).to(device)\n",
    "    all_q_features_lbls = torch.zeros((q_set.shape[0])).to(device)\n",
    "    for i in range(q_set.shape[0]):\n",
    "        x = q_set[i].view(1, -1)\n",
    "        q_sample_features = get_q_sample_features(x, cnn, kernel, z_space)\n",
    "        all_q_features[i] = q_sample_features\n",
    "        all_q_features_lbls[i] = q_set_lbl[i]\n",
    "        \n",
    "    return s_set, s_set_lbl, q_set, q_set_lbl, z_space, kernel, all_q_features, all_q_features_lbls\n",
    "\n",
    "def extend_pred_to_nclasses(pred, n_c, lbls):\n",
    "    out = torch.zeros((pred.shape[0], n_classes)).to(device)\n",
    "    for i in range(out.shape[0]):\n",
    "        # TODO-yz: not sure how this runs, probably have some global c_lbls somewhere bc it's not defined here\n",
    "        out[i][lbls] = pred[i]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafc56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO-yz: could wrap the entire forward pass of your model into a custom nn.Module and then you can use torchattacks.PGD (you will have to do this anyway for evaluation to work + it's cleaner ;)\n",
    "def project(x_adv, x_orig):\n",
    "    epsilon = 8/255.0\n",
    "    x_adv_eps = torch.minimum(torch.maximum(x_adv, x_orig-epsilon), x_orig+epsilon)\n",
    "    return torch.clamp(x_adv_eps, 0, 1)\n",
    "\n",
    "def pgd_attack_data(X, y, t_mnet, t_hnet, K, cnn, kernel, zs):\n",
    "    criterion = nn.CrossEntropyLoss()    \n",
    "    x_adv = torch.clone(X).detach()\n",
    "   \n",
    "    for i in range(20):\n",
    "        x_adv = x_adv.requires_grad_(True)\n",
    "        x_features = torch.zeros((x_adv.shape[0], K)).to(device)\n",
    "        x_features_lbls = torch.zeros((x_adv.shape[0])).to(device)\n",
    "        for j in range(x_adv.shape[0]):\n",
    "            mx = x_adv[j].view(-1, X.shape[1])\n",
    "            x_sample_features = get_q_sample_features(mx, cnn, kernel, zs)\n",
    "            x_features[j] = x_sample_features\n",
    "            x_features_lbls[j] = y[j]\n",
    "            \n",
    "        # Apply to test set\n",
    "        W_mnet = t_hnet(cond_id=0)\n",
    "        logits = t_mnet.forward(x_features, weights=W_mnet)\n",
    "        loss_adv = criterion(logits, x_features_lbls.long())\n",
    "        loss_adv.backward(retain_graph=True)\n",
    "        \n",
    "        grad = x_adv.grad.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x_adv = x_adv + 0.1 * torch.sign(grad)  # take a gradient update step to minimize the objective\n",
    "            x_adv = project(x_adv, X)               # ensure we stay in the allowed range\n",
    "            \n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbdcd4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_lbls(X_test, y_test, test_classes, hnet, mnet, Ks, cnn, n_c, q_size):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy for the sample with label test_classes in X_test.\n",
    "    Mainly used as utility for the calc_accuracy function below.\n",
    "    \n",
    "    Args:\n",
    "        X_test (tensor): entire test set\n",
    "        y_test (tensor): corresponding labels\n",
    "        test_classes: the classes we want to consider for testing accuracies (should contain Ks classes)\n",
    "        mnet : main net trained by the hypernetwork\n",
    "        Ks: the K of K-shot K-way\n",
    "        s_cnn: the cnn trained to compute the desired features\n",
    "\n",
    "    Returns:\n",
    "        type: accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test = get_s_and_q_sets(X_test, y_test, \\\n",
    "                                                                                 test_classes, Ks, q_size)\n",
    "        z_space, K = compute_kernel(s_set_test, s_set_lbl_test, cnn, Ks)\n",
    "        \n",
    "        # Accuracy\n",
    "        all_q_features = torch.zeros((q_set_test.shape[0], Ks)).to(device)\n",
    "        all_q_features_lbls = torch.zeros((q_set_test.shape[0])).to(device)\n",
    "        for i in range(q_set_test.shape[0]):\n",
    "            mx = q_set_test[i].view(-1, q_set_test.shape[1])\n",
    "            q_sample_features = get_q_sample_features(mx, cnn, K, z_space)\n",
    "            all_q_features[i] = q_sample_features\n",
    "            all_q_features_lbls[i] = q_set_lbl_test[i]\n",
    "\n",
    "        # TODO-yz: forward pass looks pretty correct to me now :)\n",
    "        W_dataset_l_acc =  hnet(uncond_input=K.view(1, -1))\n",
    "        dataset_l_P_acc = mnet.forward(all_q_features, weights=W_dataset_l_acc)\n",
    "        # TODO-yz: not sure but seems like you could probably use torch.nn.functional.one_hot for this\n",
    "        prediction_extended_acc = extend_pred_to_nclasses(dataset_l_P_acc, n_c, test_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(prediction_extended_acc, all_q_features_lbls.long())\n",
    "        accuracy = (torch.argmax(prediction_extended_acc,dim=1) == all_q_features_lbls.long()).float().mean().item()\n",
    "        # print(\"Correctly predicted samples had labels:\", all_q_features_lbls[torch.argmax(prediction_extended_acc,dim=1) == all_q_features_lbls.long()])\n",
    "    return accuracy, loss.item()\n",
    "\n",
    "\n",
    "def calc_accuracy(X_test, y_test, hnet, mnet, Ks, cnn, n_c, q_size):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy for the entire X_test test set.\n",
    "    \n",
    "    Args:\n",
    "        X_test (tensor): entire test set\n",
    "        y_test (tensor): corresponding labels\n",
    "        mnet : main net trained by the hypernetwork\n",
    "        Ks: the K of K-shot K-way\n",
    "        s_cnn: the cnn trained to compute the desired features\n",
    "\n",
    "    Returns:\n",
    "        type: average accuracy over all the label batch (of Ks different labels each time)\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(X_test):\n",
    "        X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    else:  \n",
    "        X_test_t = torch.clone(X_test)\n",
    "        \n",
    "    if not torch.is_tensor(y_test):\n",
    "        y_test_t = torch.FloatTensor(y_test).to(device)\n",
    "    else:\n",
    "        y_test_t = torch.clone(y_test)\n",
    "        \n",
    "    diff_classes = torch.unique(y_test_t)\n",
    "    n_diff_classes = diff_classes.shape[0]\n",
    "    n_sets = int(n_diff_classes / Ks)\n",
    "    acc, loss = 0.0, 0.0\n",
    "    for i in range(n_sets):\n",
    "        lbls = diff_classes[i*Ks:(i+1)*Ks].tolist()\n",
    "        d_acc, d_loss = calc_accuracy_lbls(X_test, y_test, lbls, hnet, mnet, Ks, cnn, n_c, q_size)\n",
    "        acc += d_acc\n",
    "        loss += d_loss\n",
    "    acc = acc / n_sets\n",
    "    loss = loss / n_sets\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "437afb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_lbls_adv(X_test, y_test, test_classes, mnet, Ks, s_cnn, q_set_test_adv):\n",
    "    \"\"\"\n",
    "    Same as the calc_accuracy_lbls function but replace the query set with an attacked version of itself.\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test = get_s_and_q_sets(X_test, y_test, \\\n",
    "                                                                                 test_classes, Ks, 5) \n",
    "        q_set_test = q_set_test_adv\n",
    "        z_space, K = compute_kernel(s_set_test, s_set_lbl_test, s_cnn, Ks)\n",
    "        \n",
    "        # Accuracy\n",
    "        all_q_features = torch.zeros((q_set_test.shape[0], Ks)).to(device)\n",
    "        all_q_features_lbls = torch.zeros((q_set_test.shape[0])).to(device)\n",
    "        for i in range(q_set_test.shape[0]):\n",
    "            mx = q_set_test[i].view(-1, q_set_test.shape[1])\n",
    "            my = torch.argmax(q_set_lbl_test[i])\n",
    "            q_sample_features = get_q_sample_features(mx, s_cnn, K, z_space)\n",
    "            all_q_features[i] = q_sample_features\n",
    "            all_q_features_lbls[i] = my\n",
    "\n",
    "        # TODO-yz: here the forward pass doesn't look right, really recommend having one model where you call forward\n",
    "        W_dataset_l = hnet(cond_id=0)\n",
    "        dataset_l_P = mnet.forward(all_q_features, weights=W_dataset_l)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(dataset_l_P, all_q_features_lbls.long())\n",
    "        accuracy = (torch.argmax(dataset_l_P,dim=1) == all_q_features_lbls.long()).float().mean().item()\n",
    "        # print(\"Correctly predicted labels:\", all_q_features_lbls.long()[torch.argmax(dataset_l_P,dim=1) == all_q_features_lbls.long()])\n",
    "    return accuracy, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd4014d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Creating an MLP with 3077 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 115205 weights and 3077 outputs (compression ratio: 37.44).\n",
      "The network consists of 115205 unconditional weights (115205 internally maintained) and 0 conditional weights (0 internally maintained).\n",
      "----------------------- Epoch 0  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 7.141263008117676)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 7.144166946411133)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quent\\AppData\\Local\\Temp\\ipykernel_27052\\984778371.py:119: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  out[i][lbls] = pred[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global loss at the end of epoch: 0 : 118.49699687957764\n",
      "--> Global test accuracy after epoch: 0 --> (0.19999998807907104, 4.333171415328979)\n",
      "\n",
      "----------------------- Epoch 1  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 4.3311991691589355)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 4.334333896636963)\n",
      "Global loss at the end of epoch: 1 : 62.48989200592041\n",
      "--> Global test accuracy after epoch: 1 --> (0.19999998807907104, 2.10187885761261)\n",
      "\n",
      "----------------------- Epoch 2  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 2.1012516021728516)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.1029868125915527)\n",
      "Global loss at the end of epoch: 2 : 35.12727439403534\n",
      "--> Global test accuracy after epoch: 2 --> (0.19999998807907104, 1.6340398490428925)\n",
      "\n",
      "----------------------- Epoch 3  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6338237524032593)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6342957019805908)\n",
      "Global loss at the end of epoch: 3 : 32.39669585227966\n",
      "--> Global test accuracy after epoch: 3 --> (0.19999998807907104, 1.614332628250122)\n",
      "\n",
      "----------------------- Epoch 4  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6143161058425903)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6145058870315552)\n",
      "Global loss at the end of epoch: 4 : 32.2524129152298\n",
      "--> Global test accuracy after epoch: 4 --> (0.19599998891353607, 1.6110615313053132)\n",
      "\n",
      "----------------------- Epoch 5  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6110459566116333)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6111772060394287)\n",
      "Global loss at the end of epoch: 5 : 32.21866071224213\n",
      "--> Global test accuracy after epoch: 5 --> (0.19999998807907104, 1.6107374489307404)\n",
      "\n",
      "----------------------- Epoch 6  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6107282638549805)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6108478307724)\n",
      "Global loss at the end of epoch: 6 : 32.21121633052826\n",
      "--> Global test accuracy after epoch: 6 --> (0.19999998807907104, 1.6104856729507446)\n",
      "\n",
      "----------------------- Epoch 7  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6104774475097656)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.610596776008606)\n",
      "Global loss at the end of epoch: 7 : 32.207234382629395\n",
      "--> Global test accuracy after epoch: 7 --> (0.19399999044835567, 1.6103200018405914)\n",
      "\n",
      "----------------------- Epoch 8  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6103122234344482)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6104316711425781)\n",
      "Global loss at the end of epoch: 8 : 32.20434379577637\n",
      "--> Global test accuracy after epoch: 8 --> (0.20599998980760575, 1.610194730758667)\n",
      "\n",
      "----------------------- Epoch 9  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.1599999964237213, 1.610184669494629)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.610306739807129)\n",
      "Global loss at the end of epoch: 9 : 32.20207750797272\n",
      "--> Global test accuracy after epoch: 9 --> (0.20199999026954174, 1.6100946724414826)\n",
      "\n",
      "----------------------- Epoch 10  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6100856065750122)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6102055311203003)\n",
      "Global loss at the end of epoch: 10 : 32.20025050640106\n",
      "--> Global test accuracy after epoch: 10 --> (0.19199998937547208, 1.6100135564804077)\n",
      "\n",
      "----------------------- Epoch 11  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.610002875328064)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6101268529891968)\n",
      "Global loss at the end of epoch: 11 : 32.19876408576965\n",
      "--> Global test accuracy after epoch: 11 --> (0.18799998946487903, 1.609947031736374)\n",
      "\n",
      "----------------------- Epoch 12  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6099356412887573)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.610060453414917)\n",
      "Global loss at the end of epoch: 12 : 32.1975280046463\n",
      "--> Global test accuracy after epoch: 12 --> (0.18999998979270458, 1.6098924160003663)\n",
      "\n",
      "----------------------- Epoch 13  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6098792552947998)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6100058555603027)\n",
      "Global loss at the end of epoch: 13 : 32.196491837501526\n",
      "--> Global test accuracy after epoch: 13 --> (0.18999998979270458, 1.60984628200531)\n",
      "\n",
      "----------------------- Epoch 14  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.609831690788269)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6099615097045898)\n",
      "Global loss at the end of epoch: 14 : 32.195618629455566\n",
      "--> Global test accuracy after epoch: 14 --> (0.2059999883174896, 1.6098077416419982)\n",
      "\n",
      "----------------------- Epoch 15  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097936630249023)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6099246740341187)\n",
      "Global loss at the end of epoch: 15 : 32.194872975349426\n",
      "--> Global test accuracy after epoch: 15 --> (0.19999998807907104, 1.6097751677036285)\n",
      "\n",
      "----------------------- Epoch 16  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.60975980758667)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6098955869674683)\n",
      "Global loss at the end of epoch: 16 : 32.1942241191864\n",
      "--> Global test accuracy after epoch: 16 --> (0.19999998807907104, 1.6097491204738616)\n",
      "\n",
      "----------------------- Epoch 17  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097325086593628)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6098721027374268)\n",
      "Global loss at the end of epoch: 17 : 32.193649649620056\n",
      "--> Global test accuracy after epoch: 17 --> (0.19599998854100703, 1.6097253680229187)\n",
      "\n",
      "----------------------- Epoch 18  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097068786621094)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6098507642745972)\n",
      "Global loss at the end of epoch: 18 : 32.19312882423401\n",
      "--> Global test accuracy after epoch: 18 --> (0.19799998886883258, 1.6097024440765382)\n",
      "\n",
      "----------------------- Epoch 19  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.609682559967041)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6098285913467407)\n",
      "Global loss at the end of epoch: 19 : 32.19266152381897\n",
      "--> Global test accuracy after epoch: 19 --> (0.19799998998641968, 1.6096825003623962)\n",
      "\n",
      "----------------------- Epoch 20  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6096606254577637)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6098095178604126)\n",
      "Global loss at the end of epoch: 20 : 32.19223952293396\n",
      "--> Global test accuracy after epoch: 20 --> (0.20199998952448367, 1.6096656620502472)\n",
      "\n",
      "----------------------- Epoch 21  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.1599999964237213, 1.6096410751342773)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.609794020652771)\n",
      "Global loss at the end of epoch: 21 : 32.191861271858215\n",
      "--> Global test accuracy after epoch: 21 --> (0.20999998971819878, 1.609651893377304)\n",
      "\n",
      "----------------------- Epoch 22  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6096246242523193)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097843647003174)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global loss at the end of epoch: 22 : 32.19151246547699\n",
      "--> Global test accuracy after epoch: 22 --> (0.21599998995661734, 1.609638696908951)\n",
      "\n",
      "----------------------- Epoch 23  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.609610915184021)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097735166549683)\n",
      "Global loss at the end of epoch: 23 : 32.19120264053345\n",
      "--> Global test accuracy after epoch: 23 --> (0.21399998888373375, 1.6096271276474)\n",
      "\n",
      "----------------------- Epoch 24  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.609595537185669)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097668409347534)\n",
      "Global loss at the end of epoch: 24 : 32.1908962726593\n",
      "--> Global test accuracy after epoch: 24 --> (0.20999998971819878, 1.6096172630786896)\n",
      "\n",
      "----------------------- Epoch 25  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6095833778381348)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097590923309326)\n",
      "Global loss at the end of epoch: 25 : 32.190627574920654\n",
      "--> Global test accuracy after epoch: 25 --> (0.2019999898970127, 1.6096088409423828)\n",
      "\n",
      "----------------------- Epoch 26  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6095696687698364)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097564697265625)\n",
      "Global loss at the end of epoch: 26 : 32.19034123420715\n",
      "--> Global test accuracy after epoch: 26 --> (0.1959999892860651, 1.6096008479595185)\n",
      "\n",
      "----------------------- Epoch 27  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6095590591430664)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097530126571655)\n",
      "Global loss at the end of epoch: 27 : 32.19008696079254\n",
      "--> Global test accuracy after epoch: 27 --> (0.19999999031424523, 1.6095953106880188)\n",
      "\n",
      "----------------------- Epoch 28  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.1599999964237213, 1.6095490455627441)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097559928894043)\n",
      "Global loss at the end of epoch: 28 : 32.1898330450058\n",
      "--> Global test accuracy after epoch: 28 --> (0.20599998980760575, 1.609589898586273)\n",
      "\n",
      "----------------------- Epoch 29  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6095362901687622)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097595691680908)\n",
      "Global loss at the end of epoch: 29 : 32.18956136703491\n",
      "--> Global test accuracy after epoch: 29 --> (0.20399999022483825, 1.6095856606960297)\n",
      "\n",
      "----------------------- Epoch 30  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.23999999463558197, 1.6095272302627563)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097626686096191)\n",
      "Global loss at the end of epoch: 30 : 32.1893105506897\n",
      "--> Global test accuracy after epoch: 30 --> (0.2019999884068966, 1.6095829844474792)\n",
      "\n",
      "----------------------- Epoch 31  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6095173358917236)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6097718477249146)\n",
      "Global loss at the end of epoch: 31 : 32.189045786857605\n",
      "--> Global test accuracy after epoch: 31 --> (0.2019999884068966, 1.6095807671546936)\n",
      "\n",
      "----------------------- Epoch 32  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6095083951950073)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.609785795211792)\n",
      "Global loss at the end of epoch: 32 : 32.188756227493286\n",
      "--> Global test accuracy after epoch: 32 --> (0.20399998873472214, 1.6095785260200501)\n",
      "\n",
      "----------------------- Epoch 33  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.609495997428894)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6098039150238037)\n",
      "Global loss at the end of epoch: 33 : 32.18845272064209\n",
      "--> Global test accuracy after epoch: 33 --> (0.2019999884068966, 1.609577625989914)\n",
      "\n",
      "----------------------- Epoch 34  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6094863414764404)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6098227500915527)\n",
      "Global loss at the end of epoch: 34 : 32.18813908100128\n",
      "--> Global test accuracy after epoch: 34 --> (0.19999998807907104, 1.6095786213874816)\n",
      "\n",
      "----------------------- Epoch 35  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6094762086868286)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6098490953445435)\n",
      "Global loss at the end of epoch: 35 : 32.1878023147583\n",
      "--> Global test accuracy after epoch: 35 --> (0.19999998807907104, 1.609581309556961)\n",
      "\n",
      "----------------------- Epoch 36  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6094664335250854)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6098847389221191)\n",
      "Global loss at the end of epoch: 36 : 32.187424421310425\n",
      "--> Global test accuracy after epoch: 36 --> (0.19999998807907104, 1.6095875382423401)\n",
      "\n",
      "----------------------- Epoch 37  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6094567775726318)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6099318265914917)\n",
      "Global loss at the end of epoch: 37 : 32.18698036670685\n",
      "--> Global test accuracy after epoch: 37 --> (0.19999998807907104, 1.6095981478691102)\n",
      "\n",
      "----------------------- Epoch 38  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6094468832015991)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.609999179840088)\n",
      "Global loss at the end of epoch: 38 : 32.186437249183655\n",
      "--> Global test accuracy after epoch: 38 --> (0.1939999893307686, 1.609616994857788)\n",
      "\n",
      "----------------------- Epoch 39  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6094447374343872)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6100971698760986)\n",
      "Global loss at the end of epoch: 39 : 32.185808062553406\n",
      "--> Global test accuracy after epoch: 39 --> (0.19799998849630357, 1.6096517324447632)\n",
      "\n",
      "----------------------- Epoch 40  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6094509363174438)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6102478504180908)\n",
      "Global loss at the end of epoch: 40 : 32.18502879142761\n",
      "--> Global test accuracy after epoch: 40 --> (0.20199998952448367, 1.6097167491912843)\n",
      "\n",
      "----------------------- Epoch 41  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6094670295715332)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6104730367660522)\n",
      "Global loss at the end of epoch: 41 : 32.18409860134125\n",
      "--> Global test accuracy after epoch: 41 --> (0.18999998904764653, 1.6098491966724395)\n",
      "\n",
      "----------------------- Epoch 42  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.1599999964237213, 1.6095091104507446)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 1.6108331680297852)\n",
      "Global loss at the end of epoch: 42 : 32.18329620361328\n",
      "--> Global test accuracy after epoch: 42 --> (0.19199998900294304, 1.6101605594158173)\n",
      "\n",
      "----------------------- Epoch 43  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6096235513687134)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.611397385597229)\n",
      "Global loss at the end of epoch: 43 : 32.18474495410919\n",
      "--> Global test accuracy after epoch: 43 --> (0.19799998849630357, 1.6107268273830413)\n",
      "\n",
      "----------------------- Epoch 44  -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.609778642654419)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.612632393836975)\n",
      "Global loss at the end of epoch: 44 : 32.20045328140259\n",
      "--> Global test accuracy after epoch: 44 --> (0.19999998807907104, 1.6130117058753968)\n",
      "\n",
      "----------------------- Epoch 45  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6140981912612915)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.61781644821167)\n",
      "Global loss at the end of epoch: 45 : 32.203161001205444\n",
      "--> Global test accuracy after epoch: 45 --> (0.1819999895989895, 1.6112777054309846)\n",
      "\n",
      "----------------------- Epoch 46  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6091302633285522)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.615115761756897)\n",
      "Global loss at the end of epoch: 46 : 32.175822019577026\n",
      "--> Global test accuracy after epoch: 46 --> (0.18999998942017554, 1.6144503831863404)\n",
      "\n",
      "----------------------- Epoch 47  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6096819639205933)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6249481439590454)\n",
      "Global loss at the end of epoch: 47 : 32.16257607936859\n",
      "--> Global test accuracy after epoch: 47 --> (0.19799998849630357, 1.623243808746338)\n",
      "\n",
      "----------------------- Epoch 48  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6101434230804443)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6465688943862915)\n",
      "Global loss at the end of epoch: 48 : 32.21135592460632\n",
      "--> Global test accuracy after epoch: 48 --> (0.19599999114871025, 1.6454323530197144)\n",
      "\n",
      "----------------------- Epoch 49  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.23999999463558197, 1.6103274822235107)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6533094644546509)\n",
      "Global loss at the end of epoch: 49 : 32.67417526245117\n",
      "--> Global test accuracy after epoch: 49 --> (0.18599999137222767, 1.658691591024399)\n",
      "\n",
      "----------------------- Epoch 50  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.611018419265747)\n",
      "Local test acc and loss at the end of set: 0 --> (0.03999999910593033, 1.6671921014785767)\n",
      "Global loss at the end of epoch: 50 : 32.395811319351196\n",
      "--> Global test accuracy after epoch: 50 --> (0.18599999099969863, 1.6418484091758727)\n",
      "\n",
      "----------------------- Epoch 51  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6140366792678833)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6895617246627808)\n",
      "Global loss at the end of epoch: 51 : 32.24729645252228\n",
      "--> Global test accuracy after epoch: 51 --> (0.18599998913705348, 1.6763725399971008)\n",
      "\n",
      "----------------------- Epoch 52  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6693190336227417)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.814226508140564)\n",
      "Global loss at the end of epoch: 52 : 32.45945084095001\n",
      "--> Global test accuracy after epoch: 52 --> (0.1839999906718731, 1.698285835981369)\n",
      "\n",
      "----------------------- Epoch 53  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.23999999463558197, 1.6494781970977783)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.916051983833313)\n",
      "Global loss at the end of epoch: 53 : 32.569865703582764\n",
      "--> Global test accuracy after epoch: 53 --> (0.18999999165534973, 1.7052452743053437)\n",
      "\n",
      "----------------------- Epoch 54  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6266813278198242)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.7762079238891602)\n",
      "Global loss at the end of epoch: 54 : 32.07492542266846\n",
      "--> Global test accuracy after epoch: 54 --> (0.18999999277293683, 1.6948078095912933)\n",
      "\n",
      "----------------------- Epoch 55  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6047345399856567)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.7846510410308838)\n",
      "Global loss at the end of epoch: 55 : 31.756534814834595\n",
      "--> Global test accuracy after epoch: 55 --> (0.18599999248981475, 1.7190652310848236)\n",
      "\n",
      "----------------------- Epoch 56  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.23999999463558197, 1.597933292388916)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 1.916777491569519)\n",
      "Global loss at the end of epoch: 56 : 31.579567909240723\n",
      "--> Global test accuracy after epoch: 56 --> (0.19199999198317527, 1.7720525085926055)\n",
      "\n",
      "----------------------- Epoch 57  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.2800000011920929, 1.5906699895858765)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.088270664215088)\n",
      "Global loss at the end of epoch: 57 : 31.359516978263855\n",
      "--> Global test accuracy after epoch: 57 --> (0.19999999180436134, 1.8142429292201996)\n",
      "\n",
      "----------------------- Epoch 58  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.23999999463558197, 1.5807536840438843)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.197725772857666)\n",
      "Global loss at the end of epoch: 58 : 31.16466736793518\n",
      "--> Global test accuracy after epoch: 58 --> (0.18799999207258225, 1.8379015505313874)\n",
      "\n",
      "----------------------- Epoch 59  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.35999998450279236, 1.5902029275894165)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.3004417419433594)\n",
      "Global loss at the end of epoch: 59 : 30.860989809036255\n",
      "--> Global test accuracy after epoch: 59 --> (0.17399999164044858, 1.8626458644866943)\n",
      "\n",
      "----------------------- Epoch 60  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.23999999463558197, 1.5781559944152832)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.5412964820861816)\n",
      "Global loss at the end of epoch: 60 : 31.147714614868164\n",
      "--> Global test accuracy after epoch: 60 --> (0.18799999132752418, 2.1252542436122894)\n",
      "\n",
      "----------------------- Epoch 61  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.7395782470703125)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.6234915256500244)\n",
      "Global loss at the end of epoch: 61 : 33.665228605270386\n",
      "--> Global test accuracy after epoch: 61 --> (0.19799999259412288, 1.8351284086704254)\n",
      "\n",
      "----------------------- Epoch 62  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.6440322399139404)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.346386671066284)\n",
      "Global loss at the end of epoch: 62 : 31.01054084300995\n",
      "--> Global test accuracy after epoch: 62 --> (0.18599999211728574, 1.8248932182788848)\n",
      "\n",
      "----------------------- Epoch 63  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.23999999463558197, 1.5485855340957642)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.4499704837799072)\n",
      "Global loss at the end of epoch: 63 : 30.537685751914978\n",
      "--> Global test accuracy after epoch: 63 --> (0.19999999292194842, 1.9176474869251252)\n",
      "\n",
      "----------------------- Epoch 64  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.23999999463558197, 1.5210233926773071)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.668600082397461)\n",
      "Global loss at the end of epoch: 64 : 30.229105710983276\n",
      "--> Global test accuracy after epoch: 64 --> (0.16799999438226224, 2.126815402507782)\n",
      "\n",
      "----------------------- Epoch 65  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.5342199802398682)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.2994439601898193)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global loss at the end of epoch: 65 : 30.150235891342163\n",
      "--> Global test accuracy after epoch: 65 --> (0.1719999924302101, 2.4239167749881743)\n",
      "\n",
      "----------------------- Epoch 66  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.19999998807907104, 1.5841341018676758)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.9374501705169678)\n",
      "Global loss at the end of epoch: 66 : 30.927644729614258\n",
      "--> Global test accuracy after epoch: 66 --> (0.1779999926686287, 2.791691893339157)\n",
      "\n",
      "----------------------- Epoch 67  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.2800000011920929, 1.5383062362670898)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 4.558361053466797)\n",
      "Global loss at the end of epoch: 67 : 32.41080319881439\n",
      "--> Global test accuracy after epoch: 67 --> (0.19399999193847178, 2.552351200580597)\n",
      "\n",
      "----------------------- Epoch 68  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.3199999928474426, 1.5009815692901611)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.995015859603882)\n",
      "Global loss at the end of epoch: 68 : 32.15784549713135\n",
      "--> Global test accuracy after epoch: 68 --> (0.1979999929666519, 2.2104887425899507)\n",
      "\n",
      "----------------------- Epoch 69  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.3199999928474426, 1.6240174770355225)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.131704092025757)\n",
      "Global loss at the end of epoch: 69 : 31.080180644989014\n",
      "--> Global test accuracy after epoch: 69 --> (0.18399999253451824, 2.104648399353027)\n",
      "\n",
      "----------------------- Epoch 70  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.23999999463558197, 1.686445713043213)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.582244634628296)\n",
      "Global loss at the end of epoch: 70 : 31.44074559211731\n",
      "--> Global test accuracy after epoch: 70 --> (0.20599999278783798, 2.1786674201488494)\n",
      "\n",
      "----------------------- Epoch 71  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.23999999463558197, 1.678776741027832)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.925583839416504)\n",
      "Global loss at the end of epoch: 71 : 30.877288103103638\n",
      "--> Global test accuracy after epoch: 71 --> (0.20199998915195466, 2.2418471813201903)\n",
      "\n",
      "----------------------- Epoch 72  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.35999998450279236, 1.4118536710739136)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.8646087646484375)\n",
      "Global loss at the end of epoch: 72 : 28.356751203536987\n",
      "--> Global test accuracy after epoch: 72 --> (0.19399999380111693, 2.0563053488731384)\n",
      "\n",
      "----------------------- Epoch 73  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.23999999463558197, 1.5424364805221558)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.3777618408203125)\n",
      "Global loss at the end of epoch: 73 : 29.71158730983734\n",
      "--> Global test accuracy after epoch: 73 --> (0.18599999286234378, 1.882450121641159)\n",
      "\n",
      "----------------------- Epoch 74  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.2800000011920929, 1.3725048303604126)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.1424851417541504)\n",
      "Global loss at the end of epoch: 74 : 27.47196078300476\n",
      "--> Global test accuracy after epoch: 74 --> (0.19999999403953553, 1.9993140876293183)\n",
      "\n",
      "----------------------- Epoch 75  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.3999999761581421, 1.3727329969406128)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.6448657512664795)\n",
      "Global loss at the end of epoch: 75 : 28.247066855430603\n",
      "--> Global test accuracy after epoch: 75 --> (0.20799999199807645, 1.9595324218273162)\n",
      "\n",
      "----------------------- Epoch 76  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.2800000011920929, 1.4008195400238037)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.2125372886657715)\n",
      "Global loss at the end of epoch: 76 : 27.235942840576172\n",
      "--> Global test accuracy after epoch: 76 --> (0.1839999955147505, 1.9781254470348357)\n",
      "\n",
      "----------------------- Epoch 77  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.3999999761581421, 1.3029005527496338)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.671560049057007)\n",
      "Global loss at the end of epoch: 77 : 27.78707754611969\n",
      "--> Global test accuracy after epoch: 77 --> (0.19799999333918095, 1.90151526927948)\n",
      "\n",
      "----------------------- Epoch 78  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.47999998927116394, 1.2871289253234863)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.0899085998535156)\n",
      "Global loss at the end of epoch: 78 : 26.681596398353577\n",
      "--> Global test accuracy after epoch: 78 --> (0.19199999421834946, 2.017623722553253)\n",
      "\n",
      "----------------------- Epoch 79  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.5199999809265137, 1.2516493797302246)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.501539707183838)\n",
      "Global loss at the end of epoch: 79 : 26.263923287391663\n",
      "--> Global test accuracy after epoch: 79 --> (0.17199999354779721, 2.1136316657066345)\n",
      "\n",
      "----------------------- Epoch 80  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.5199999809265137, 1.2423039674758911)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.785569429397583)\n",
      "Global loss at the end of epoch: 80 : 26.79132843017578\n",
      "--> Global test accuracy after epoch: 80 --> (0.1959999930113554, 2.216278386116028)\n",
      "\n",
      "----------------------- Epoch 81  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.47999998927116394, 1.3257946968078613)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.243854284286499)\n",
      "Global loss at the end of epoch: 81 : 25.560842037200928\n",
      "--> Global test accuracy after epoch: 81 --> (0.18599999509751797, 2.3218580722808837)\n",
      "\n",
      "----------------------- Epoch 82  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.5600000023841858, 1.2766586542129517)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.08803653717041)\n",
      "Global loss at the end of epoch: 82 : 25.789400696754456\n",
      "--> Global test accuracy after epoch: 82 --> (0.20599999390542506, 2.250104087591171)\n",
      "\n",
      "----------------------- Epoch 83  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.5199999809265137, 1.226768970489502)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.375756025314331)\n",
      "Global loss at the end of epoch: 83 : 24.345845103263855\n",
      "--> Global test accuracy after epoch: 83 --> (0.1859999939799309, 2.3435390174388884)\n",
      "\n",
      "----------------------- Epoch 84  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.6800000071525574, 1.1735855340957642)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.129615068435669)\n",
      "Global loss at the end of epoch: 84 : 24.51426935195923\n",
      "--> Global test accuracy after epoch: 84 --> (0.2019999947398901, 2.3259448230266573)\n",
      "\n",
      "----------------------- Epoch 85  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.5999999642372131, 1.1759138107299805)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.4643020629882812)\n",
      "Global loss at the end of epoch: 85 : 23.640581369400024\n",
      "--> Global test accuracy after epoch: 85 --> (0.18199999295175076, 2.438386803865433)\n",
      "\n",
      "----------------------- Epoch 86  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7599999904632568, 1.1031757593154907)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.5485973358154297)\n",
      "Global loss at the end of epoch: 86 : 23.43242621421814\n",
      "--> Global test accuracy after epoch: 86 --> (0.18599999472498893, 2.400448423624039)\n",
      "\n",
      "----------------------- Epoch 87  -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local train acc and loss at the end of set: 0 --> (0.7599999904632568, 1.080634355545044)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.614271879196167)\n",
      "Global loss at the end of epoch: 87 : 22.828442096710205\n",
      "--> Global test accuracy after epoch: 87 --> (0.1879999939352274, 2.383891898393631)\n",
      "\n",
      "----------------------- Epoch 88  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7599999904632568, 1.021337628364563)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.4041695594787598)\n",
      "Global loss at the end of epoch: 88 : 22.381117463111877\n",
      "--> Global test accuracy after epoch: 88 --> (0.1979999929666519, 2.2842501223087313)\n",
      "\n",
      "----------------------- Epoch 89  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7199999690055847, 1.0227943658828735)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 3.074826717376709)\n",
      "Global loss at the end of epoch: 89 : 22.302888453006744\n",
      "--> Global test accuracy after epoch: 89 --> (0.19999999292194842, 2.255255103111267)\n",
      "\n",
      "----------------------- Epoch 90  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7599999904632568, 0.9724493622779846)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.8467535972595215)\n",
      "Global loss at the end of epoch: 90 : 22.055264830589294\n",
      "--> Global test accuracy after epoch: 90 --> (0.19399999380111693, 2.225167787075043)\n",
      "\n",
      "----------------------- Epoch 91  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.6399999856948853, 0.9555641412734985)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.6460840702056885)\n",
      "Global loss at the end of epoch: 91 : 21.558086156845093\n",
      "--> Global test accuracy after epoch: 91 --> (0.19399999268352985, 2.225634390115738)\n",
      "\n",
      "----------------------- Epoch 92  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7199999690055847, 0.9185569882392883)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.526020050048828)\n",
      "Global loss at the end of epoch: 92 : 21.225329279899597\n",
      "--> Global test accuracy after epoch: 92 --> (0.18199999406933784, 2.2730300605297087)\n",
      "\n",
      "----------------------- Epoch 93  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7199999690055847, 0.879798412322998)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.7930185794830322)\n",
      "Global loss at the end of epoch: 93 : 21.15225750207901\n",
      "--> Global test accuracy after epoch: 93 --> (0.17999999448657036, 2.325013756752014)\n",
      "\n",
      "----------------------- Epoch 94  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.6399999856948853, 0.8797043561935425)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.7626845836639404)\n",
      "Global loss at the end of epoch: 94 : 21.516346096992493\n",
      "--> Global test accuracy after epoch: 94 --> (0.18199999444186687, 2.394158220291138)\n",
      "\n",
      "----------------------- Epoch 95  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.5600000023841858, 0.9172866344451904)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.1851961612701416)\n",
      "Global loss at the end of epoch: 95 : 20.926712095737457\n",
      "--> Global test accuracy after epoch: 95 --> (0.17399999499320984, 2.3644882917404173)\n",
      "\n",
      "----------------------- Epoch 96  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.6800000071525574, 0.916557252407074)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.683302879333496)\n",
      "Global loss at the end of epoch: 96 : 21.297226071357727\n",
      "--> Global test accuracy after epoch: 96 --> (0.18799999430775644, 2.48821102976799)\n",
      "\n",
      "----------------------- Epoch 97  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.5999999642372131, 0.9219196438789368)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 3.086524248123169)\n",
      "Global loss at the end of epoch: 97 : 20.660442531108856\n",
      "--> Global test accuracy after epoch: 97 --> (0.19199999533593654, 2.414188140630722)\n",
      "\n",
      "----------------------- Epoch 98  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7599999904632568, 0.8537654876708984)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.9449024200439453)\n",
      "Global loss at the end of epoch: 98 : 20.184832215309143\n",
      "--> Global test accuracy after epoch: 98 --> (0.18599999472498893, 2.5177038967609406)\n",
      "\n",
      "----------------------- Epoch 99  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7199999690055847, 0.8185628652572632)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 3.1275434494018555)\n",
      "Global loss at the end of epoch: 99 : 20.19537180662155\n",
      "--> Global test accuracy after epoch: 99 --> (0.1799999952316284, 2.4310352861881257)\n",
      "\n",
      "----------------------- Epoch 100  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7199999690055847, 0.7323834896087646)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.9699018001556396)\n",
      "Global loss at the end of epoch: 100 : 19.41882026195526\n",
      "--> Global test accuracy after epoch: 100 --> (0.1799999937415123, 2.3847262859344482)\n",
      "\n",
      "----------------------- Epoch 101  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7599999904632568, 0.7851436734199524)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.563694715499878)\n",
      "Global loss at the end of epoch: 101 : 19.4469553232193\n",
      "--> Global test accuracy after epoch: 101 --> (0.19399999380111693, 2.408640760183334)\n",
      "\n",
      "----------------------- Epoch 102  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7999999523162842, 0.770170271396637)\n",
      "Local test acc and loss at the end of set: 0 --> (0.2800000011920929, 2.523378610610962)\n",
      "Global loss at the end of epoch: 102 : 19.692555487155914\n",
      "--> Global test accuracy after epoch: 102 --> (0.18999999277293683, 2.425407516956329)\n",
      "\n",
      "----------------------- Epoch 103  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8799999952316284, 0.7277290225028992)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.471775770187378)\n",
      "Global loss at the end of epoch: 103 : 18.441984474658966\n",
      "--> Global test accuracy after epoch: 103 --> (0.17599999383091927, 2.4355014264583588)\n",
      "\n",
      "----------------------- Epoch 104  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8399999737739563, 0.6859616637229919)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.6087729930877686)\n",
      "Global loss at the end of epoch: 104 : 17.657160222530365\n",
      "--> Global test accuracy after epoch: 104 --> (0.1919999945908785, 2.4712450981140135)\n",
      "\n",
      "----------------------- Epoch 105  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8399999737739563, 0.6429457068443298)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.6206839084625244)\n",
      "Global loss at the end of epoch: 105 : 17.088104486465454\n",
      "--> Global test accuracy after epoch: 105 --> (0.17999999597668648, 2.5156121611595155)\n",
      "\n",
      "----------------------- Epoch 106  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7999999523162842, 0.6261939406394958)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.6397104263305664)\n",
      "Global loss at the end of epoch: 106 : 16.647313475608826\n",
      "--> Global test accuracy after epoch: 106 --> (0.1759999968111515, 2.5679141759872435)\n",
      "\n",
      "----------------------- Epoch 107  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7999999523162842, 0.621996283531189)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.6936984062194824)\n",
      "Global loss at the end of epoch: 107 : 16.337239384651184\n",
      "--> Global test accuracy after epoch: 107 --> (0.17799999602138997, 2.598572814464569)\n",
      "\n",
      "----------------------- Epoch 108  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7999999523162842, 0.5865052342414856)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.673428535461426)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global loss at the end of epoch: 108 : 15.908565402030945\n",
      "--> Global test accuracy after epoch: 108 --> (0.17399999611079692, 2.6378873229026794)\n",
      "\n",
      "----------------------- Epoch 109  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7999999523162842, 0.5695369839668274)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.6740024089813232)\n",
      "Global loss at the end of epoch: 109 : 15.594175159931183\n",
      "--> Global test accuracy after epoch: 109 --> (0.18199999630451202, 2.676229691505432)\n",
      "\n",
      "----------------------- Epoch 110  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7599999904632568, 0.5480635166168213)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.6384541988372803)\n",
      "Global loss at the end of epoch: 110 : 15.34645265340805\n",
      "--> Global test accuracy after epoch: 110 --> (0.18199999555945395, 2.7193612813949586)\n",
      "\n",
      "----------------------- Epoch 111  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8399999737739563, 0.538973867893219)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.622800350189209)\n",
      "Global loss at the end of epoch: 111 : 15.300998151302338\n",
      "--> Global test accuracy after epoch: 111 --> (0.19799999482929706, 2.7838278889656065)\n",
      "\n",
      "----------------------- Epoch 112  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7999999523162842, 0.5507827401161194)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.620546579360962)\n",
      "Global loss at the end of epoch: 112 : 15.253916919231415\n",
      "--> Global test accuracy after epoch: 112 --> (0.19599999450147151, 2.8555214166641236)\n",
      "\n",
      "----------------------- Epoch 113  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8399999737739563, 0.5596833825111389)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.6374380588531494)\n",
      "Global loss at the end of epoch: 113 : 15.405301809310913\n",
      "--> Global test accuracy after epoch: 113 --> (0.19199999421834946, 2.909527003765106)\n",
      "\n",
      "----------------------- Epoch 114  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8399999737739563, 0.5316935181617737)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.640299081802368)\n",
      "Global loss at the end of epoch: 114 : 15.48048996925354\n",
      "--> Global test accuracy after epoch: 114 --> (0.18799999617040158, 2.8657931089401245)\n",
      "\n",
      "----------------------- Epoch 115  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8799999952316284, 0.4712807834148407)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.718435287475586)\n",
      "Global loss at the end of epoch: 115 : 15.458651572465897\n",
      "--> Global test accuracy after epoch: 115 --> (0.18799999505281448, 2.8869479060173036)\n",
      "\n",
      "----------------------- Epoch 116  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.4866212010383606)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.805985450744629)\n",
      "Global loss at the end of epoch: 116 : 15.642411172389984\n",
      "--> Global test accuracy after epoch: 116 --> (0.17599999643862246, 2.9000093221664427)\n",
      "\n",
      "----------------------- Epoch 117  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.6800000071525574, 0.6165947914123535)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.7968311309814453)\n",
      "Global loss at the end of epoch: 117 : 15.322791397571564\n",
      "--> Global test accuracy after epoch: 117 --> (0.1819999948143959, 2.978770172595978)\n",
      "\n",
      "----------------------- Epoch 118  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.7599999904632568, 0.6355409026145935)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.744025945663452)\n",
      "Global loss at the end of epoch: 118 : 16.00419718027115\n",
      "--> Global test accuracy after epoch: 118 --> (0.17199999541044236, 2.9766114950180054)\n",
      "\n",
      "----------------------- Epoch 119  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.5999999642372131, 0.738679826259613)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.8246755599975586)\n",
      "Global loss at the end of epoch: 119 : 15.1154066324234\n",
      "--> Global test accuracy after epoch: 119 --> (0.1739999957382679, 2.9902610063552855)\n",
      "\n",
      "----------------------- Epoch 120  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8399999737739563, 0.43581363558769226)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.8012828826904297)\n",
      "Global loss at the end of epoch: 120 : 14.319180876016617\n",
      "--> Global test accuracy after epoch: 120 --> (0.18399999439716339, 3.0267712354660032)\n",
      "\n",
      "----------------------- Epoch 121  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.8399999737739563, 0.46300628781318665)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.8769800662994385)\n",
      "Global loss at the end of epoch: 121 : 13.668185502290726\n",
      "--> Global test accuracy after epoch: 121 --> (0.19199999570846557, 3.068003797531128)\n",
      "\n",
      "----------------------- Epoch 122  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9199999570846558, 0.4306654632091522)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.8072116374969482)\n",
      "Global loss at the end of epoch: 122 : 13.490209013223648\n",
      "--> Global test accuracy after epoch: 122 --> (0.18999999687075614, 3.089515471458435)\n",
      "\n",
      "----------------------- Epoch 123  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.3795453608036041)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.7508609294891357)\n",
      "Global loss at the end of epoch: 123 : 12.837643474340439\n",
      "--> Global test accuracy after epoch: 123 --> (0.16999999508261682, 3.09309743642807)\n",
      "\n",
      "----------------------- Epoch 124  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.3626614511013031)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.8500397205352783)\n",
      "Global loss at the end of epoch: 124 : 12.292564690113068\n",
      "--> Global test accuracy after epoch: 124 --> (0.177999996766448, 3.1474174737930296)\n",
      "\n",
      "----------------------- Epoch 125  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9199999570846558, 0.3804532587528229)\n",
      "Local test acc and loss at the end of set: 0 --> (0.23999999463558197, 2.9841442108154297)\n",
      "Global loss at the end of epoch: 125 : 12.144473373889923\n",
      "--> Global test accuracy after epoch: 125 --> (0.17199999578297137, 3.172457015514374)\n",
      "\n",
      "----------------------- Epoch 126  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.3545968532562256)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.970290184020996)\n",
      "Global loss at the end of epoch: 126 : 11.566707640886307\n",
      "--> Global test accuracy after epoch: 126 --> (0.1719999961555004, 3.224105107784271)\n",
      "\n",
      "----------------------- Epoch 127  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.33226051926612854)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.9696555137634277)\n",
      "Global loss at the end of epoch: 127 : 11.24454253911972\n",
      "--> Global test accuracy after epoch: 127 --> (0.18199999630451202, 3.2769452929496765)\n",
      "\n",
      "----------------------- Epoch 128  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.314859002828598)\n",
      "Local test acc and loss at the end of set: 0 --> (0.19999998807907104, 2.994079828262329)\n",
      "Global loss at the end of epoch: 128 : 11.047539472579956\n",
      "--> Global test accuracy after epoch: 128 --> (0.177999996393919, 3.32306227684021)\n",
      "\n",
      "----------------------- Epoch 129  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.3042405843734741)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.0145699977874756)\n",
      "Global loss at the end of epoch: 129 : 10.841232985258102\n",
      "--> Global test accuracy after epoch: 129 --> (0.17599999643862246, 3.3692409873008726)\n",
      "\n",
      "----------------------- Epoch 130  -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.2954214811325073)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.079030990600586)\n",
      "Global loss at the end of epoch: 130 : 10.66417345404625\n",
      "--> Global test accuracy after epoch: 130 --> (0.177999996393919, 3.410180914402008)\n",
      "\n",
      "----------------------- Epoch 131  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.2909676730632782)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.142622709274292)\n",
      "Global loss at the end of epoch: 131 : 10.531276434659958\n",
      "--> Global test accuracy after epoch: 131 --> (0.17199999541044236, 3.446195113658905)\n",
      "\n",
      "----------------------- Epoch 132  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.2864277958869934)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.1837680339813232)\n",
      "Global loss at the end of epoch: 132 : 10.418351083993912\n",
      "--> Global test accuracy after epoch: 132 --> (0.16799999512732028, 3.48101966381073)\n",
      "\n",
      "----------------------- Epoch 133  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.28159481287002563)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.194157123565674)\n",
      "Global loss at the end of epoch: 133 : 10.35825365781784\n",
      "--> Global test accuracy after epoch: 133 --> (0.1659999955445528, 3.523905348777771)\n",
      "\n",
      "----------------------- Epoch 134  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.2737177908420563)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.155643939971924)\n",
      "Global loss at the end of epoch: 134 : 10.074102222919464\n",
      "--> Global test accuracy after epoch: 134 --> (0.17199999541044236, 3.556324028968811)\n",
      "\n",
      "----------------------- Epoch 135  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.26792290806770325)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.135441780090332)\n",
      "Global loss at the end of epoch: 135 : 9.810119569301605\n",
      "--> Global test accuracy after epoch: 135 --> (0.16799999512732028, 3.6001622080802917)\n",
      "\n",
      "----------------------- Epoch 136  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.26070043444633484)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.16157603263855)\n",
      "Global loss at the end of epoch: 136 : 9.602175444364548\n",
      "--> Global test accuracy after epoch: 136 --> (0.1659999955445528, 3.6427155256271364)\n",
      "\n",
      "----------------------- Epoch 137  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.25259730219841003)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.2245278358459473)\n",
      "Global loss at the end of epoch: 137 : 9.406473278999329\n",
      "--> Global test accuracy after epoch: 137 --> (0.1659999955445528, 3.6783629655838013)\n",
      "\n",
      "----------------------- Epoch 138  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.24260185658931732)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.3233654499053955)\n",
      "Global loss at the end of epoch: 138 : 9.353393539786339\n",
      "--> Global test accuracy after epoch: 138 --> (0.17599999494850635, 3.735624146461487)\n",
      "\n",
      "----------------------- Epoch 139  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.22951236367225647)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.411214828491211)\n",
      "Global loss at the end of epoch: 139 : 9.19205629825592\n",
      "--> Global test accuracy after epoch: 139 --> (0.1739999946206808, 3.77765816450119)\n",
      "\n",
      "----------------------- Epoch 140  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.22352325916290283)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.4258017539978027)\n",
      "Global loss at the end of epoch: 140 : 9.076957762241364\n",
      "--> Global test accuracy after epoch: 140 --> (0.1799999952316284, 3.8372900247573853)\n",
      "\n",
      "----------------------- Epoch 141  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.22423329949378967)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.3795101642608643)\n",
      "Global loss at the end of epoch: 141 : 9.034292817115784\n",
      "--> Global test accuracy after epoch: 141 --> (0.17599999383091927, 3.882954204082489)\n",
      "\n",
      "----------------------- Epoch 142  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.21514804661273956)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.3312172889709473)\n",
      "Global loss at the end of epoch: 142 : 8.913307145237923\n",
      "--> Global test accuracy after epoch: 142 --> (0.17199999429285526, 3.8983149409294127)\n",
      "\n",
      "----------------------- Epoch 143  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.208908811211586)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.3541808128356934)\n",
      "Global loss at the end of epoch: 143 : 8.674002274870872\n",
      "--> Global test accuracy after epoch: 143 --> (0.16599999479949473, 3.907059609889984)\n",
      "\n",
      "----------------------- Epoch 144  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.21247966587543488)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.425156593322754)\n",
      "Global loss at the end of epoch: 144 : 8.460220351815224\n",
      "--> Global test accuracy after epoch: 144 --> (0.1639999955892563, 3.9317018032073974)\n",
      "\n",
      "----------------------- Epoch 145  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (0.9599999785423279, 0.20865510404109955)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.509047508239746)\n",
      "Global loss at the end of epoch: 145 : 8.359770938754082\n",
      "--> Global test accuracy after epoch: 145 --> (0.16199999526143075, 3.972874569892883)\n",
      "\n",
      "----------------------- Epoch 146  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.20260554552078247)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.5555124282836914)\n",
      "Global loss at the end of epoch: 146 : 8.238522231578827\n",
      "--> Global test accuracy after epoch: 146 --> (0.16799999475479127, 4.00942850112915)\n",
      "\n",
      "----------------------- Epoch 147  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.1926356703042984)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.5592105388641357)\n",
      "Global loss at the end of epoch: 147 : 8.093571677803993\n",
      "--> Global test accuracy after epoch: 147 --> (0.17199999392032622, 4.076758766174317)\n",
      "\n",
      "----------------------- Epoch 148  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.18281768262386322)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.51652455329895)\n",
      "Global loss at the end of epoch: 148 : 8.010093301534653\n",
      "--> Global test accuracy after epoch: 148 --> (0.1679999940097332, 4.1111796498298645)\n",
      "\n",
      "----------------------- Epoch 149  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.1775282472372055)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.519188642501831)\n",
      "Global loss at the end of epoch: 149 : 7.875442147254944\n",
      "--> Global test accuracy after epoch: 149 --> (0.16799999438226224, 4.1340328574180605)\n",
      "\n",
      "----------------------- Epoch 150  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.17510543763637543)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.55229115486145)\n",
      "Global loss at the end of epoch: 150 : 7.743858516216278\n",
      "--> Global test accuracy after epoch: 150 --> (0.16399999447166919, 4.150444602966308)\n",
      "\n",
      "----------------------- Epoch 151  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.17014437913894653)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.6220946311950684)\n",
      "Global loss at the end of epoch: 151 : 7.609381332993507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Global test accuracy after epoch: 151 --> (0.16799999438226224, 4.18933287858963)\n",
      "\n",
      "----------------------- Epoch 152  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.16656121611595154)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.657118320465088)\n",
      "Global loss at the end of epoch: 152 : 7.5179958045482635\n",
      "--> Global test accuracy after epoch: 152 --> (0.16399999447166919, 4.218471944332123)\n",
      "\n",
      "----------------------- Epoch 153  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.1624755859375)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.6641721725463867)\n",
      "Global loss at the end of epoch: 153 : 7.419630840420723\n",
      "--> Global test accuracy after epoch: 153 --> (0.16399999447166919, 4.2578210711479185)\n",
      "\n",
      "----------------------- Epoch 154  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.15902835130691528)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.6406826972961426)\n",
      "Global loss at the end of epoch: 154 : 7.342084839940071\n",
      "--> Global test accuracy after epoch: 154 --> (0.16399999447166919, 4.285182404518127)\n",
      "\n",
      "----------------------- Epoch 155  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.15432564914226532)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.662125825881958)\n",
      "Global loss at the end of epoch: 155 : 7.245428517460823\n",
      "--> Global test accuracy after epoch: 155 --> (0.1659999940544367, 4.313545548915863)\n",
      "\n",
      "----------------------- Epoch 156  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.1506865918636322)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.699721574783325)\n",
      "Global loss at the end of epoch: 156 : 7.156655579805374\n",
      "--> Global test accuracy after epoch: 156 --> (0.16199999451637268, 4.329796731472015)\n",
      "\n",
      "----------------------- Epoch 157  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.1488770693540573)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.722733736038208)\n",
      "Global loss at the end of epoch: 157 : 7.0769181698560715\n",
      "--> Global test accuracy after epoch: 157 --> (0.16199999451637268, 4.354228937625885)\n",
      "\n",
      "----------------------- Epoch 158  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.14610250294208527)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.738804817199707)\n",
      "Global loss at the end of epoch: 158 : 6.9996156096458435\n",
      "--> Global test accuracy after epoch: 158 --> (0.17199999354779721, 4.394723987579345)\n",
      "\n",
      "----------------------- Epoch 159  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.14073343575000763)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.7397522926330566)\n",
      "Global loss at the end of epoch: 159 : 6.946571573615074\n",
      "--> Global test accuracy after epoch: 159 --> (0.16999999471008778, 4.426412963867188)\n",
      "\n",
      "----------------------- Epoch 160  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.13937532901763916)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.7498836517333984)\n",
      "Global loss at the end of epoch: 160 : 6.883171185851097\n",
      "--> Global test accuracy after epoch: 160 --> (0.16399999447166919, 4.435358548164368)\n",
      "\n",
      "----------------------- Epoch 161  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.13818930089473724)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.790646553039551)\n",
      "Global loss at the end of epoch: 161 : 6.786534547805786\n",
      "--> Global test accuracy after epoch: 161 --> (0.16399999447166919, 4.454489922523498)\n",
      "\n",
      "----------------------- Epoch 162  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.13595730066299438)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.8030996322631836)\n",
      "Global loss at the end of epoch: 162 : 6.714040637016296\n",
      "--> Global test accuracy after epoch: 162 --> (0.16599999442696572, 4.484536182880402)\n",
      "\n",
      "----------------------- Epoch 163  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.13399745523929596)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.7983014583587646)\n",
      "Global loss at the end of epoch: 163 : 6.64577804505825\n",
      "--> Global test accuracy after epoch: 163 --> (0.16599999442696572, 4.507790768146515)\n",
      "\n",
      "----------------------- Epoch 164  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.13018079102039337)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.8262665271759033)\n",
      "Global loss at the end of epoch: 164 : 6.596618637442589\n",
      "--> Global test accuracy after epoch: 164 --> (0.16599999442696572, 4.532728815078736)\n",
      "\n",
      "----------------------- Epoch 165  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.12914052605628967)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.835780382156372)\n",
      "Global loss at the end of epoch: 165 : 6.533272817730904\n",
      "--> Global test accuracy after epoch: 165 --> (0.16399999447166919, 4.558632671833038)\n",
      "\n",
      "----------------------- Epoch 166  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.12653273344039917)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.870166063308716)\n",
      "Global loss at the end of epoch: 166 : 6.483150660991669\n",
      "--> Global test accuracy after epoch: 166 --> (0.16599999442696572, 4.569362843036652)\n",
      "\n",
      "----------------------- Epoch 167  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.12526817619800568)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.872121572494507)\n",
      "Global loss at the end of epoch: 167 : 6.420001998543739\n",
      "--> Global test accuracy after epoch: 167 --> (0.16599999442696572, 4.587793362140656)\n",
      "\n",
      "----------------------- Epoch 168  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.1239926889538765)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.8793630599975586)\n",
      "Global loss at the end of epoch: 168 : 6.3717241659760475\n",
      "--> Global test accuracy after epoch: 168 --> (0.1679999940097332, 4.606245863437652)\n",
      "\n",
      "----------------------- Epoch 169  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.12246517091989517)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.888144016265869)\n",
      "Global loss at the end of epoch: 169 : 6.320884130895138\n",
      "--> Global test accuracy after epoch: 169 --> (0.1679999940097332, 4.625639307498932)\n",
      "\n",
      "----------------------- Epoch 170  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.12250875681638718)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.887946367263794)\n",
      "Global loss at the end of epoch: 170 : 6.278782360255718\n",
      "--> Global test accuracy after epoch: 170 --> (0.1679999940097332, 4.6492567420005795)\n",
      "\n",
      "----------------------- Epoch 171  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.11922148615121841)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.9270317554473877)\n",
      "Global loss at the end of epoch: 171 : 6.232039295136929\n",
      "--> Global test accuracy after epoch: 171 --> (0.16599999442696572, 4.658698892593383)\n",
      "\n",
      "----------------------- Epoch 172  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.11860661208629608)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.9357199668884277)\n",
      "Global loss at the end of epoch: 172 : 6.190976470708847\n",
      "--> Global test accuracy after epoch: 172 --> (0.17199999392032622, 4.681541621685028)\n",
      "\n",
      "----------------------- Epoch 173  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.11645800620317459)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.9432315826416016)\n",
      "Global loss at the end of epoch: 173 : 6.1549931690096855\n",
      "--> Global test accuracy after epoch: 173 --> (0.1679999940097332, 4.686560070514679)\n",
      "\n",
      "----------------------- Epoch 174  -----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.11642985045909882)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.945157766342163)\n",
      "Global loss at the end of epoch: 174 : 6.112835392355919\n",
      "--> Global test accuracy after epoch: 174 --> (0.1679999940097332, 4.7017657995224)\n",
      "\n",
      "----------------------- Epoch 175  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.11507392674684525)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.955948829650879)\n",
      "Global loss at the end of epoch: 175 : 6.073535360395908\n",
      "--> Global test accuracy after epoch: 175 --> (0.16999999433755875, 4.720366585254669)\n",
      "\n",
      "----------------------- Epoch 176  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.11272719502449036)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.970547676086426)\n",
      "Global loss at the end of epoch: 176 : 6.044090524315834\n",
      "--> Global test accuracy after epoch: 176 --> (0.1679999940097332, 4.739313805103302)\n",
      "\n",
      "----------------------- Epoch 177  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.11209812760353088)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.9755172729492188)\n",
      "Global loss at the end of epoch: 177 : 6.0152765065431595\n",
      "--> Global test accuracy after epoch: 177 --> (0.16399999409914018, 4.739931833744049)\n",
      "\n",
      "----------------------- Epoch 178  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.11205349117517471)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.9879250526428223)\n",
      "Global loss at the end of epoch: 178 : 5.978853665292263\n",
      "--> Global test accuracy after epoch: 178 --> (0.16599999442696572, 4.754113936424256)\n",
      "\n",
      "----------------------- Epoch 179  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.11082062870264053)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 3.991966485977173)\n",
      "Global loss at the end of epoch: 179 : 5.944424651563168\n",
      "--> Global test accuracy after epoch: 179 --> (0.1679999940097332, 4.7705984830856325)\n",
      "\n",
      "----------------------- Epoch 180  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10937543213367462)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.002485752105713)\n",
      "Global loss at the end of epoch: 180 : 5.922555208206177\n",
      "--> Global test accuracy after epoch: 180 --> (0.16399999409914018, 4.777763259410858)\n",
      "\n",
      "----------------------- Epoch 181  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10956484824419022)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.005102157592773)\n",
      "Global loss at the end of epoch: 181 : 5.890378586947918\n",
      "--> Global test accuracy after epoch: 181 --> (0.16399999409914018, 4.78783757686615)\n",
      "\n",
      "----------------------- Epoch 182  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10885348170995712)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.016680717468262)\n",
      "Global loss at the end of epoch: 182 : 5.866239421069622\n",
      "--> Global test accuracy after epoch: 182 --> (0.1679999940097332, 4.8031787991523744)\n",
      "\n",
      "----------------------- Epoch 183  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10765626281499863)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.019205093383789)\n",
      "Global loss at the end of epoch: 183 : 5.843799866735935\n",
      "--> Global test accuracy after epoch: 183 --> (0.1679999940097332, 4.809159004688263)\n",
      "\n",
      "----------------------- Epoch 184  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10740143805742264)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.022888660430908)\n",
      "Global loss at the end of epoch: 184 : 5.819874607026577\n",
      "--> Global test accuracy after epoch: 184 --> (0.16399999409914018, 4.817245471477508)\n",
      "\n",
      "----------------------- Epoch 185  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10684356093406677)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.039175033569336)\n",
      "Global loss at the end of epoch: 185 : 5.797148048877716\n",
      "--> Global test accuracy after epoch: 185 --> (0.1679999940097332, 4.830206274986267)\n",
      "\n",
      "----------------------- Epoch 186  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10559878498315811)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.046282768249512)\n",
      "Global loss at the end of epoch: 186 : 5.778657637536526\n",
      "--> Global test accuracy after epoch: 186 --> (0.1679999940097332, 4.834972882270813)\n",
      "\n",
      "----------------------- Epoch 187  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10569194704294205)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.039990425109863)\n",
      "Global loss at the end of epoch: 187 : 5.758032433688641\n",
      "--> Global test accuracy after epoch: 187 --> (0.16599999368190765, 4.842214357852936)\n",
      "\n",
      "----------------------- Epoch 188  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10533013939857483)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.051161289215088)\n",
      "Global loss at the end of epoch: 188 : 5.737176612019539\n",
      "--> Global test accuracy after epoch: 188 --> (0.1679999940097332, 4.851967847347259)\n",
      "\n",
      "----------------------- Epoch 189  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10423438251018524)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.0609331130981445)\n",
      "Global loss at the end of epoch: 189 : 5.720008343458176\n",
      "--> Global test accuracy after epoch: 189 --> (0.1679999940097332, 4.864078712463379)\n",
      "\n",
      "----------------------- Epoch 190  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10352364182472229)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.065583229064941)\n",
      "Global loss at the end of epoch: 190 : 5.70847961306572\n",
      "--> Global test accuracy after epoch: 190 --> (0.1679999940097332, 4.8659546852111815)\n",
      "\n",
      "----------------------- Epoch 191  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.1035824567079544)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.061770915985107)\n",
      "Global loss at the end of epoch: 191 : 5.689580619335175\n",
      "--> Global test accuracy after epoch: 191 --> (0.16599999368190765, 4.870438838005066)\n",
      "\n",
      "----------------------- Epoch 192  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10366957634687424)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.067803382873535)\n",
      "Global loss at the end of epoch: 192 : 5.674399368464947\n",
      "--> Global test accuracy after epoch: 192 --> (0.1679999940097332, 4.883124577999115)\n",
      "\n",
      "----------------------- Epoch 193  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10227050632238388)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.077699184417725)\n",
      "Global loss at the end of epoch: 193 : 5.660900793969631\n",
      "--> Global test accuracy after epoch: 193 --> (0.1679999940097332, 4.88618278503418)\n",
      "\n",
      "----------------------- Epoch 194  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10223716497421265)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.078199863433838)\n",
      "Global loss at the end of epoch: 194 : 5.645861200988293\n",
      "--> Global test accuracy after epoch: 194 --> (0.1679999940097332, 4.892452597618103)\n",
      "\n",
      "----------------------- Epoch 195  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10195380449295044)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.078549861907959)\n",
      "Global loss at the end of epoch: 195 : 5.631375692784786\n",
      "--> Global test accuracy after epoch: 195 --> (0.1679999940097332, 4.896613001823425)\n",
      "\n",
      "----------------------- Epoch 196  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10159566253423691)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.08360481262207)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global loss at the end of epoch: 196 : 5.618761591613293\n",
      "--> Global test accuracy after epoch: 196 --> (0.1679999940097332, 4.905916237831116)\n",
      "\n",
      "----------------------- Epoch 197  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.10120054334402084)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.092220306396484)\n",
      "Global loss at the end of epoch: 197 : 5.606085509061813\n",
      "--> Global test accuracy after epoch: 197 --> (0.1679999940097332, 4.909440529346466)\n",
      "\n",
      "----------------------- Epoch 198  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.1007632240653038)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.097860336303711)\n",
      "Global loss at the end of epoch: 198 : 5.593737252056599\n",
      "--> Global test accuracy after epoch: 198 --> (0.1679999940097332, 4.915054953098297)\n",
      "\n",
      "----------------------- Epoch 199  -----------------------\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.1003924310207367)\n",
      "Local test acc and loss at the end of set: 0 --> (0.1599999964237213, 4.096972465515137)\n",
      "Global loss at the end of epoch: 199 : 5.581700652837753\n",
      "--> Global test accuracy after epoch: 199 --> (0.1679999940097332, 4.921018385887146)\n",
      "\n",
      "END OF ITERATION: 1\n"
     ]
    }
   ],
   "source": [
    "# Configure training.\n",
    "nepochs=200\n",
    "# epoch after which adversarial training starts\n",
    "do_adv_train = 10000\n",
    "# K-shot k-way\n",
    "Ks = 5\n",
    "# Length of the embeddings produced by the CNN\n",
    "z_len = 100\n",
    "\n",
    "load_weights = 0\n",
    "continue_training = 0\n",
    "\n",
    "# Array storing statistics (not used for now)\n",
    "accuracies_dataset_0 = []\n",
    "accuracies_dataset_0_adv = []\n",
    "accuracies_dataset_1 = []\n",
    "accuracies_dataset_1_adv = []\n",
    "\n",
    "# Loop in case we want to do statistics (not sued for now)\n",
    "for o in range(1):\n",
    "    print(\"Iteration\", o+1)\n",
    "    \n",
    "    if continue_training == 0:\n",
    "        # Models definition\n",
    "        kcnn = LeNet(z_len).to(device)\n",
    "        mnet = MLP(n_in=Ks, n_out=Ks, hidden_layers=[64, 32, 16]).to(device)\n",
    "        hnet = HMLP(mnet.param_shapes, uncond_in_size=Ks*Ks, cond_in_size=0,\n",
    "                    layers=[128, 64, 32], num_cond_embs=0).to(device)\n",
    "        params = hnet.conditional_params.copy()\n",
    "        hnet.apply_hyperfan_init(mnet=mnet)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # If we want to load weights from anywhere\n",
    "        if load_weights == 1:\n",
    "            file_path = 'models/hnet_20231229022719_49.pth'\n",
    "            hnet.load_state_dict(torch.load(file_path))\n",
    "            file_path = 'models/kcnn_20231229022719_49.pth'\n",
    "            kcnn.load_state_dict(torch.load(file_path))\n",
    "\n",
    "        # The amount of sets of Ks labels we can do during training\n",
    "        n_sets = int(len(lbls_0) / Ks)\n",
    "\n",
    "        # Compute training and validation sets for each of the n_sets labels sets\n",
    "        train_test_sets = []\n",
    "        all_test_sets = np.empty((0, dataset_0.shape[1]))\n",
    "        all_test_sets_lbl = np.empty((0))\n",
    "        for l_set_id in range(n_sets):\n",
    "            c_lbls = lbls_0[l_set_id*Ks:(l_set_id+1)*Ks]\n",
    "            if (l_set_id+1) % 100 == 0:\n",
    "                print(\"Generated train-test split for\", l_set_id+1,\"/\",n_sets)\n",
    "            mask_b = np.isin(dataset_0_lbl, np.array(c_lbls))\n",
    "            dataset_0_b, dataset_0_lbl_b = dataset_0[mask_b], dataset_0_lbl[mask_b]\n",
    "            dataset_0_train, dataset_0_test, dataset_0_lbl_train, dataset_0_lbl_test = \\\n",
    "                            train_test_split(dataset_0_b, dataset_0_lbl_b, random_state=42, test_size=0.5, stratify=dataset_0_lbl_b)\n",
    "            \n",
    "            rotate_m10 = rotate_dataset(dataset_0_train, -10)\n",
    "            rotate_m5 = rotate_dataset(dataset_0_train, -5)\n",
    "            rotate_10 = rotate_dataset(dataset_0_train, 10)\n",
    "            rotate_5 = rotate_dataset(dataset_0_train, 5)\n",
    "            zoom_110 = zoom_dataset(dataset_0_train, 1.10)\n",
    "            zoom_125 = zoom_dataset(dataset_0_train, 1.25)\n",
    "            \n",
    "            assert(rotate_m10.shape == dataset_0_train.shape)\n",
    "            assert(zoom_125.shape == dataset_0_train.shape)\n",
    "            \n",
    "            dataset_0_train = np.concatenate((dataset_0_train, rotate_m10, rotate_m5,\n",
    "                                                                           rotate_5,\n",
    "                                                                           rotate_10,\n",
    "                                                                           zoom_110,\n",
    "                                                                           zoom_125), axis=0)\n",
    "            dataset_0_lbl_train = np.repeat(dataset_0_lbl_train, 7, axis=0)\n",
    "            \n",
    "            \n",
    "            all_test_sets = np.concatenate((all_test_sets, dataset_0_test), axis=0)\n",
    "            all_test_sets_lbl = np.concatenate((all_test_sets_lbl, dataset_0_lbl_test), axis=0)\n",
    "            train_test_sets.append((dataset_0_train, dataset_0_test, dataset_0_lbl_train, dataset_0_lbl_test, c_lbls))\n",
    "    \n",
    "    # Optimizer and scheduler initialization\n",
    "    # TODO-yz: it's correct you only have 2 optimizers here. if you introduce a kernel function f, you'll need a third one though\n",
    "    optimizer = optim.Adam(hnet.parameters(), lr=0.00005)\n",
    "    optimizer_s = optim.Adam(kcnn.parameters(), lr=0.00005)\n",
    "    # optimizer = optim.SGD(hnet.parameters(), lr=0.0005, momentum=0.9)\n",
    "    # optimizer_s = optim.SGD(kcnn.parameters(), lr=0.0005, momentum=0.9)\n",
    "    # lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, threshold=0.01, patience=3, verbose=True)\n",
    "    # lr_scheduler_s = ReduceLROnPlateau(optimizer_s, mode='min', factor=0.1, threshold=0.01, patience=3, verbose=True)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=int(nepochs / 1), eta_min=0.000001)\n",
    "    scheduler_s = CosineAnnealingLR(optimizer_s, T_max=int(nepochs / 1), eta_min=0.000001)\n",
    "        \n",
    "    # Main training loop\n",
    "    for epoch in range(nepochs): # For each epoch.\n",
    "        print(\"----------------------- Epoch\", epoch, \" -----------------------\")\n",
    "        # Stores the loss over all labels sets\n",
    "        global_loss = 0.0\n",
    "        global_loss_float = 0.0\n",
    "        # We loop over all our sets at each epoch\n",
    "        for l_set_id in range(n_sets):\n",
    "            loss_dataset_l = 0.0\n",
    "            (dataset_l_train, dataset_l_test, dataset_l_lbl_train, dataset_l_lbl_test, c_lbls) = train_test_sets[l_set_id]\n",
    "            \n",
    "            s_set_train, s_set_lbl_train, q_set_train, q_set_lbl_train, z_space, K, all_q_features, all_q_features_lbls = \\\n",
    "            compute_sets_and_features(dataset_l_train, dataset_l_lbl_train, c_lbls, kcnn, Ks, 5)\n",
    "            \n",
    "            # Formward pass\n",
    "            W_dataset_l = hnet(uncond_input=K.view(1, -1))\n",
    "            dataset_l_P = mnet.forward(all_q_features, weights=W_dataset_l)\n",
    "            prediction_extended = extend_pred_to_nclasses(dataset_l_P, n_classes, c_lbls)\n",
    "            loss_dataset_l += criterion(prediction_extended, all_q_features_lbls.long())\n",
    "\n",
    "            # Adversarial training\n",
    "            # TODO-yz: think it's good to do adversarial querying, this gives a more meaningful baseline result\n",
    "            if epoch == do_adv_train and l_set_id == 0:\n",
    "                print(\"Adversarial training starts.\")\n",
    "            if epoch >= do_adv_train:\n",
    "                mx_adv = pgd_attack_data(q_set_train, q_set_lbl_train, mnet, hnet, Ks, kcnn, K, z_space)\n",
    "                \n",
    "                all_q_features_adv = torch.zeros((q_set_train.shape[0], Ks)).to(device)\n",
    "                for i in range(mx_adv.shape[0]):\n",
    "                    mxx = mx_adv[i].view(-1, q_set_train.shape[1])\n",
    "                    q_sample_features_adv = get_q_sample_features(mxx, kcnn, K, z_space)\n",
    "                    all_q_features_adv[i] = q_sample_features_adv\n",
    "                dataset_l_P_adv = mnet.forward(all_q_features_adv, weights=W_dataset_l)\n",
    "                prediction_extended_adv = extend_pred_to_nclasses(dataset_l_P_adv, n_classes, c_lbls)\n",
    "                loss_dataset_l_adv = criterion(prediction_extended_adv, all_q_features_lbls.long())\n",
    "                loss_dataset_l += loss_dataset_l_adv\n",
    "            \n",
    "            global_loss += loss_dataset_l\n",
    "            global_loss_float += loss_dataset_l.item()\n",
    "            if l_set_id % 100 == 0:\n",
    "                train_metrics = calc_accuracy(dataset_l_train, dataset_l_lbl_train, hnet, mnet, Ks, kcnn, n_classes, 5)\n",
    "                test_metrics = calc_accuracy(dataset_l_test, dataset_l_lbl_test, hnet, mnet, Ks, kcnn, n_classes, 5)\n",
    "                print(\"Local train acc and loss at the end of set:\", l_set_id, \"-->\", train_metrics)\n",
    "                print(\"Local test acc and loss at the end of set:\", l_set_id, \"-->\", test_metrics)\n",
    "                if do_adv_train < nepochs:\n",
    "                    s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test, z_space_tes, K_test, all_q_features_test, all_q_features_lbls_test = \\\n",
    "                    compute_sets_and_features(dataset_l_test, dataset_l_lbl_test, c_lbls, kcnn, Ks, 5)\n",
    "                    mx_adv_test = pgd_attack_data(q_set_test, q_set_lbl_test, mnet, hnet, Ks, kcnn, K, z_space)\n",
    "                    print(\"Local adv test acc and loss at the end of set:\", l_set_id, \"-->\", calc_accuracy_lbls_adv(dataset_l_test, dataset_l_lbl_test, c_lbls, hnet, mnet,\\\n",
    "                                                                                           Ks, kcnn, mx_adv_test))\n",
    "           \n",
    "            # global_loss.backward()\n",
    "            loss_dataset_l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer_s.step()\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_s.zero_grad()\n",
    "        # TODO-yz: missing optimizer_s.zero_grad()\n",
    "        scheduler.step()\n",
    "        scheduler_s.step()\n",
    "                \n",
    "  \n",
    "        print(\"Global loss at the end of epoch:\", epoch, \":\", global_loss_float)\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            # Create a file name with the current time\n",
    "            hnet_file = f'models/hnet_{current_time}_{epoch}.pth'\n",
    "            torch.save(hnet.state_dict(), hnet_file)\n",
    "            kcnn_file = f'models/kcnn_{current_time}_{epoch}.pth'\n",
    "            torch.save(kcnn.state_dict(), kcnn_file)\n",
    "            print(\"--> Global test accuracy after epoch:\", epoch, \"-->\", calc_accuracy(all_test_sets, all_test_sets_lbl,\\\n",
    "                                                                                       hnet, mnet, Ks, kcnn, n_classes, 5))\n",
    "        print()\n",
    "\n",
    "    print(\"END OF ITERATION:\",o+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd33649",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# Create a file name with the current time\n",
    "hnet_file = f'models/hnet_{current_time}_best.pth'\n",
    "torch.save(hnet.state_dict(), hnet_file)\n",
    "kcnn_file = f'models/kcnn_{current_time}_best.pth'\n",
    "torch.save(kcnn.state_dict(), kcnn_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145607b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_adv_dataset_1 = pgd_attack_data(dataset_1, dataset_1_lbl, mnet, hnet, z_space_1, K_1, 1)\n",
    "# x_adv_dataset_1_np = x_adv_dataset_1.detach().cpu().numpy()\n",
    "# x_adv_dataset_0_test = pgd_attack_data(dataset_0_test, dataset_0_lbl_test, mnet, hnet, z_space, K, 0)\n",
    "# x_adv_dataset_0_test_np = x_adv_dataset_0_test.detach().cpu().numpy()\n",
    "\n",
    "print(calc_accuracy(all_test_sets, all_test_sets_lbl, hnet, mnet, Ks, kcnn, n_classes, 5))\n",
    "print(calc_accuracy(dataset_1, dataset_1_lbl, hnet, mnet, Ks, kcnn, n_classes, 5))\n",
    "# accuracies_dataset_0_adv.append((calc_accuracy(x_adv_dataset_0_test_np, dataset_0_lbl_test, mnet, W_dataset_0)).detach().cpu())\n",
    "# accuracies_dataset_1_adv.append((calc_accuracy(x_adv_dataset_1_np, dataset_1_lbl, mnet, W_dataset_1)).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045661ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean:\")\n",
    "print(\"dataset 0 accuracy:\", np.mean(np.array(accuracies_dataset_0)))\n",
    "print(\"dataset 1 accuracy:\", np.mean(np.array(accuracies_dataset_1)))\n",
    "print(\"dataset 0 adv accuracy:\", np.mean(np.array(accuracies_dataset_0_adv)))\n",
    "print(\"dataset 1 adv accuracy:\", np.mean(np.array(accuracies_dataset_1_adv)))\n",
    "print()\n",
    "print(\"Standard deviation:\")\n",
    "print(\"dataset 0 accuracy:\", np.std(np.array(accuracies_dataset_0)))\n",
    "print(\"dataset 1 accuracy:\", np.std(np.array(accuracies_dataset_1)))\n",
    "print(\"dataset 0 adv accuracy:\", np.std(np.array(accuracies_dataset_0_adv)))\n",
    "print(\"dataset 1 adv accuracy:\", np.std(np.array(accuracies_dataset_1_adv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e2e88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
