{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a3ce0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will use: cuda\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torchattacks import PGD\n",
    "import random\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Omniglot\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Will use:\", device)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8780cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29600a",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fae8e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading MNIST dataset ...\n",
      "Elapsed time to read dataset: 0.323039 sec\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from hypnettorch.data import FashionMNISTData, MNISTData\n",
    "from hypnettorch.data.dataset import Dataset\n",
    "from hypnettorch.mnets import LeNet\n",
    "from hypnettorch.mnets.resnet import ResNet\n",
    "from hypnettorch.mnets.mlp import MLP\n",
    "from hypnettorch.hnets import HMLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import learn2learn as l2l\n",
    "import copy\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mnist = MNISTData(data_dir, use_one_hot=True, validation_size=0)\n",
    "fmnist = FashionMNISTData(data_dir, use_one_hot=True, validation_size=0)\n",
    "\n",
    "omniglot = l2l.vision.datasets.FullOmniglot(root=data_dir,\n",
    "                                            transform=transforms.Compose([\n",
    "                                                transforms.Resize(28, interpolation=Image.LANCZOS),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                lambda x: 1.0 - x,\n",
    "                                            ]),\n",
    "                                            download=True)\n",
    "omniglot = l2l.data.MetaDataset(omniglot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e95df",
   "metadata": {},
   "source": [
    "## Convert the dataset to numpy for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf2a597",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension: (32460, 1, 28, 28)\n",
      "Labels dimension: (32460,)\n",
      "0\n",
      "1622\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for batching and shuffling the data\n",
    "batch_size = len(omniglot)  # Set batch size to the total number of examples to load all data at once\n",
    "data_loader = DataLoader(omniglot, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "for batch in data_loader:\n",
    "    images, labels = batch\n",
    "    # Convert PyTorch tensors to NumPy arrays\n",
    "    dataset = images.numpy()\n",
    "    dataset_lbl = labels.numpy()    \n",
    "    sizes = dataset.shape\n",
    "    \n",
    "# print(\"Dataset dimension:\", dataset.shape)\n",
    "# print(\"Labels dimension:\", dataset_lbl.shape)\n",
    "# print(np.min(dataset_lbl))\n",
    "# print(np.max(dataset_lbl))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b48adcc",
   "metadata": {},
   "source": [
    "## Create 2 different datasets for two disjoint set of labels (deterministic for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe5509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0 ... 1622 1622 1622]\n",
      "(32460, 784)\n",
      "(32460,)\n",
      "Shape of the dataset_0: (22000, 784)\n",
      "Shape of the dataset_1: (2000, 784)\n",
      "Some labels in set 1: [2 2 2 2 2 2 2 2 2 2]\n",
      "Some labels in set 2: [0 0 0 0 0 0 0 0 0 0]\n",
      "Minimum and maximum amount of sample per classes in the dataset\n",
      "Each classes contains at least 20 samples\n",
      "Each classes contains at most 20 samples\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of training samples from each data handler.\n",
    "# mnist_inps, mnist_trgts = mnist.next_train_batch(4)\n",
    "# dataset_inps, dataset_trgts = dataset.next_train_batch(4)\n",
    "# dataset_full, dataset_full_lbl = dataset.next_train_batch(60000)\n",
    "print(dataset_lbl)\n",
    "\n",
    "n_classes = len(np.unique(dataset_lbl))\n",
    "dataset_full = dataset.reshape((dataset.shape[0], dataset.shape[2]*dataset.shape[3]))\n",
    "dataset_full_lbl = dataset_lbl\n",
    "\n",
    "# print(dataset_full.shape)\n",
    "# print(dataset_full_lbl.shape)\n",
    "\n",
    "# TODO-yz: you will need to use the same split as is used in evaluation (pull and merge with main to get access to datasets.get_benchmark_tasksets which gives you train/val/test split over classes)\n",
    "classes = list(range(n_classes))\n",
    "random.Random(42).shuffle(classes)\n",
    "lbls_0 = classes[:1100]\n",
    "lbls_1 = classes[1100:1200]\n",
    "\n",
    "mask_0 = np.isin(dataset_full_lbl, np.array(lbls_0))\n",
    "mask_1 = np.isin(dataset_full_lbl, np.array(lbls_1))\n",
    "dataset_0, dataset_0_lbl = dataset_full[mask_0], dataset_full_lbl[mask_0]\n",
    "\n",
    "# print(\"Shape of the dataset_0:\",dataset_0.shape)\n",
    "\n",
    "dataset_1, dataset_1_lbl = dataset_full[mask_1], dataset_full_lbl[mask_1]\n",
    "\n",
    "# print(\"Shape of the dataset_1:\",dataset_1.shape)\n",
    "\n",
    "# print(\"Some labels in set 1:\", dataset_0_lbl[0:10])\n",
    "# print(\"Some labels in set 2:\", dataset_1_lbl[0:10])\n",
    "assert(np.all(np.isin(dataset_0_lbl, lbls_0)))\n",
    "assert(np.all(np.isin(dataset_1_lbl, lbls_1)))\n",
    "\n",
    "torch_dataset = torch.tensor(dataset_full_lbl)\n",
    "unique_values, counts = torch.unique(torch_dataset, return_counts=True)\n",
    "\n",
    "# print(\"Minimum and maximum amount of sample per classes in the dataset\")\n",
    "# print(\"Each classes contains at least\", torch.min(counts).item(), \"samples\")\n",
    "# print(\"Each classes contains at most\", torch.max(counts).item(), \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7356d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom, rotate\n",
    "from scipy.interpolate import interp2d\n",
    "\n",
    "def rotate_dataset(dataset, angle):\n",
    "    dataset_unflatten = dataset.reshape(-1, 1, 28, 28)\n",
    "    rotated_data = rotate(dataset_unflatten, angle, axes=(2, 3), reshape=False)\n",
    "    return rotated_data.reshape(-1, 784)\n",
    "\n",
    "def zoom_dataset(dataset, zoom_factor):\n",
    "    dataset_unflatten = dataset.reshape(-1, 1, 28, 28)\n",
    "    zoomed_dataset = zoom(dataset_unflatten, (1, 1, zoom_factor, zoom_factor), order=1)\n",
    "    \n",
    "    original_size = dataset_unflatten.shape\n",
    "    zoomed_size = zoomed_dataset.shape\n",
    "    diff = int((zoomed_size[2] - original_size[2])/2)\n",
    "    interpolated_data = zoomed_dataset[:,:,diff:diff+original_size[2], diff:diff+original_size[2]]\n",
    "    return interpolated_data.reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4e57d",
   "metadata": {},
   "source": [
    "### Compute a pgd attack on test set to assert robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deaa9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, z_length):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.z_length = z_length\n",
    "        resnet18 = models.resnet18(pretrained=False)\n",
    "        resnet18.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3), bias=False)\n",
    "        resnet18.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, self.z_length)\n",
    "        self.resnet = resnet18\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e0ea965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ResNet12\n",
    "\n",
    "class Hypershot(nn.Module):\n",
    "    def __init__(self, kcnn_input_channels, z_length, kcnn_weights,\n",
    "                       hnet_hidden_layers, hnet_hidden_size, hnet_weights,\n",
    "                       mnet_hidden_layers, mnet_hidden_size,\n",
    "                       K, W, i_dim=28, i_cha=1, load_w = False):\n",
    "        super(Hypershot, self).__init__()\n",
    "        \n",
    "        self.kcnn_input_channels = kcnn_input_channels\n",
    "        self.z_length = z_length\n",
    "        self.kcnn_weights = kcnn_weights\n",
    "        self.hnet_hidden_layers = hnet_hidden_layers\n",
    "        self.hnet_hidden_size = hnet_hidden_size\n",
    "        self.hnet_weights = hnet_weights\n",
    "        self.mnet_hidden_layers = mnet_hidden_layers\n",
    "        self.mnet_hidden_size = mnet_hidden_size\n",
    "        \n",
    "        self.i_dim = i_dim\n",
    "        self.i_cha = i_cha\n",
    "        \n",
    "        self.K = K\n",
    "        self.W = W\n",
    "        self.kernel = None\n",
    "        self.z_space = None\n",
    "        \n",
    "        # self.kcnn = ResNet12(output_size=z_length, hidden_size=64, channels=self.kcnn_input_channels, dropblock_dropout=0, avg_pool=False)\n",
    "        self.kcnn = ResNet(z_length=z_length)\n",
    "        self.mnet = MLP(n_in=W, n_out=W, hidden_layers=self.mnet_hidden_layers * [self.mnet_hidden_size])\n",
    "        # K**2 is the size of the kernel\n",
    "        self.hnet = HMLP(self.mnet.param_shapes, uncond_in_size=W**2, cond_in_size=0, \\\n",
    "                         layers = self.hnet_hidden_layers * [self.hnet_hidden_size],\\\n",
    "                         num_cond_embs=0)\n",
    "        self.hnet.apply_hyperfan_init(mnet=self.mnet)\n",
    "        \n",
    "        if load_w:\n",
    "            hnet.load_state_dict(torch.load(self.hnet_weights))\n",
    "            kcnn.load_state_dict(torch.load(self.kcnn_weights))\n",
    "            \n",
    "    \n",
    "    def compute_kernel(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute Hypershot kernel for a support set X and label y\n",
    "        It takes the average of the z's for each label as suggested in the Hypershot paper\n",
    "\n",
    "        Args:\n",
    "            X (tensor): Support set used to compute the kernel\n",
    "            y (tensor): corresponding labels\n",
    "\n",
    "        Returns:\n",
    "            type: embeddings, kernel\n",
    "        \"\"\"\n",
    "        # Obtain the indices that would sort y_test\n",
    "        indices = torch.argsort(y)\n",
    "\n",
    "        # Use the indices to sort the rows of X_test\n",
    "        sorted_X = X[indices].to(device)\n",
    "        sorted_y = y[indices].to(device)\n",
    "\n",
    "        reshaped_X = sorted_X.view(sorted_X.shape[0], self.i_cha, self.i_dim, self.i_dim).to(device)\n",
    "        nn_X = self.kcnn(reshaped_X)\n",
    "        \n",
    "        mean_X = torch.zeros((int(nn_X.shape[0] / self.K), nn_X.shape[1])).to(device)\n",
    "        for i in range(self.W):\n",
    "            mean_X[i] = torch.mean(nn_X[i*self.K:(i+1)*self.K], dim = 0)\n",
    "        norm_mean_X = F.normalize(mean_X, p=2, dim=1)\n",
    "        \n",
    "        assert(nn_X.shape==(sorted_X.shape[0], self.z_length))\n",
    "        \n",
    "        return norm_mean_X, torch.matmul(norm_mean_X, torch.t(norm_mean_X)) \n",
    "\n",
    "    def get_s_and_q_sets(self, X, y, trgt_lbls, q_size):\n",
    "        \"\"\"\n",
    "        Computes a support set for data X for classes in y with K sample per classes\n",
    "        and corresponding query sets of size q_size.\n",
    "\n",
    "        Args:\n",
    "            X (tensor): Data used to compute the sets (can contain label you do not want for your sets)\n",
    "            y (tensor): corresponding labels\n",
    "            trgt_lbls : the labels that end up in the sets\n",
    "            q_size: amount of sample per classes in query set\n",
    "\n",
    "        Returns:\n",
    "            type: support set, support set labels, query set, query set labels\n",
    "        \"\"\"\n",
    "        s_set = np.zeros((len(trgt_lbls) * self.K, X.shape[1]))\n",
    "        s_set_lbl = np.zeros((len(trgt_lbls) * self.K))\n",
    "\n",
    "        q_set = np.zeros((len(trgt_lbls) * q_size, X.shape[1]))\n",
    "        q_set_lbl = np.zeros((len(trgt_lbls) * q_size))\n",
    "        for j, l in enumerate(trgt_lbls):\n",
    "            mask = (y == l)\n",
    "            masked_data = X[mask]\n",
    "            masked_lbls = y[mask]\n",
    "            s_set[j*self.K:(j+1)*self.K] = masked_data[0:self.K]\n",
    "            s_set_lbl[j*self.K:(j+1)*self.K] = masked_lbls[0:self.K]\n",
    "            q_set[j*q_size:(j+1)*q_size] = masked_data[self.K:self.K+q_size]\n",
    "            q_set_lbl[j*q_size:(j+1)*q_size] = masked_lbls[self.K:self.K+q_size]\n",
    "\n",
    "        s_set = torch.tensor(s_set, requires_grad=True).to(device).float()\n",
    "        s_set_lbl = torch.tensor(s_set_lbl, requires_grad=True).to(device).float()\n",
    "        q_set = torch.tensor(q_set, requires_grad=True).to(device).float()\n",
    "        q_set_lbl = torch.tensor(q_set_lbl, requires_grad=True).to(device).float()\n",
    "        \n",
    "        return s_set, s_set_lbl, q_set, q_set_lbl\n",
    "\n",
    "    def get_q_sample_features(self, X):\n",
    "       \"\"\"\n",
    "        Computes the final features used for classification, given a query sample X\n",
    "\n",
    "        Args:\n",
    "            X (tensor): query sample \n",
    "\n",
    "        Returns:\n",
    "            type: final features use by the main network\n",
    "        \"\"\"\n",
    "        X = X.view(-1, self.i_cha, self.i_dim, self.i_dim)\n",
    "        zs_q = self.kcnn(X)\n",
    "        zs_q = F.normalize(zs_q, p=2, dim=1)\n",
    "        zs_q_m = torch.matmul(self.z_space, torch.t(zs_q))\n",
    "        return torch.t(zs_q_m)\n",
    "\n",
    "    def compute_sets_and_features(self, X, y, trgt_lbls, q_size, update_kernel):\n",
    "        \"\"\"\n",
    "        Computes support and query sets given a list of labels. It can update the kernel directly if desired.\n",
    "\n",
    "        Args:\n",
    "            X (tensor): data\n",
    "            y (tensor): data labels\n",
    "            trgt_lbls : the labels that will be contained inside the returned sets\n",
    "            q_size : query set's amount of sample per label\n",
    "            update_kernel (bool) : if we want to update the hypershot kernel and z space directly\n",
    "\n",
    "        Returns:\n",
    "            type: final features use by the main network\n",
    "        \"\"\"\n",
    "        s_set, s_set_lbl, q_set, q_set_lbl = self.get_s_and_q_sets(X, y, trgt_lbls, q_size)\n",
    "        \n",
    "        z_space, kernel = self.compute_kernel(s_set, s_set_lbl)\n",
    "        if update_kernel:\n",
    "            self.z_space = z_space\n",
    "            self.kernel = kernel\n",
    "            \n",
    "        return s_set, s_set_lbl, q_set, q_set_lbl\n",
    "\n",
    "    def extend_pred_to_nclasses(self, pred, n_c, lbls):\n",
    "        out = torch.zeros((pred.shape[0], n_c)).to(device)\n",
    "        int_lbls = [int(x) for x in lbls]\n",
    "        for i in range(out.shape[0]):\n",
    "            out[i][int_lbls] = pred[i]\n",
    "        return out\n",
    "    \n",
    "    def update_kernel(s_set, s_set_lbl):\n",
    "        z_space, kernel = self.compute_kernel(s_set, s_set_lbl)\n",
    "        self.z_space = z_space\n",
    "        self.kernel = kernel\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q_features = self.get_q_sample_features(x)\n",
    "        W = self.hnet(uncond_input=self.kernel.view(1, -1))\n",
    "        P = self.mnet.forward(q_features, weights=W)\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbdcd4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_lbls(X_test, y_test, test_classes, hs, n_c, q_size):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy for the sample with label test_classes in X_test.\n",
    "    Mainly used as utility for the calc_accuracy function below.\n",
    "    \n",
    "    Args:\n",
    "        X_test (tensor): entire test set\n",
    "        y_test (tensor): corresponding labels\n",
    "        test_classes: the classes we want to consider for testing accuracies (should contain Ks classes)\n",
    "        hs : hypershot model\n",
    "        n_c: number of classes in the entire dataset\n",
    "        q_size : query set's amount of sample per label\n",
    "\n",
    "    Returns:\n",
    "        type: accuracy\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test = \\\n",
    "        hs.compute_sets_and_features(X_test, y_test, test_classes, q_size, True)\n",
    "\n",
    "        p = hs.forward(q_set_test)\n",
    "        prediction_extended_acc = hs.extend_pred_to_nclasses(p, n_c, test_classes)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(prediction_extended_acc, q_set_lbl_test.long())\n",
    "        accuracy = (torch.argmax(prediction_extended_acc,dim=1) == q_set_lbl_test.long()).float().mean().item()\n",
    "        # print(\"Correctly predicted samples had labels:\", all_q_features_lbls[torch.argmax(prediction_extended_acc,dim=1) == all_q_features_lbls.long()])\n",
    "    return round(accuracy, 2), round(loss.item(), 2)\n",
    "\n",
    "\n",
    "def calc_accuracy(X_test, y_test, hs, n_c, q_size):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy for the entire X_test test set.\n",
    "    \n",
    "    Args:\n",
    "        X_test (tensor): entire test set\n",
    "        y_test (tensor): corresponding labels\n",
    "        hs : hypershot model\n",
    "        n_c: number of classes in the entire dataset\n",
    "        q_size : query set's amount of sample per label\n",
    "\n",
    "    Returns:\n",
    "        average accuracy and loss\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(X_test):\n",
    "        X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    else:  \n",
    "        X_test_t = torch.clone(X_test)\n",
    "        \n",
    "    if not torch.is_tensor(y_test):\n",
    "        y_test_t = torch.FloatTensor(y_test).to(device)\n",
    "    else:\n",
    "        y_test_t = torch.clone(y_test)\n",
    "        \n",
    "    diff_classes = torch.unique(y_test_t)\n",
    "    n_diff_classes = diff_classes.shape[0]\n",
    "    n_sets = int(n_diff_classes / hs.W)\n",
    "    acc, loss = 0.0, 0.0\n",
    "    for i in range(n_sets):\n",
    "        lbls = diff_classes[i*hs.W:(i+1)*hs.W].tolist()\n",
    "        d_acc, d_loss = calc_accuracy_lbls(X_test, y_test, lbls, hs, n_c, q_size)\n",
    "        acc += d_acc\n",
    "        loss += d_loss\n",
    "    acc = acc / n_sets\n",
    "    loss = loss / n_sets\n",
    "    return round(acc,2), round(loss, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "120adb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_lbls_adv(q_set_test, q_set_lbl_test, test_classes, hs, n_c):\n",
    "    \"\"\"\n",
    "    Compute accuracy for a given query test set. We assume the kernel and the z_space of the given hypershot\n",
    "    model hs to be set accordingly. \n",
    "    \n",
    "    \"\"\"\n",
    "    hs.eval()\n",
    "    p = hs.forward(q_set_test)\n",
    "    prediction_extended_acc = hs.extend_pred_to_nclasses(p, n_c, test_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(prediction_extended_acc, q_set_lbl_test.long())\n",
    "    accuracy = (torch.argmax(prediction_extended_acc,dim=1) == q_set_lbl_test.long()).float().mean().item()\n",
    "    # print(\"Correctly predicted samples had labels:\", all_q_features_lbls[torch.argmax(prediction_extended_acc,dim=1) == all_q_features_lbls.long()])\n",
    "    return round(accuracy, 2), round(loss.item(), 2)\n",
    "\n",
    "def calc_accuracy_adv_helper(X_test, y_test, test_classes, hs, n_c, q_size, pgd):\n",
    "    \"\"\"\n",
    "    Compute a support set and a query set for some given test classes. Only used within calc_accuracy_adv\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Updating the kernel here\n",
    "    s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test = \\\n",
    "    hs.compute_sets_and_features(X_test, y_test, test_classes, q_size, True)\n",
    "\n",
    "    unique_values = torch.unique(q_set_lbl_test.long())\n",
    "    value_to_index = {value.item(): index for index, value in enumerate(unique_values)}\n",
    "    q_set_lbl_test_m = torch.FloatTensor([value_to_index[value.item()] for value in q_set_lbl_test.long()])\n",
    "\n",
    "    adv_inputs = pgd_attack(q_set_test, q_set_lbl_test_m.long())\n",
    "\n",
    "    p = hs.forward(adv_inputs)\n",
    "    prediction_extended_acc = hs.extend_pred_to_nclasses(p, n_c, test_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(prediction_extended_acc, q_set_lbl_test.long())\n",
    "    accuracy = (torch.argmax(prediction_extended_acc,dim=1) == q_set_lbl_test.long()).float().mean().item()\n",
    "    # print(\"Correctly predicted samples had labels:\", all_q_features_lbls[torch.argmax(prediction_extended_acc,dim=1) == all_q_features_lbls.long()])\n",
    "    return accuracy, loss.item()\n",
    "\n",
    "def calc_accuracy_adv(X_test, y_test, hs, n_c, q_size, pgd):\n",
    "    \"\"\"\n",
    "    Computes the prediction accuracy for the entire X_test test set applying PGD.\n",
    "    \n",
    "    Args:\n",
    "        X_test (tensor): test set\n",
    "        y_test (tensor): corresponding labels\n",
    "        hs : hypershot model\n",
    "        n_c: number of classes in the entire dataset\n",
    "        q_size : query set's amount of sample per label\n",
    "        pgd : a pgd attack from torchattacks\n",
    "\n",
    "    Returns:\n",
    "        type: average accuracy and loss\n",
    "    \"\"\"\n",
    "    hs.eval()\n",
    "    if not torch.is_tensor(X_test):\n",
    "        X_test_t = torch.FloatTensor(X_test).requires_grad_(True).to(device)\n",
    "    else:  \n",
    "        X_test_t = torch.clone(X_test).requires_grad_(True)\n",
    "        \n",
    "    if not torch.is_tensor(y_test):\n",
    "        y_test_t = torch.FloatTensor(y_test).requires_grad_(True).to(device)\n",
    "    else:\n",
    "        y_test_t = torch.clone(y_test).requires_grad_(True)\n",
    "        \n",
    "    diff_classes = torch.unique(y_test_t)\n",
    "    n_diff_classes = diff_classes.shape[0]\n",
    "    n_sets = int(n_diff_classes / hs.W)\n",
    "    acc, loss = 0.0, 0.0\n",
    "    for i in range(n_sets):\n",
    "        lbls = diff_classes[i*hs.W:(i+1)*hs.W].tolist()\n",
    "        d_acc, d_loss = calc_accuracy_adv_helper(X_test, y_test, lbls, hs, n_c, q_size, pgd)\n",
    "        acc += d_acc\n",
    "        loss += d_loss\n",
    "    acc = acc / n_sets\n",
    "    loss = loss / n_sets\n",
    "    return round(acc,2), round(loss, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd4014d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Creating an MLP with 1413 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 369797 weights and 1413 outputs (compression ratio: 261.71).\n",
      "The network consists of 369797 unconditional weights (369797 internally maintained) and 0 conditional weights (0 internally maintained).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quent\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\quent\\anaconda3\\envs\\hypernets\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- Epoch 0  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Adversarial training starts.\n",
      "Local train acc and loss at the end of set: 0 --> (0.2, 6.99)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.2, 7.01)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.2, 1.57)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.2, 1.59)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (0.61, 0.92)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.53, 1.14)\n",
      "----\n",
      "Global loss at the end of epoch: 0 : 673.9761847257614\n",
      "--> Global test accuracy after epoch: 0 --> 0.6\n",
      "--> Global adv test accuracy after epoch: 0 --> 0.4\n",
      "\n",
      "----------------------- Epoch 1  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (0.64, 0.9)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.33, 1.46)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.79, 0.51)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.61, 0.73)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (0.94, 0.25)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.9, 0.38)\n",
      "----\n",
      "Global loss at the end of epoch: 1 : 271.99277317523956\n",
      "--> Global test accuracy after epoch: 1 --> 0.79\n",
      "--> Global adv test accuracy after epoch: 1 --> 0.58\n",
      "\n",
      "----------------------- Epoch 2  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (0.83, 0.39)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.57, 0.98)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.93, 0.28)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.83, 0.61)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (0.93, 0.25)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.63, 1.21)\n",
      "----\n",
      "Global loss at the end of epoch: 2 : 195.90722090005875\n",
      "--> Global test accuracy after epoch: 2 --> 0.84\n",
      "--> Global adv test accuracy after epoch: 2 --> 0.66\n",
      "\n",
      "----------------------- Epoch 3  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (0.86, 0.33)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.63, 0.96)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.96, 0.11)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.9, 0.25)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (0.94, 0.16)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.81, 0.43)\n",
      "----\n",
      "Global loss at the end of epoch: 3 : 115.67937263846397\n",
      "--> Global test accuracy after epoch: 3 --> 0.85\n",
      "--> Global adv test accuracy after epoch: 3 --> 0.7\n",
      "\n",
      "----------------------- Epoch 4  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (0.89, 0.42)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.77, 0.62)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.89, 0.27)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.69, 0.69)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (0.86, 0.3)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.6, 0.85)\n",
      "----\n",
      "Global loss at the end of epoch: 4 : 114.92915650457144\n",
      "--> Global test accuracy after epoch: 4 --> 0.89\n",
      "--> Global adv test accuracy after epoch: 4 --> 0.74\n",
      "\n",
      "----------------------- Epoch 5  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (0.9, 0.3)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.73, 0.94)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.97, 0.12)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.87, 0.46)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (0.97, 0.05)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.93, 0.13)\n",
      "----\n",
      "Global loss at the end of epoch: 5 : 88.1486495975405\n",
      "--> Global test accuracy after epoch: 5 --> 0.89\n",
      "--> Global adv test accuracy after epoch: 5 --> 0.73\n",
      "\n",
      "----------------------- Epoch 6  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (0.94, 0.16)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.86, 0.3)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.96, 0.09)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.89, 0.26)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (0.97, 0.09)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.8, 0.38)\n",
      "----\n",
      "Global loss at the end of epoch: 6 : 83.82653380930424\n",
      "--> Global test accuracy after epoch: 6 --> 0.91\n",
      "--> Global adv test accuracy after epoch: 6 --> 0.81\n",
      "\n",
      "----------------------- Epoch 7  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (0.97, 0.08)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.86, 0.26)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.02)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.94, 0.16)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (0.94, 0.17)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.67, 0.77)\n",
      "----\n",
      "Global loss at the end of epoch: 7 : 64.23727932572365\n",
      "--> Global test accuracy after epoch: 7 --> 0.93\n",
      "--> Global adv test accuracy after epoch: 7 --> 0.84\n",
      "\n",
      "----------------------- Epoch 8  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.0)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (1.0, 0.05)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.87, 0.24)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.81, 0.6)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (0.97, 0.06)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.93, 0.15)\n",
      "----\n",
      "Global loss at the end of epoch: 8 : 50.36510947998613\n",
      "--> Global test accuracy after epoch: 8 --> 0.94\n",
      "--> Global adv test accuracy after epoch: 8 --> 0.83\n",
      "\n",
      "----------------------- Epoch 9  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (0.99, 0.05)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.99, 0.11)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.02)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.9, 0.23)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.02)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.99, 0.05)\n",
      "----\n",
      "Global loss at the end of epoch: 9 : 52.21618404984474\n",
      "--> Global test accuracy after epoch: 9 --> 0.94\n",
      "--> Global adv test accuracy after epoch: 9 --> 0.85\n",
      "\n",
      "----------------------- Epoch 10  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.0)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.99, 0.04)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.02)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.99, 0.07)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.01)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.89, 0.27)\n",
      "----\n",
      "Global loss at the end of epoch: 10 : 37.45908302348107\n",
      "--> Global test accuracy after epoch: 10 --> 0.95\n",
      "--> Global adv test accuracy after epoch: 10 --> 0.86\n",
      "\n",
      "----------------------- Epoch 11  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.0)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (1.0, 0.04)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.0)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.99, 0.03)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.02)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.99, 0.05)\n",
      "----\n",
      "Global loss at the end of epoch: 11 : 45.543999628163874\n",
      "--> Global test accuracy after epoch: 11 --> 0.94\n",
      "--> Global adv test accuracy after epoch: 11 --> 0.85\n",
      "\n",
      "----------------------- Epoch 12  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (0.99, 0.02)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.97, 0.1)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.97, 0.13)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.9, 0.26)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.0)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.99, 0.07)\n",
      "----\n",
      "Global loss at the end of epoch: 12 : 31.69521727086976\n",
      "--> Global test accuracy after epoch: 12 --> 0.95\n",
      "--> Global adv test accuracy after epoch: 12 --> 0.88\n",
      "\n",
      "----------------------- Epoch 13  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.0)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (1.0, 0.02)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.01)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.96, 0.1)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.01)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (1.0, 0.03)\n",
      "----\n",
      "Global loss at the end of epoch: 13 : 32.90515324717853\n",
      "--> Global test accuracy after epoch: 13 --> 0.95\n",
      "--> Global adv test accuracy after epoch: 13 --> 0.88\n",
      "\n",
      "----------------------- Epoch 14  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (0.97, 0.12)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.96, 0.22)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.01)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.94, 0.11)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (0.96, 0.08)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.94, 0.19)\n",
      "----\n",
      "Global loss at the end of epoch: 14 : 25.537862464319915\n",
      "--> Global test accuracy after epoch: 14 --> 0.95\n",
      "--> Global adv test accuracy after epoch: 14 --> 0.88\n",
      "\n",
      "----------------------- Epoch 15  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.02)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.93, 0.18)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.01)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.97, 0.05)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.01)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.99, 0.05)\n",
      "----\n",
      "Global loss at the end of epoch: 15 : 30.03906202764483\n",
      "--> Global test accuracy after epoch: 15 --> 0.95\n",
      "--> Global adv test accuracy after epoch: 15 --> 0.89\n",
      "\n",
      "----------------------- Epoch 16  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.01)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.96, 0.06)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.96, 0.15)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.73, 0.7)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.0)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (1.0, 0.02)\n",
      "----\n",
      "Global loss at the end of epoch: 16 : 30.869666078593582\n",
      "--> Global test accuracy after epoch: 16 --> 0.95\n",
      "--> Global adv test accuracy after epoch: 16 --> 0.89\n",
      "\n",
      "----------------------- Epoch 17  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (0.97, 0.08)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.93, 0.21)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.99, 0.02)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.97, 0.09)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.01)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.96, 0.11)\n",
      "----\n",
      "Global loss at the end of epoch: 17 : 20.984250698587857\n",
      "--> Global test accuracy after epoch: 17 --> 0.96\n",
      "--> Global adv test accuracy after epoch: 17 --> 0.9\n",
      "\n",
      "----------------------- Epoch 18  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.01)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.99, 0.03)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (1.0, 0.0)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.99, 0.03)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.01)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.97, 0.07)\n",
      "----\n",
      "Global loss at the end of epoch: 18 : 25.310140203218907\n",
      "--> Global test accuracy after epoch: 18 --> 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Global adv test accuracy after epoch: 18 --> 0.9\n",
      "\n",
      "----------------------- Epoch 19  -----------------------\n",
      "Generated train-test split for 100 / 220\n",
      "Generated train-test split for 200 / 220\n",
      "Local train acc and loss at the end of set: 0 --> (1.0, 0.01)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 0 --> (0.99, 0.04)\n",
      "----\n",
      "Went over 50 over 220 sets.\n",
      "Went over 100 over 220 sets.\n",
      "Local train acc and loss at the end of set: 100 --> (0.99, 0.04)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 100 --> (0.93, 0.14)\n",
      "----\n",
      "Went over 150 over 220 sets.\n",
      "Went over 200 over 220 sets.\n",
      "Local train acc and loss at the end of set: 200 --> (1.0, 0.0)\n",
      "Local adv train acc and loss (train kernel, q attacked only): 200 --> (0.97, 0.06)\n",
      "----\n",
      "Global loss at the end of epoch: 19 : 16.072455246932805\n",
      "--> Global test accuracy after epoch: 19 --> 0.96\n",
      "--> Global adv test accuracy after epoch: 19 --> 0.9\n",
      "\n",
      "END OF ITERATION: 1\n"
     ]
    }
   ],
   "source": [
    "# Configure training.\n",
    "# we used 20 for training with adversarial in both K = 1 and K = 5\n",
    "# we used 50 for training with no adversarial in both K = 1 and K = 5\n",
    "nepochs=20\n",
    "\n",
    "# Epoch after which adversarial training starts\n",
    "# To disable adversarial training, just put a number higher than nepochs\n",
    "do_adv_train = 0\n",
    "\n",
    "# K-shot W-way\n",
    "K = 5\n",
    "W = 5\n",
    "\n",
    "# Epsilon parameter of the PGD attack, here it is static\n",
    "eps = 8.0 / 255\n",
    "\n",
    "# Length of the embeddings produced by the CNN (Hypershot parameter)\n",
    "z_len = 256\n",
    "\n",
    "load_weights = 0\n",
    "continue_training = 0\n",
    "\n",
    "# Amount of sample in query sets during training (for one corresponding support set)\n",
    "q_train = 14\n",
    "# Amount of sample in query sets during validation and testing (for one corresponding support set)\n",
    "q_test = 15\n",
    "\n",
    "# Loop in case we want to do statistics (not sued for now)\n",
    "for o in range(1):\n",
    "    print(\"Iteration\", o+1)\n",
    "    \n",
    "    # If we want to train the model for some more epochs\n",
    "    if continue_training == 0:\n",
    "        hypershot = Hypershot(kcnn_input_channels=1, z_length=z_len, kcnn_weights=None,\n",
    "                              hnet_hidden_layers=1, hnet_hidden_size=256, hnet_weights=None,\n",
    "                              mnet_hidden_layers=1, mnet_hidden_size=128,\n",
    "                              K=K, W=W, i_dim=28, i_cha=1, load_w = False).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        pgd_attack = PGD(hypershot, eps=eps, alpha=2.0 / 255, steps=20)\n",
    "        n_sets = int(len(lbls_0) / W)\n",
    "    \n",
    "    # Optimizer and scheduler initialization\n",
    "    optimizer = optim.Adam(hypershot.parameters(), lr=0.0001)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=int(nepochs / 1), eta_min=0.00001)\n",
    "        \n",
    "    # Main training loop\n",
    "    for epoch in range(nepochs): # For each epoch.\n",
    "        print(\"----------------------- Epoch\", epoch, \" -----------------------\")\n",
    "        \n",
    "        # We start by generating training and test set split\n",
    "        train_test_sets = []\n",
    "        all_test_sets = np.empty((0, dataset_0.shape[1]))\n",
    "        all_test_sets_lbl = np.empty((0))\n",
    "        for l_set_id in range(n_sets):\n",
    "            r_lbls = random.sample(lbls_0, W)\n",
    "            r_lbls.sort()\n",
    "            if (l_set_id+1) % 100 == 0:\n",
    "                print(\"Generated train-test split for\", l_set_id+1,\"/\",n_sets)\n",
    "            mask_b = np.isin(dataset_0_lbl, np.array(r_lbls))\n",
    "            dataset_0_b, dataset_0_lbl_b = dataset_0[mask_b], dataset_0_lbl[mask_b]\n",
    "            dataset_0_train, dataset_0_test, dataset_0_lbl_train, dataset_0_lbl_test = \\\n",
    "                            train_test_split(dataset_0_b, dataset_0_lbl_b, random_state=42, test_size=0.05, stratify=dataset_0_lbl_b)\n",
    "            train_test_sets.append((dataset_0_train, dataset_0_test, dataset_0_lbl_train, dataset_0_lbl_test, r_lbls))\n",
    "            \n",
    "            all_test_sets = np.concatenate((all_test_sets, dataset_0_test), axis=0)\n",
    "            all_test_sets_lbl = np.concatenate((all_test_sets_lbl, dataset_0_lbl_test), axis=0)\n",
    "            train_test_sets.append((dataset_0_train, dataset_0_test, dataset_0_lbl_train, dataset_0_lbl_test, r_lbls))\n",
    "        \n",
    "        # Stores the loss over all labels sets\n",
    "        global_loss = 0.0\n",
    "        global_loss_float = 0.0\n",
    "        \n",
    "        # We loop over all our sets at each epoch\n",
    "        for l_set_id in range(n_sets):\n",
    "            if (l_set_id + 1) % 50 == 0:\n",
    "                print(\"Went over\", l_set_id+1, \"over\", n_sets, \"sets.\")\n",
    "            hypershot.train()\n",
    "            loss_dataset_l = 0.0\n",
    "            (dataset_l_train, dataset_l_test, dataset_l_lbl_train, dataset_l_lbl_test, c_lbls) = train_test_sets[l_set_id]\n",
    "            \n",
    "            # This also modifies the kernel and z_space of hypershot\n",
    "            # print(\"Generating sets for training\", c_lbls)\n",
    "            s_set_train, s_set_lbl_train, q_set_train, q_set_lbl_train = \\\n",
    "            hypershot.compute_sets_and_features(dataset_l_train, dataset_l_lbl_train, c_lbls, q_train, True)\n",
    "            \n",
    "            # print(s_set_train.shape)\n",
    "            # print(q_set_train.shape)\n",
    "            \n",
    "            # Formward pass\n",
    "            dataset_l_P = hypershot.forward(q_set_train)\n",
    "            prediction_extended = hypershot.extend_pred_to_nclasses(dataset_l_P, n_classes, c_lbls)\n",
    "            loss_dataset_l += criterion(prediction_extended, q_set_lbl_train.long())\n",
    "\n",
    "            # Adversarial training\n",
    "            if epoch == do_adv_train and l_set_id == 0:\n",
    "                print(\"Adversarial training starts.\")\n",
    "            if epoch >= do_adv_train:\n",
    "                unique_values = torch.unique(q_set_lbl_train.long())\n",
    "                value_to_index = {value.item(): index for index, value in enumerate(unique_values)}\n",
    "                q_set_lbl_train_m = torch.tensor([value_to_index[value.item()] for value in q_set_lbl_train.long()])\n",
    "                \n",
    "                adv_inputs = pgd_attack(q_set_train, q_set_lbl_train_m.long())\n",
    "                adv_outputs = hypershot.forward(adv_inputs)\n",
    "                prediction_extended_adv = hypershot.extend_pred_to_nclasses(adv_outputs, n_classes, c_lbls)\n",
    "                adv_loss = criterion(prediction_extended_adv, q_set_lbl_train.long())\n",
    "                loss_dataset_l += adv_loss\n",
    "            \n",
    "            global_loss += loss_dataset_l\n",
    "            global_loss_float += loss_dataset_l.item()\n",
    "            \n",
    "            if l_set_id % 100 == 0:\n",
    "                train_metrics = calc_accuracy(dataset_l_train, dataset_l_lbl_train, hypershot, n_classes, q_train)\n",
    "                # test_metrics = calc_accuracy(dataset_l_test, dataset_l_lbl_test, hypershot, n_classes, q_test)\n",
    "                print(\"Local train acc and loss at the end of set:\", l_set_id, \"-->\", train_metrics)\n",
    "                # print(\"Local valid acc and loss at the end of set:\", l_set_id, \"-->\", test_metrics)\n",
    "                \n",
    "                # Training loss when attacking query image only\n",
    "                unique_values = torch.unique(q_set_lbl_train.long())\n",
    "                value_to_index = {value.item(): index for index, value in enumerate(unique_values)}\n",
    "                q_set_lbl_train_m = torch.tensor([value_to_index[value.item()] for value in q_set_lbl_train.long()])\n",
    "\n",
    "                adv_inputs_train = pgd_attack(q_set_train, q_set_lbl_train_m.long())\n",
    "                train_metrics_adv = calc_accuracy_lbls_adv(adv_inputs_train, q_set_lbl_train.long(), c_lbls, hypershot, n_classes)\n",
    "                print(\"Local adv train acc and loss (train kernel, q attacked only):\", l_set_id, \"-->\", train_metrics_adv)\n",
    "\n",
    "                # Validation loss when attacking query image only\n",
    "                # We update the kernel here (but we did not attack the support set)\n",
    "                # s_set_test, s_set_lbl_test, q_set_test, q_set_lbl_test = \\\n",
    "                # hypershot.compute_sets_and_features(dataset_l_test, dataset_l_lbl_test, c_lbls, q_test, True)\n",
    "\n",
    "                # unique_values = torch.unique(q_set_lbl_test.long())\n",
    "                # value_to_index = {value.item(): index for index, value in enumerate(unique_values)}\n",
    "                # q_set_lbl_test_m = torch.tensor([value_to_index[value.item()] for value in q_set_lbl_test.long()])\n",
    "\n",
    "                # adv_inputs_test = pgd_attack(q_set_test, q_set_lbl_test_m.long())\n",
    "                # test_metrics_adv = calc_accuracy_lbls_adv(adv_inputs_test, q_set_lbl_test.long(), c_lbls, hypershot, n_classes)\n",
    "                # print(\"Local adv valid acc and loss (valid kernel, q attacked only):\", l_set_id, \"-->\", test_metrics_adv)\n",
    "                print(\"----\")\n",
    "\n",
    "            loss_dataset_l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        scheduler.step()                \n",
    "  \n",
    "        print(\"Global loss at the end of epoch:\", epoch, \":\", global_loss_float)\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            # (gva, gvl) = calc_accuracy(all_test_sets, all_test_sets_lbl, hypershot, n_classes, q_test)\n",
    "            (gta, gtl) = calc_accuracy(dataset_1, dataset_1_lbl, hypershot, n_classes, q_test)\n",
    "            # print(\"--> Global valid accuracy after epoch:\", epoch, \"-->\", gva)\n",
    "            print(\"--> Global test accuracy after epoch:\", epoch, \"-->\", gta)\n",
    "            # (gva_adv, gvl_adv) = calc_accuracy_adv(all_test_sets, all_test_sets_lbl, hypershot, n_classes, q_test, pgd_attack)\n",
    "            (gta_adv, gtl_adv) = calc_accuracy_adv(dataset_1, dataset_1_lbl, hypershot, n_classes, q_test, pgd_attack)\n",
    "            # print(\"--> Global adv valid accuracy after epoch:\", epoch, \"-->\", gva_adv)\n",
    "            print(\"--> Global adv test accuracy after epoch:\", epoch, \"-->\", gta_adv)\n",
    "            current_time = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "            gta_r = round(gta, 2)\n",
    "            gta_str = str(gta_r).replace('.', '_')\n",
    "            gta_r_adv = round(gta_adv, 2)\n",
    "            gta_str_adv = str(gta_r_adv).replace('.', '_')\n",
    "            hs_file = f'models/HS_{K}Shot_{W}Way_{int(eps*255)}eps_{gta_str}Acc_{gta_str_adv}Adv_{current_time}_{epoch}.pth'\n",
    "            torch.save(hypershot.state_dict(), hs_file)\n",
    "        print()\n",
    "\n",
    "    print(\"END OF ITERATION:\",o+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
